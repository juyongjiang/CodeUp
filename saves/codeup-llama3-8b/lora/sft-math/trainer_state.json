{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 16665,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0018001800180018,
      "grad_norm": 1.2423814535140991,
      "learning_rate": 5.998800239952011e-07,
      "loss": 0.8217,
      "step": 10
    },
    {
      "epoch": 0.0036003600360036,
      "grad_norm": 1.171341896057129,
      "learning_rate": 1.1997600479904021e-06,
      "loss": 0.8286,
      "step": 20
    },
    {
      "epoch": 0.0054005400540054005,
      "grad_norm": 1.1023069620132446,
      "learning_rate": 1.799640071985603e-06,
      "loss": 0.8099,
      "step": 30
    },
    {
      "epoch": 0.0072007200720072,
      "grad_norm": 1.3468056917190552,
      "learning_rate": 2.3995200959808042e-06,
      "loss": 0.8024,
      "step": 40
    },
    {
      "epoch": 0.009000900090009001,
      "grad_norm": 1.1060608625411987,
      "learning_rate": 2.999400119976005e-06,
      "loss": 0.7372,
      "step": 50
    },
    {
      "epoch": 0.010801080108010801,
      "grad_norm": 0.9075316786766052,
      "learning_rate": 3.599280143971206e-06,
      "loss": 0.6418,
      "step": 60
    },
    {
      "epoch": 0.012601260126012601,
      "grad_norm": 0.6361078023910522,
      "learning_rate": 4.1991601679664066e-06,
      "loss": 0.5447,
      "step": 70
    },
    {
      "epoch": 0.0144014401440144,
      "grad_norm": 0.4494197368621826,
      "learning_rate": 4.7990401919616085e-06,
      "loss": 0.4573,
      "step": 80
    },
    {
      "epoch": 0.016201620162016202,
      "grad_norm": 0.3865918815135956,
      "learning_rate": 5.398920215956809e-06,
      "loss": 0.4282,
      "step": 90
    },
    {
      "epoch": 0.018001800180018002,
      "grad_norm": 0.3896229565143585,
      "learning_rate": 5.99880023995201e-06,
      "loss": 0.4059,
      "step": 100
    },
    {
      "epoch": 0.019801980198019802,
      "grad_norm": 0.3697451651096344,
      "learning_rate": 6.598680263947212e-06,
      "loss": 0.3889,
      "step": 110
    },
    {
      "epoch": 0.021602160216021602,
      "grad_norm": 0.3609845042228699,
      "learning_rate": 7.198560287942412e-06,
      "loss": 0.3737,
      "step": 120
    },
    {
      "epoch": 0.023402340234023402,
      "grad_norm": 0.3533078134059906,
      "learning_rate": 7.798440311937614e-06,
      "loss": 0.3528,
      "step": 130
    },
    {
      "epoch": 0.025202520252025202,
      "grad_norm": 0.34070929884910583,
      "learning_rate": 8.398320335932813e-06,
      "loss": 0.3532,
      "step": 140
    },
    {
      "epoch": 0.027002700270027002,
      "grad_norm": 0.3591287136077881,
      "learning_rate": 8.998200359928014e-06,
      "loss": 0.3415,
      "step": 150
    },
    {
      "epoch": 0.0288028802880288,
      "grad_norm": 0.3753328025341034,
      "learning_rate": 9.598080383923217e-06,
      "loss": 0.3408,
      "step": 160
    },
    {
      "epoch": 0.0306030603060306,
      "grad_norm": 0.40738967061042786,
      "learning_rate": 1.0197960407918416e-05,
      "loss": 0.339,
      "step": 170
    },
    {
      "epoch": 0.032403240324032405,
      "grad_norm": 0.3948952257633209,
      "learning_rate": 1.0797840431913617e-05,
      "loss": 0.3243,
      "step": 180
    },
    {
      "epoch": 0.034203420342034205,
      "grad_norm": 0.3819776177406311,
      "learning_rate": 1.1397720455908818e-05,
      "loss": 0.3128,
      "step": 190
    },
    {
      "epoch": 0.036003600360036005,
      "grad_norm": 0.4043758511543274,
      "learning_rate": 1.199760047990402e-05,
      "loss": 0.3167,
      "step": 200
    },
    {
      "epoch": 0.037803780378037805,
      "grad_norm": 0.44234296679496765,
      "learning_rate": 1.259748050389922e-05,
      "loss": 0.3084,
      "step": 210
    },
    {
      "epoch": 0.039603960396039604,
      "grad_norm": 0.3967500627040863,
      "learning_rate": 1.3197360527894423e-05,
      "loss": 0.3073,
      "step": 220
    },
    {
      "epoch": 0.041404140414041404,
      "grad_norm": 0.37582042813301086,
      "learning_rate": 1.3797240551889623e-05,
      "loss": 0.3099,
      "step": 230
    },
    {
      "epoch": 0.043204320432043204,
      "grad_norm": 0.4131649434566498,
      "learning_rate": 1.4397120575884824e-05,
      "loss": 0.2976,
      "step": 240
    },
    {
      "epoch": 0.045004500450045004,
      "grad_norm": 0.41162756085395813,
      "learning_rate": 1.4997000599880023e-05,
      "loss": 0.3065,
      "step": 250
    },
    {
      "epoch": 0.046804680468046804,
      "grad_norm": 0.3904203176498413,
      "learning_rate": 1.5596880623875228e-05,
      "loss": 0.2956,
      "step": 260
    },
    {
      "epoch": 0.048604860486048604,
      "grad_norm": 0.4602794051170349,
      "learning_rate": 1.6196760647870427e-05,
      "loss": 0.3041,
      "step": 270
    },
    {
      "epoch": 0.050405040504050404,
      "grad_norm": 0.4311939775943756,
      "learning_rate": 1.6796640671865626e-05,
      "loss": 0.2945,
      "step": 280
    },
    {
      "epoch": 0.052205220522052204,
      "grad_norm": 0.4787069261074066,
      "learning_rate": 1.739652069586083e-05,
      "loss": 0.2852,
      "step": 290
    },
    {
      "epoch": 0.054005400540054004,
      "grad_norm": 0.4486701190471649,
      "learning_rate": 1.799640071985603e-05,
      "loss": 0.2913,
      "step": 300
    },
    {
      "epoch": 0.0558055805580558,
      "grad_norm": 0.45706430077552795,
      "learning_rate": 1.859628074385123e-05,
      "loss": 0.2865,
      "step": 310
    },
    {
      "epoch": 0.0576057605760576,
      "grad_norm": 0.4793759286403656,
      "learning_rate": 1.9196160767846434e-05,
      "loss": 0.2844,
      "step": 320
    },
    {
      "epoch": 0.0594059405940594,
      "grad_norm": 0.4402153491973877,
      "learning_rate": 1.9796040791841633e-05,
      "loss": 0.2761,
      "step": 330
    },
    {
      "epoch": 0.0612061206120612,
      "grad_norm": 0.4271174371242523,
      "learning_rate": 2.0395920815836833e-05,
      "loss": 0.2768,
      "step": 340
    },
    {
      "epoch": 0.063006300630063,
      "grad_norm": 0.4309771656990051,
      "learning_rate": 2.0995800839832032e-05,
      "loss": 0.2948,
      "step": 350
    },
    {
      "epoch": 0.06480648064806481,
      "grad_norm": 0.49882325530052185,
      "learning_rate": 2.1595680863827235e-05,
      "loss": 0.278,
      "step": 360
    },
    {
      "epoch": 0.0666066606660666,
      "grad_norm": 0.4641379714012146,
      "learning_rate": 2.2195560887822437e-05,
      "loss": 0.2668,
      "step": 370
    },
    {
      "epoch": 0.06840684068406841,
      "grad_norm": 0.4513561427593231,
      "learning_rate": 2.2795440911817637e-05,
      "loss": 0.2719,
      "step": 380
    },
    {
      "epoch": 0.0702070207020702,
      "grad_norm": 0.4461786448955536,
      "learning_rate": 2.339532093581284e-05,
      "loss": 0.2754,
      "step": 390
    },
    {
      "epoch": 0.07200720072007201,
      "grad_norm": 0.4302377700805664,
      "learning_rate": 2.399520095980804e-05,
      "loss": 0.2821,
      "step": 400
    },
    {
      "epoch": 0.0738073807380738,
      "grad_norm": 0.4108532965183258,
      "learning_rate": 2.459508098380324e-05,
      "loss": 0.2669,
      "step": 410
    },
    {
      "epoch": 0.07560756075607561,
      "grad_norm": 0.4499903619289398,
      "learning_rate": 2.519496100779844e-05,
      "loss": 0.2744,
      "step": 420
    },
    {
      "epoch": 0.0774077407740774,
      "grad_norm": 0.4148717522621155,
      "learning_rate": 2.5794841031793644e-05,
      "loss": 0.2643,
      "step": 430
    },
    {
      "epoch": 0.07920792079207921,
      "grad_norm": 0.4294882118701935,
      "learning_rate": 2.6394721055788847e-05,
      "loss": 0.2656,
      "step": 440
    },
    {
      "epoch": 0.081008100810081,
      "grad_norm": 0.45851072669029236,
      "learning_rate": 2.6994601079784043e-05,
      "loss": 0.2649,
      "step": 450
    },
    {
      "epoch": 0.08280828082808281,
      "grad_norm": 0.4480215013027191,
      "learning_rate": 2.7594481103779245e-05,
      "loss": 0.2788,
      "step": 460
    },
    {
      "epoch": 0.0846084608460846,
      "grad_norm": 0.4454787075519562,
      "learning_rate": 2.8194361127774448e-05,
      "loss": 0.2622,
      "step": 470
    },
    {
      "epoch": 0.08640864086408641,
      "grad_norm": 0.4034762382507324,
      "learning_rate": 2.8794241151769647e-05,
      "loss": 0.2603,
      "step": 480
    },
    {
      "epoch": 0.08820882088208822,
      "grad_norm": 0.4599131643772125,
      "learning_rate": 2.939412117576485e-05,
      "loss": 0.2569,
      "step": 490
    },
    {
      "epoch": 0.09000900090009001,
      "grad_norm": 0.43139198422431946,
      "learning_rate": 2.9994001199760046e-05,
      "loss": 0.2582,
      "step": 500
    },
    {
      "epoch": 0.09000900090009001,
      "eval_loss": 0.2654580771923065,
      "eval_runtime": 316.7503,
      "eval_samples_per_second": 124.704,
      "eval_steps_per_second": 3.899,
      "step": 500
    },
    {
      "epoch": 0.09180918091809182,
      "grad_norm": 0.4563123881816864,
      "learning_rate": 3.059388122375525e-05,
      "loss": 0.2568,
      "step": 510
    },
    {
      "epoch": 0.09360936093609361,
      "grad_norm": 0.4074452817440033,
      "learning_rate": 3.1193761247750455e-05,
      "loss": 0.2589,
      "step": 520
    },
    {
      "epoch": 0.09540954095409541,
      "grad_norm": 0.380723237991333,
      "learning_rate": 3.179364127174565e-05,
      "loss": 0.2697,
      "step": 530
    },
    {
      "epoch": 0.09720972097209721,
      "grad_norm": 0.44449037313461304,
      "learning_rate": 3.2393521295740854e-05,
      "loss": 0.2608,
      "step": 540
    },
    {
      "epoch": 0.09900990099009901,
      "grad_norm": 0.3717697262763977,
      "learning_rate": 3.2993401319736057e-05,
      "loss": 0.2607,
      "step": 550
    },
    {
      "epoch": 0.10081008100810081,
      "grad_norm": 0.42521175742149353,
      "learning_rate": 3.359328134373125e-05,
      "loss": 0.2569,
      "step": 560
    },
    {
      "epoch": 0.10261026102610261,
      "grad_norm": 0.40064287185668945,
      "learning_rate": 3.4193161367726455e-05,
      "loss": 0.2622,
      "step": 570
    },
    {
      "epoch": 0.10441044104410441,
      "grad_norm": 0.35444778203964233,
      "learning_rate": 3.479304139172166e-05,
      "loss": 0.2587,
      "step": 580
    },
    {
      "epoch": 0.10621062106210621,
      "grad_norm": 0.4273347854614258,
      "learning_rate": 3.5392921415716854e-05,
      "loss": 0.2597,
      "step": 590
    },
    {
      "epoch": 0.10801080108010801,
      "grad_norm": 0.40581977367401123,
      "learning_rate": 3.599280143971206e-05,
      "loss": 0.2618,
      "step": 600
    },
    {
      "epoch": 0.10981098109810981,
      "grad_norm": 0.42926231026649475,
      "learning_rate": 3.659268146370726e-05,
      "loss": 0.2485,
      "step": 610
    },
    {
      "epoch": 0.1116111611161116,
      "grad_norm": 0.3858729302883148,
      "learning_rate": 3.719256148770246e-05,
      "loss": 0.2527,
      "step": 620
    },
    {
      "epoch": 0.11341134113411341,
      "grad_norm": 0.4210394620895386,
      "learning_rate": 3.7792441511697665e-05,
      "loss": 0.2491,
      "step": 630
    },
    {
      "epoch": 0.1152115211521152,
      "grad_norm": 0.4262006878852844,
      "learning_rate": 3.839232153569287e-05,
      "loss": 0.2549,
      "step": 640
    },
    {
      "epoch": 0.11701170117011701,
      "grad_norm": 0.42690521478652954,
      "learning_rate": 3.8992201559688064e-05,
      "loss": 0.2603,
      "step": 650
    },
    {
      "epoch": 0.1188118811881188,
      "grad_norm": 0.3733782172203064,
      "learning_rate": 3.9592081583683266e-05,
      "loss": 0.2532,
      "step": 660
    },
    {
      "epoch": 0.12061206120612061,
      "grad_norm": 0.3518364727497101,
      "learning_rate": 4.019196160767847e-05,
      "loss": 0.2506,
      "step": 670
    },
    {
      "epoch": 0.1224122412241224,
      "grad_norm": 0.407071977853775,
      "learning_rate": 4.0791841631673665e-05,
      "loss": 0.2472,
      "step": 680
    },
    {
      "epoch": 0.12421242124212421,
      "grad_norm": 0.363167941570282,
      "learning_rate": 4.139172165566887e-05,
      "loss": 0.2547,
      "step": 690
    },
    {
      "epoch": 0.126012601260126,
      "grad_norm": 0.4160621464252472,
      "learning_rate": 4.1991601679664064e-05,
      "loss": 0.2522,
      "step": 700
    },
    {
      "epoch": 0.1278127812781278,
      "grad_norm": 0.3881072998046875,
      "learning_rate": 4.259148170365927e-05,
      "loss": 0.254,
      "step": 710
    },
    {
      "epoch": 0.12961296129612962,
      "grad_norm": 0.39436206221580505,
      "learning_rate": 4.319136172765447e-05,
      "loss": 0.2574,
      "step": 720
    },
    {
      "epoch": 0.13141314131413143,
      "grad_norm": 0.3815993368625641,
      "learning_rate": 4.379124175164967e-05,
      "loss": 0.2572,
      "step": 730
    },
    {
      "epoch": 0.1332133213321332,
      "grad_norm": 0.370561808347702,
      "learning_rate": 4.4391121775644875e-05,
      "loss": 0.2568,
      "step": 740
    },
    {
      "epoch": 0.135013501350135,
      "grad_norm": 0.3727509677410126,
      "learning_rate": 4.499100179964008e-05,
      "loss": 0.248,
      "step": 750
    },
    {
      "epoch": 0.13681368136813682,
      "grad_norm": 0.3629249632358551,
      "learning_rate": 4.5590881823635274e-05,
      "loss": 0.2521,
      "step": 760
    },
    {
      "epoch": 0.13861386138613863,
      "grad_norm": 0.3322896659374237,
      "learning_rate": 4.6190761847630476e-05,
      "loss": 0.2451,
      "step": 770
    },
    {
      "epoch": 0.1404140414041404,
      "grad_norm": 0.3615323007106781,
      "learning_rate": 4.679064187162568e-05,
      "loss": 0.25,
      "step": 780
    },
    {
      "epoch": 0.1422142214221422,
      "grad_norm": 0.3946974575519562,
      "learning_rate": 4.7390521895620875e-05,
      "loss": 0.2518,
      "step": 790
    },
    {
      "epoch": 0.14401440144014402,
      "grad_norm": 0.3781331479549408,
      "learning_rate": 4.799040191961608e-05,
      "loss": 0.2507,
      "step": 800
    },
    {
      "epoch": 0.14581458145814583,
      "grad_norm": 0.35483571887016296,
      "learning_rate": 4.859028194361128e-05,
      "loss": 0.2414,
      "step": 810
    },
    {
      "epoch": 0.1476147614761476,
      "grad_norm": 0.34985390305519104,
      "learning_rate": 4.919016196760648e-05,
      "loss": 0.238,
      "step": 820
    },
    {
      "epoch": 0.1494149414941494,
      "grad_norm": 0.34842243790626526,
      "learning_rate": 4.979004199160168e-05,
      "loss": 0.2478,
      "step": 830
    },
    {
      "epoch": 0.15121512151215122,
      "grad_norm": 0.35104289650917053,
      "learning_rate": 5.038992201559688e-05,
      "loss": 0.2585,
      "step": 840
    },
    {
      "epoch": 0.15301530153015303,
      "grad_norm": 0.34902697801589966,
      "learning_rate": 5.0989802039592085e-05,
      "loss": 0.2542,
      "step": 850
    },
    {
      "epoch": 0.1548154815481548,
      "grad_norm": 0.3519550859928131,
      "learning_rate": 5.158968206358729e-05,
      "loss": 0.2471,
      "step": 860
    },
    {
      "epoch": 0.1566156615661566,
      "grad_norm": 0.3343985080718994,
      "learning_rate": 5.218956208758249e-05,
      "loss": 0.2416,
      "step": 870
    },
    {
      "epoch": 0.15841584158415842,
      "grad_norm": 0.3293175995349884,
      "learning_rate": 5.278944211157769e-05,
      "loss": 0.2488,
      "step": 880
    },
    {
      "epoch": 0.16021602160216022,
      "grad_norm": 0.3182697594165802,
      "learning_rate": 5.338932213557288e-05,
      "loss": 0.2374,
      "step": 890
    },
    {
      "epoch": 0.162016201620162,
      "grad_norm": 0.4135315716266632,
      "learning_rate": 5.3989202159568085e-05,
      "loss": 0.2541,
      "step": 900
    },
    {
      "epoch": 0.1638163816381638,
      "grad_norm": 0.3215128183364868,
      "learning_rate": 5.458908218356329e-05,
      "loss": 0.2434,
      "step": 910
    },
    {
      "epoch": 0.16561656165616562,
      "grad_norm": 0.3220362067222595,
      "learning_rate": 5.518896220755849e-05,
      "loss": 0.2409,
      "step": 920
    },
    {
      "epoch": 0.16741674167416742,
      "grad_norm": 0.3379752039909363,
      "learning_rate": 5.578884223155369e-05,
      "loss": 0.2416,
      "step": 930
    },
    {
      "epoch": 0.1692169216921692,
      "grad_norm": 0.33060601353645325,
      "learning_rate": 5.6388722255548896e-05,
      "loss": 0.2367,
      "step": 940
    },
    {
      "epoch": 0.171017101710171,
      "grad_norm": 0.2988029718399048,
      "learning_rate": 5.698860227954409e-05,
      "loss": 0.2486,
      "step": 950
    },
    {
      "epoch": 0.17281728172817282,
      "grad_norm": 0.3071451485157013,
      "learning_rate": 5.7588482303539295e-05,
      "loss": 0.24,
      "step": 960
    },
    {
      "epoch": 0.17461746174617462,
      "grad_norm": 0.3413279950618744,
      "learning_rate": 5.81883623275345e-05,
      "loss": 0.2503,
      "step": 970
    },
    {
      "epoch": 0.17641764176417643,
      "grad_norm": 0.339693158864975,
      "learning_rate": 5.87882423515297e-05,
      "loss": 0.239,
      "step": 980
    },
    {
      "epoch": 0.1782178217821782,
      "grad_norm": 0.3433760106563568,
      "learning_rate": 5.93881223755249e-05,
      "loss": 0.2408,
      "step": 990
    },
    {
      "epoch": 0.18001800180018002,
      "grad_norm": 0.3255997598171234,
      "learning_rate": 5.998800239952009e-05,
      "loss": 0.2403,
      "step": 1000
    },
    {
      "epoch": 0.18001800180018002,
      "eval_loss": 0.24217712879180908,
      "eval_runtime": 316.6745,
      "eval_samples_per_second": 124.734,
      "eval_steps_per_second": 3.9,
      "step": 1000
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 0.3237132430076599,
      "learning_rate": 6.0587882423515295e-05,
      "loss": 0.2521,
      "step": 1010
    },
    {
      "epoch": 0.18361836183618363,
      "grad_norm": 0.31490692496299744,
      "learning_rate": 6.11877624475105e-05,
      "loss": 0.2376,
      "step": 1020
    },
    {
      "epoch": 0.1854185418541854,
      "grad_norm": 0.3275722563266754,
      "learning_rate": 6.178764247150571e-05,
      "loss": 0.2508,
      "step": 1030
    },
    {
      "epoch": 0.18721872187218722,
      "grad_norm": 0.3383451998233795,
      "learning_rate": 6.238752249550091e-05,
      "loss": 0.2425,
      "step": 1040
    },
    {
      "epoch": 0.18901890189018902,
      "grad_norm": 0.2780287563800812,
      "learning_rate": 6.298740251949611e-05,
      "loss": 0.2449,
      "step": 1050
    },
    {
      "epoch": 0.19081908190819083,
      "grad_norm": 0.36391913890838623,
      "learning_rate": 6.35872825434913e-05,
      "loss": 0.2376,
      "step": 1060
    },
    {
      "epoch": 0.1926192619261926,
      "grad_norm": 0.31692343950271606,
      "learning_rate": 6.41871625674865e-05,
      "loss": 0.2292,
      "step": 1070
    },
    {
      "epoch": 0.19441944194419442,
      "grad_norm": 0.28668802976608276,
      "learning_rate": 6.478704259148171e-05,
      "loss": 0.2313,
      "step": 1080
    },
    {
      "epoch": 0.19621962196219622,
      "grad_norm": 0.3369818329811096,
      "learning_rate": 6.538692261547691e-05,
      "loss": 0.2411,
      "step": 1090
    },
    {
      "epoch": 0.19801980198019803,
      "grad_norm": 0.3234890401363373,
      "learning_rate": 6.598680263947211e-05,
      "loss": 0.2416,
      "step": 1100
    },
    {
      "epoch": 0.1998199819981998,
      "grad_norm": 0.3045274615287781,
      "learning_rate": 6.658668266346732e-05,
      "loss": 0.2355,
      "step": 1110
    },
    {
      "epoch": 0.20162016201620162,
      "grad_norm": 0.32221919298171997,
      "learning_rate": 6.71865626874625e-05,
      "loss": 0.2344,
      "step": 1120
    },
    {
      "epoch": 0.20342034203420342,
      "grad_norm": 0.32345613837242126,
      "learning_rate": 6.778644271145771e-05,
      "loss": 0.2391,
      "step": 1130
    },
    {
      "epoch": 0.20522052205220523,
      "grad_norm": 0.305133193731308,
      "learning_rate": 6.838632273545291e-05,
      "loss": 0.2363,
      "step": 1140
    },
    {
      "epoch": 0.207020702070207,
      "grad_norm": 0.2937854826450348,
      "learning_rate": 6.898620275944811e-05,
      "loss": 0.2373,
      "step": 1150
    },
    {
      "epoch": 0.20882088208820881,
      "grad_norm": 0.2911037802696228,
      "learning_rate": 6.958608278344332e-05,
      "loss": 0.2385,
      "step": 1160
    },
    {
      "epoch": 0.21062106210621062,
      "grad_norm": 0.3210409879684448,
      "learning_rate": 7.018596280743852e-05,
      "loss": 0.2426,
      "step": 1170
    },
    {
      "epoch": 0.21242124212421243,
      "grad_norm": 0.3020002841949463,
      "learning_rate": 7.078584283143371e-05,
      "loss": 0.2312,
      "step": 1180
    },
    {
      "epoch": 0.21422142214221424,
      "grad_norm": 0.26574328541755676,
      "learning_rate": 7.138572285542891e-05,
      "loss": 0.2379,
      "step": 1190
    },
    {
      "epoch": 0.21602160216021601,
      "grad_norm": 0.29705411195755005,
      "learning_rate": 7.198560287942411e-05,
      "loss": 0.229,
      "step": 1200
    },
    {
      "epoch": 0.21782178217821782,
      "grad_norm": 0.3083536922931671,
      "learning_rate": 7.258548290341932e-05,
      "loss": 0.2429,
      "step": 1210
    },
    {
      "epoch": 0.21962196219621963,
      "grad_norm": 0.28074610233306885,
      "learning_rate": 7.318536292741452e-05,
      "loss": 0.2357,
      "step": 1220
    },
    {
      "epoch": 0.22142214221422143,
      "grad_norm": 0.2670040726661682,
      "learning_rate": 7.378524295140972e-05,
      "loss": 0.2381,
      "step": 1230
    },
    {
      "epoch": 0.2232223222322232,
      "grad_norm": 0.26997634768486023,
      "learning_rate": 7.438512297540492e-05,
      "loss": 0.2381,
      "step": 1240
    },
    {
      "epoch": 0.22502250225022502,
      "grad_norm": 0.28783535957336426,
      "learning_rate": 7.498500299940013e-05,
      "loss": 0.246,
      "step": 1250
    },
    {
      "epoch": 0.22682268226822683,
      "grad_norm": 0.2848479747772217,
      "learning_rate": 7.558488302339533e-05,
      "loss": 0.2399,
      "step": 1260
    },
    {
      "epoch": 0.22862286228622863,
      "grad_norm": 0.28767073154449463,
      "learning_rate": 7.618476304739053e-05,
      "loss": 0.2366,
      "step": 1270
    },
    {
      "epoch": 0.2304230423042304,
      "grad_norm": 0.28846070170402527,
      "learning_rate": 7.678464307138574e-05,
      "loss": 0.239,
      "step": 1280
    },
    {
      "epoch": 0.23222322232223222,
      "grad_norm": 0.27859318256378174,
      "learning_rate": 7.738452309538092e-05,
      "loss": 0.2248,
      "step": 1290
    },
    {
      "epoch": 0.23402340234023403,
      "grad_norm": 0.2588232457637787,
      "learning_rate": 7.798440311937613e-05,
      "loss": 0.2323,
      "step": 1300
    },
    {
      "epoch": 0.23582358235823583,
      "grad_norm": 0.2822347581386566,
      "learning_rate": 7.858428314337133e-05,
      "loss": 0.2354,
      "step": 1310
    },
    {
      "epoch": 0.2376237623762376,
      "grad_norm": 0.2870301306247711,
      "learning_rate": 7.918416316736653e-05,
      "loss": 0.2348,
      "step": 1320
    },
    {
      "epoch": 0.23942394239423942,
      "grad_norm": 0.2847054600715637,
      "learning_rate": 7.978404319136174e-05,
      "loss": 0.2231,
      "step": 1330
    },
    {
      "epoch": 0.24122412241224123,
      "grad_norm": 0.3215648829936981,
      "learning_rate": 8.038392321535694e-05,
      "loss": 0.2403,
      "step": 1340
    },
    {
      "epoch": 0.24302430243024303,
      "grad_norm": 0.27841004729270935,
      "learning_rate": 8.098380323935213e-05,
      "loss": 0.2275,
      "step": 1350
    },
    {
      "epoch": 0.2448244824482448,
      "grad_norm": 0.2551121115684509,
      "learning_rate": 8.158368326334733e-05,
      "loss": 0.2342,
      "step": 1360
    },
    {
      "epoch": 0.24662466246624662,
      "grad_norm": 0.3087143003940582,
      "learning_rate": 8.218356328734253e-05,
      "loss": 0.2344,
      "step": 1370
    },
    {
      "epoch": 0.24842484248424843,
      "grad_norm": 0.2705787718296051,
      "learning_rate": 8.278344331133774e-05,
      "loss": 0.2374,
      "step": 1380
    },
    {
      "epoch": 0.2502250225022502,
      "grad_norm": 0.2669019401073456,
      "learning_rate": 8.338332333533294e-05,
      "loss": 0.2319,
      "step": 1390
    },
    {
      "epoch": 0.252025202520252,
      "grad_norm": 0.2958507239818573,
      "learning_rate": 8.398320335932813e-05,
      "loss": 0.232,
      "step": 1400
    },
    {
      "epoch": 0.2538253825382538,
      "grad_norm": 0.2673927843570709,
      "learning_rate": 8.458308338332333e-05,
      "loss": 0.2334,
      "step": 1410
    },
    {
      "epoch": 0.2556255625562556,
      "grad_norm": 0.29347890615463257,
      "learning_rate": 8.518296340731853e-05,
      "loss": 0.231,
      "step": 1420
    },
    {
      "epoch": 0.25742574257425743,
      "grad_norm": 0.28088194131851196,
      "learning_rate": 8.578284343131374e-05,
      "loss": 0.2308,
      "step": 1430
    },
    {
      "epoch": 0.25922592259225924,
      "grad_norm": 0.27257800102233887,
      "learning_rate": 8.638272345530894e-05,
      "loss": 0.223,
      "step": 1440
    },
    {
      "epoch": 0.26102610261026105,
      "grad_norm": 0.27098777890205383,
      "learning_rate": 8.698260347930414e-05,
      "loss": 0.2295,
      "step": 1450
    },
    {
      "epoch": 0.26282628262826285,
      "grad_norm": 0.29519346356391907,
      "learning_rate": 8.758248350329934e-05,
      "loss": 0.233,
      "step": 1460
    },
    {
      "epoch": 0.2646264626462646,
      "grad_norm": 0.28033971786499023,
      "learning_rate": 8.818236352729455e-05,
      "loss": 0.2311,
      "step": 1470
    },
    {
      "epoch": 0.2664266426642664,
      "grad_norm": 0.26876452565193176,
      "learning_rate": 8.878224355128975e-05,
      "loss": 0.2261,
      "step": 1480
    },
    {
      "epoch": 0.2682268226822682,
      "grad_norm": 0.2660025358200073,
      "learning_rate": 8.938212357528495e-05,
      "loss": 0.2368,
      "step": 1490
    },
    {
      "epoch": 0.27002700270027,
      "grad_norm": 0.2709440588951111,
      "learning_rate": 8.998200359928016e-05,
      "loss": 0.237,
      "step": 1500
    },
    {
      "epoch": 0.27002700270027,
      "eval_loss": 0.2313167005777359,
      "eval_runtime": 316.739,
      "eval_samples_per_second": 124.708,
      "eval_steps_per_second": 3.899,
      "step": 1500
    },
    {
      "epoch": 0.27182718271827183,
      "grad_norm": 0.2770077884197235,
      "learning_rate": 9.058188362327536e-05,
      "loss": 0.2285,
      "step": 1510
    },
    {
      "epoch": 0.27362736273627364,
      "grad_norm": 0.27214711904525757,
      "learning_rate": 9.118176364727055e-05,
      "loss": 0.2358,
      "step": 1520
    },
    {
      "epoch": 0.27542754275427545,
      "grad_norm": 0.2870901823043823,
      "learning_rate": 9.178164367126575e-05,
      "loss": 0.2292,
      "step": 1530
    },
    {
      "epoch": 0.27722772277227725,
      "grad_norm": 0.27588629722595215,
      "learning_rate": 9.238152369526095e-05,
      "loss": 0.2325,
      "step": 1540
    },
    {
      "epoch": 0.279027902790279,
      "grad_norm": 0.28376758098602295,
      "learning_rate": 9.298140371925616e-05,
      "loss": 0.2228,
      "step": 1550
    },
    {
      "epoch": 0.2808280828082808,
      "grad_norm": 0.32071658968925476,
      "learning_rate": 9.358128374325136e-05,
      "loss": 0.2354,
      "step": 1560
    },
    {
      "epoch": 0.2826282628262826,
      "grad_norm": 0.28409451246261597,
      "learning_rate": 9.418116376724655e-05,
      "loss": 0.2272,
      "step": 1570
    },
    {
      "epoch": 0.2844284428442844,
      "grad_norm": 0.2800748348236084,
      "learning_rate": 9.478104379124175e-05,
      "loss": 0.2333,
      "step": 1580
    },
    {
      "epoch": 0.28622862286228623,
      "grad_norm": 0.28557470440864563,
      "learning_rate": 9.538092381523695e-05,
      "loss": 0.2282,
      "step": 1590
    },
    {
      "epoch": 0.28802880288028804,
      "grad_norm": 0.27082133293151855,
      "learning_rate": 9.598080383923216e-05,
      "loss": 0.2286,
      "step": 1600
    },
    {
      "epoch": 0.28982898289828984,
      "grad_norm": 0.23685045540332794,
      "learning_rate": 9.658068386322736e-05,
      "loss": 0.2221,
      "step": 1610
    },
    {
      "epoch": 0.29162916291629165,
      "grad_norm": 0.2607126533985138,
      "learning_rate": 9.718056388722256e-05,
      "loss": 0.2275,
      "step": 1620
    },
    {
      "epoch": 0.2934293429342934,
      "grad_norm": 0.27857154607772827,
      "learning_rate": 9.778044391121775e-05,
      "loss": 0.2287,
      "step": 1630
    },
    {
      "epoch": 0.2952295229522952,
      "grad_norm": 0.3034259080886841,
      "learning_rate": 9.838032393521295e-05,
      "loss": 0.232,
      "step": 1640
    },
    {
      "epoch": 0.297029702970297,
      "grad_norm": 0.2528027594089508,
      "learning_rate": 9.898020395920816e-05,
      "loss": 0.2298,
      "step": 1650
    },
    {
      "epoch": 0.2988298829882988,
      "grad_norm": 0.31934231519699097,
      "learning_rate": 9.958008398320336e-05,
      "loss": 0.2217,
      "step": 1660
    },
    {
      "epoch": 0.30063006300630063,
      "grad_norm": 0.28170856833457947,
      "learning_rate": 9.999999012776351e-05,
      "loss": 0.2255,
      "step": 1670
    },
    {
      "epoch": 0.30243024302430244,
      "grad_norm": 0.27932703495025635,
      "learning_rate": 9.999981462144535e-05,
      "loss": 0.2331,
      "step": 1680
    },
    {
      "epoch": 0.30423042304230424,
      "grad_norm": 0.25786229968070984,
      "learning_rate": 9.999941973298028e-05,
      "loss": 0.2241,
      "step": 1690
    },
    {
      "epoch": 0.30603060306030605,
      "grad_norm": 0.28402572870254517,
      "learning_rate": 9.999880546410095e-05,
      "loss": 0.2248,
      "step": 1700
    },
    {
      "epoch": 0.30783078307830786,
      "grad_norm": 0.25323182344436646,
      "learning_rate": 9.999797181750257e-05,
      "loss": 0.2229,
      "step": 1710
    },
    {
      "epoch": 0.3096309630963096,
      "grad_norm": 0.27878430485725403,
      "learning_rate": 9.999691879684287e-05,
      "loss": 0.2267,
      "step": 1720
    },
    {
      "epoch": 0.3114311431143114,
      "grad_norm": 0.27214643359184265,
      "learning_rate": 9.99956464067422e-05,
      "loss": 0.232,
      "step": 1730
    },
    {
      "epoch": 0.3132313231323132,
      "grad_norm": 0.27472788095474243,
      "learning_rate": 9.99941546527833e-05,
      "loss": 0.2312,
      "step": 1740
    },
    {
      "epoch": 0.31503150315031503,
      "grad_norm": 0.27812302112579346,
      "learning_rate": 9.999244354151154e-05,
      "loss": 0.231,
      "step": 1750
    },
    {
      "epoch": 0.31683168316831684,
      "grad_norm": 0.2517860233783722,
      "learning_rate": 9.999051308043466e-05,
      "loss": 0.225,
      "step": 1760
    },
    {
      "epoch": 0.31863186318631864,
      "grad_norm": 0.2553010582923889,
      "learning_rate": 9.998836327802286e-05,
      "loss": 0.2182,
      "step": 1770
    },
    {
      "epoch": 0.32043204320432045,
      "grad_norm": 0.26192453503608704,
      "learning_rate": 9.998599414370877e-05,
      "loss": 0.2284,
      "step": 1780
    },
    {
      "epoch": 0.32223222322232226,
      "grad_norm": 0.26427459716796875,
      "learning_rate": 9.998340568788733e-05,
      "loss": 0.221,
      "step": 1790
    },
    {
      "epoch": 0.324032403240324,
      "grad_norm": 0.2640756070613861,
      "learning_rate": 9.998059792191581e-05,
      "loss": 0.2293,
      "step": 1800
    },
    {
      "epoch": 0.3258325832583258,
      "grad_norm": 0.27761542797088623,
      "learning_rate": 9.99775708581137e-05,
      "loss": 0.2269,
      "step": 1810
    },
    {
      "epoch": 0.3276327632763276,
      "grad_norm": 0.29036062955856323,
      "learning_rate": 9.997432450976276e-05,
      "loss": 0.2297,
      "step": 1820
    },
    {
      "epoch": 0.32943294329432943,
      "grad_norm": 0.29958391189575195,
      "learning_rate": 9.997085889110684e-05,
      "loss": 0.2306,
      "step": 1830
    },
    {
      "epoch": 0.33123312331233123,
      "grad_norm": 0.2935033142566681,
      "learning_rate": 9.996717401735189e-05,
      "loss": 0.2275,
      "step": 1840
    },
    {
      "epoch": 0.33303330333033304,
      "grad_norm": 0.25690290331840515,
      "learning_rate": 9.996326990466591e-05,
      "loss": 0.2315,
      "step": 1850
    },
    {
      "epoch": 0.33483348334833485,
      "grad_norm": 0.2981838583946228,
      "learning_rate": 9.995914657017877e-05,
      "loss": 0.224,
      "step": 1860
    },
    {
      "epoch": 0.33663366336633666,
      "grad_norm": 0.26208075881004333,
      "learning_rate": 9.995480403198231e-05,
      "loss": 0.2279,
      "step": 1870
    },
    {
      "epoch": 0.3384338433843384,
      "grad_norm": 0.2750791907310486,
      "learning_rate": 9.995024230913005e-05,
      "loss": 0.2347,
      "step": 1880
    },
    {
      "epoch": 0.3402340234023402,
      "grad_norm": 0.24720622599124908,
      "learning_rate": 9.994546142163733e-05,
      "loss": 0.2281,
      "step": 1890
    },
    {
      "epoch": 0.342034203420342,
      "grad_norm": 0.2544737458229065,
      "learning_rate": 9.994046139048102e-05,
      "loss": 0.2205,
      "step": 1900
    },
    {
      "epoch": 0.3438343834383438,
      "grad_norm": 0.2690906524658203,
      "learning_rate": 9.993524223759957e-05,
      "loss": 0.2418,
      "step": 1910
    },
    {
      "epoch": 0.34563456345634563,
      "grad_norm": 0.23540109395980835,
      "learning_rate": 9.992980398589282e-05,
      "loss": 0.2299,
      "step": 1920
    },
    {
      "epoch": 0.34743474347434744,
      "grad_norm": 0.23287588357925415,
      "learning_rate": 9.992414665922201e-05,
      "loss": 0.2267,
      "step": 1930
    },
    {
      "epoch": 0.34923492349234925,
      "grad_norm": 0.2687840163707733,
      "learning_rate": 9.991827028240949e-05,
      "loss": 0.2192,
      "step": 1940
    },
    {
      "epoch": 0.35103510351035105,
      "grad_norm": 0.27197232842445374,
      "learning_rate": 9.991217488123887e-05,
      "loss": 0.2227,
      "step": 1950
    },
    {
      "epoch": 0.35283528352835286,
      "grad_norm": 0.26791131496429443,
      "learning_rate": 9.990586048245467e-05,
      "loss": 0.2208,
      "step": 1960
    },
    {
      "epoch": 0.3546354635463546,
      "grad_norm": 0.2575250267982483,
      "learning_rate": 9.98993271137623e-05,
      "loss": 0.2231,
      "step": 1970
    },
    {
      "epoch": 0.3564356435643564,
      "grad_norm": 0.24592310190200806,
      "learning_rate": 9.989257480382799e-05,
      "loss": 0.2259,
      "step": 1980
    },
    {
      "epoch": 0.3582358235823582,
      "grad_norm": 0.2355838418006897,
      "learning_rate": 9.988560358227854e-05,
      "loss": 0.2283,
      "step": 1990
    },
    {
      "epoch": 0.36003600360036003,
      "grad_norm": 0.26246750354766846,
      "learning_rate": 9.987841347970132e-05,
      "loss": 0.2206,
      "step": 2000
    },
    {
      "epoch": 0.36003600360036003,
      "eval_loss": 0.223710298538208,
      "eval_runtime": 316.7268,
      "eval_samples_per_second": 124.713,
      "eval_steps_per_second": 3.899,
      "step": 2000
    },
    {
      "epoch": 0.36183618361836184,
      "grad_norm": 0.2537144124507904,
      "learning_rate": 9.987100452764403e-05,
      "loss": 0.2311,
      "step": 2010
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 0.25574520230293274,
      "learning_rate": 9.986337675861464e-05,
      "loss": 0.225,
      "step": 2020
    },
    {
      "epoch": 0.36543654365436545,
      "grad_norm": 0.2514253258705139,
      "learning_rate": 9.98555302060812e-05,
      "loss": 0.2184,
      "step": 2030
    },
    {
      "epoch": 0.36723672367236726,
      "grad_norm": 0.24544960260391235,
      "learning_rate": 9.98474649044717e-05,
      "loss": 0.2234,
      "step": 2040
    },
    {
      "epoch": 0.369036903690369,
      "grad_norm": 0.23743936419487,
      "learning_rate": 9.983918088917395e-05,
      "loss": 0.222,
      "step": 2050
    },
    {
      "epoch": 0.3708370837083708,
      "grad_norm": 0.2586144804954529,
      "learning_rate": 9.983067819653536e-05,
      "loss": 0.2265,
      "step": 2060
    },
    {
      "epoch": 0.3726372637263726,
      "grad_norm": 0.26042240858078003,
      "learning_rate": 9.982195686386286e-05,
      "loss": 0.2268,
      "step": 2070
    },
    {
      "epoch": 0.37443744374437443,
      "grad_norm": 0.2678053081035614,
      "learning_rate": 9.981301692942271e-05,
      "loss": 0.2225,
      "step": 2080
    },
    {
      "epoch": 0.37623762376237624,
      "grad_norm": 0.23515863716602325,
      "learning_rate": 9.980385843244026e-05,
      "loss": 0.2216,
      "step": 2090
    },
    {
      "epoch": 0.37803780378037805,
      "grad_norm": 0.23945562541484833,
      "learning_rate": 9.979448141309991e-05,
      "loss": 0.2217,
      "step": 2100
    },
    {
      "epoch": 0.37983798379837985,
      "grad_norm": 0.23530229926109314,
      "learning_rate": 9.978488591254477e-05,
      "loss": 0.221,
      "step": 2110
    },
    {
      "epoch": 0.38163816381638166,
      "grad_norm": 0.25200438499450684,
      "learning_rate": 9.97750719728767e-05,
      "loss": 0.2161,
      "step": 2120
    },
    {
      "epoch": 0.38343834383438347,
      "grad_norm": 0.2574562728404999,
      "learning_rate": 9.976503963715587e-05,
      "loss": 0.2148,
      "step": 2130
    },
    {
      "epoch": 0.3852385238523852,
      "grad_norm": 0.2390211969614029,
      "learning_rate": 9.975478894940076e-05,
      "loss": 0.2262,
      "step": 2140
    },
    {
      "epoch": 0.387038703870387,
      "grad_norm": 0.27864697575569153,
      "learning_rate": 9.974431995458789e-05,
      "loss": 0.2275,
      "step": 2150
    },
    {
      "epoch": 0.38883888388838883,
      "grad_norm": 0.2537368834018707,
      "learning_rate": 9.973363269865165e-05,
      "loss": 0.2215,
      "step": 2160
    },
    {
      "epoch": 0.39063906390639064,
      "grad_norm": 0.22219201922416687,
      "learning_rate": 9.972272722848408e-05,
      "loss": 0.2125,
      "step": 2170
    },
    {
      "epoch": 0.39243924392439244,
      "grad_norm": 0.24325935542583466,
      "learning_rate": 9.971160359193466e-05,
      "loss": 0.2208,
      "step": 2180
    },
    {
      "epoch": 0.39423942394239425,
      "grad_norm": 0.2233138382434845,
      "learning_rate": 9.970026183781013e-05,
      "loss": 0.219,
      "step": 2190
    },
    {
      "epoch": 0.39603960396039606,
      "grad_norm": 0.24517716467380524,
      "learning_rate": 9.968870201587422e-05,
      "loss": 0.2145,
      "step": 2200
    },
    {
      "epoch": 0.39783978397839787,
      "grad_norm": 0.26965415477752686,
      "learning_rate": 9.96769241768475e-05,
      "loss": 0.2229,
      "step": 2210
    },
    {
      "epoch": 0.3996399639963996,
      "grad_norm": 0.24545322358608246,
      "learning_rate": 9.966492837240713e-05,
      "loss": 0.2293,
      "step": 2220
    },
    {
      "epoch": 0.4014401440144014,
      "grad_norm": 0.25347474217414856,
      "learning_rate": 9.965271465518658e-05,
      "loss": 0.2201,
      "step": 2230
    },
    {
      "epoch": 0.40324032403240323,
      "grad_norm": 0.2585768699645996,
      "learning_rate": 9.96402830787755e-05,
      "loss": 0.2142,
      "step": 2240
    },
    {
      "epoch": 0.40504050405040504,
      "grad_norm": 0.2522677183151245,
      "learning_rate": 9.962763369771941e-05,
      "loss": 0.2295,
      "step": 2250
    },
    {
      "epoch": 0.40684068406840684,
      "grad_norm": 0.2655642032623291,
      "learning_rate": 9.961476656751948e-05,
      "loss": 0.217,
      "step": 2260
    },
    {
      "epoch": 0.40864086408640865,
      "grad_norm": 0.25372374057769775,
      "learning_rate": 9.960168174463227e-05,
      "loss": 0.2242,
      "step": 2270
    },
    {
      "epoch": 0.41044104410441046,
      "grad_norm": 0.24343790113925934,
      "learning_rate": 9.958837928646955e-05,
      "loss": 0.222,
      "step": 2280
    },
    {
      "epoch": 0.41224122412241226,
      "grad_norm": 0.2507123649120331,
      "learning_rate": 9.957485925139799e-05,
      "loss": 0.2215,
      "step": 2290
    },
    {
      "epoch": 0.414041404140414,
      "grad_norm": 0.2486102133989334,
      "learning_rate": 9.956112169873886e-05,
      "loss": 0.2195,
      "step": 2300
    },
    {
      "epoch": 0.4158415841584158,
      "grad_norm": 0.25398188829421997,
      "learning_rate": 9.954716668876786e-05,
      "loss": 0.2175,
      "step": 2310
    },
    {
      "epoch": 0.41764176417641763,
      "grad_norm": 0.24960486590862274,
      "learning_rate": 9.953299428271484e-05,
      "loss": 0.2199,
      "step": 2320
    },
    {
      "epoch": 0.41944194419441944,
      "grad_norm": 0.21711131930351257,
      "learning_rate": 9.95186045427635e-05,
      "loss": 0.2192,
      "step": 2330
    },
    {
      "epoch": 0.42124212421242124,
      "grad_norm": 0.2641173303127289,
      "learning_rate": 9.950399753205106e-05,
      "loss": 0.2139,
      "step": 2340
    },
    {
      "epoch": 0.42304230423042305,
      "grad_norm": 0.24927592277526855,
      "learning_rate": 9.948917331466815e-05,
      "loss": 0.2177,
      "step": 2350
    },
    {
      "epoch": 0.42484248424842486,
      "grad_norm": 0.23509396612644196,
      "learning_rate": 9.947413195565837e-05,
      "loss": 0.2202,
      "step": 2360
    },
    {
      "epoch": 0.42664266426642666,
      "grad_norm": 0.2371230125427246,
      "learning_rate": 9.94588735210181e-05,
      "loss": 0.2188,
      "step": 2370
    },
    {
      "epoch": 0.42844284428442847,
      "grad_norm": 0.23997090756893158,
      "learning_rate": 9.944339807769612e-05,
      "loss": 0.2131,
      "step": 2380
    },
    {
      "epoch": 0.4302430243024302,
      "grad_norm": 0.2586556673049927,
      "learning_rate": 9.942770569359341e-05,
      "loss": 0.2163,
      "step": 2390
    },
    {
      "epoch": 0.43204320432043203,
      "grad_norm": 0.2490890920162201,
      "learning_rate": 9.941179643756279e-05,
      "loss": 0.2124,
      "step": 2400
    },
    {
      "epoch": 0.43384338433843384,
      "grad_norm": 0.24167722463607788,
      "learning_rate": 9.93956703794087e-05,
      "loss": 0.2218,
      "step": 2410
    },
    {
      "epoch": 0.43564356435643564,
      "grad_norm": 0.22996798157691956,
      "learning_rate": 9.937932758988675e-05,
      "loss": 0.2152,
      "step": 2420
    },
    {
      "epoch": 0.43744374437443745,
      "grad_norm": 0.27105507254600525,
      "learning_rate": 9.936276814070354e-05,
      "loss": 0.2067,
      "step": 2430
    },
    {
      "epoch": 0.43924392439243926,
      "grad_norm": 0.23687943816184998,
      "learning_rate": 9.934599210451627e-05,
      "loss": 0.2205,
      "step": 2440
    },
    {
      "epoch": 0.44104410441044106,
      "grad_norm": 0.22914555668830872,
      "learning_rate": 9.93289995549325e-05,
      "loss": 0.2183,
      "step": 2450
    },
    {
      "epoch": 0.44284428442844287,
      "grad_norm": 0.26036006212234497,
      "learning_rate": 9.931179056650973e-05,
      "loss": 0.2172,
      "step": 2460
    },
    {
      "epoch": 0.4446444644464446,
      "grad_norm": 0.24683235585689545,
      "learning_rate": 9.929436521475514e-05,
      "loss": 0.2192,
      "step": 2470
    },
    {
      "epoch": 0.4464446444644464,
      "grad_norm": 0.2654927670955658,
      "learning_rate": 9.927672357612523e-05,
      "loss": 0.2181,
      "step": 2480
    },
    {
      "epoch": 0.44824482448244823,
      "grad_norm": 0.2445693016052246,
      "learning_rate": 9.925886572802551e-05,
      "loss": 0.2162,
      "step": 2490
    },
    {
      "epoch": 0.45004500450045004,
      "grad_norm": 0.22371965646743774,
      "learning_rate": 9.924079174881014e-05,
      "loss": 0.2176,
      "step": 2500
    },
    {
      "epoch": 0.45004500450045004,
      "eval_loss": 0.21701721847057343,
      "eval_runtime": 316.8164,
      "eval_samples_per_second": 124.678,
      "eval_steps_per_second": 3.898,
      "step": 2500
    },
    {
      "epoch": 0.45184518451845185,
      "grad_norm": 0.23449668288230896,
      "learning_rate": 9.922250171778156e-05,
      "loss": 0.2124,
      "step": 2510
    },
    {
      "epoch": 0.45364536453645365,
      "grad_norm": 0.24857591092586517,
      "learning_rate": 9.92039957151902e-05,
      "loss": 0.2175,
      "step": 2520
    },
    {
      "epoch": 0.45544554455445546,
      "grad_norm": 0.25581657886505127,
      "learning_rate": 9.918527382223409e-05,
      "loss": 0.2208,
      "step": 2530
    },
    {
      "epoch": 0.45724572457245727,
      "grad_norm": 0.23570847511291504,
      "learning_rate": 9.91663361210585e-05,
      "loss": 0.2116,
      "step": 2540
    },
    {
      "epoch": 0.459045904590459,
      "grad_norm": 0.24531251192092896,
      "learning_rate": 9.914718269475565e-05,
      "loss": 0.2145,
      "step": 2550
    },
    {
      "epoch": 0.4608460846084608,
      "grad_norm": 0.2686268985271454,
      "learning_rate": 9.912781362736421e-05,
      "loss": 0.2133,
      "step": 2560
    },
    {
      "epoch": 0.46264626462646263,
      "grad_norm": 0.23374398052692413,
      "learning_rate": 9.910822900386906e-05,
      "loss": 0.214,
      "step": 2570
    },
    {
      "epoch": 0.46444644464446444,
      "grad_norm": 0.25247329473495483,
      "learning_rate": 9.908842891020086e-05,
      "loss": 0.2185,
      "step": 2580
    },
    {
      "epoch": 0.46624662466246625,
      "grad_norm": 0.26151832938194275,
      "learning_rate": 9.906841343323568e-05,
      "loss": 0.2088,
      "step": 2590
    },
    {
      "epoch": 0.46804680468046805,
      "grad_norm": 0.2571202218532562,
      "learning_rate": 9.904818266079459e-05,
      "loss": 0.2173,
      "step": 2600
    },
    {
      "epoch": 0.46984698469846986,
      "grad_norm": 0.24051345884799957,
      "learning_rate": 9.902773668164334e-05,
      "loss": 0.2174,
      "step": 2610
    },
    {
      "epoch": 0.47164716471647167,
      "grad_norm": 0.240805521607399,
      "learning_rate": 9.900707558549191e-05,
      "loss": 0.2142,
      "step": 2620
    },
    {
      "epoch": 0.4734473447344735,
      "grad_norm": 0.2345510572195053,
      "learning_rate": 9.898619946299414e-05,
      "loss": 0.2139,
      "step": 2630
    },
    {
      "epoch": 0.4752475247524752,
      "grad_norm": 0.27843087911605835,
      "learning_rate": 9.896510840574738e-05,
      "loss": 0.2137,
      "step": 2640
    },
    {
      "epoch": 0.47704770477047703,
      "grad_norm": 0.226681649684906,
      "learning_rate": 9.894380250629195e-05,
      "loss": 0.2184,
      "step": 2650
    },
    {
      "epoch": 0.47884788478847884,
      "grad_norm": 0.2570042908191681,
      "learning_rate": 9.892228185811089e-05,
      "loss": 0.2101,
      "step": 2660
    },
    {
      "epoch": 0.48064806480648065,
      "grad_norm": 0.2479577362537384,
      "learning_rate": 9.890054655562951e-05,
      "loss": 0.2111,
      "step": 2670
    },
    {
      "epoch": 0.48244824482448245,
      "grad_norm": 0.2679947018623352,
      "learning_rate": 9.887859669421487e-05,
      "loss": 0.217,
      "step": 2680
    },
    {
      "epoch": 0.48424842484248426,
      "grad_norm": 0.2199818193912506,
      "learning_rate": 9.88564323701755e-05,
      "loss": 0.2101,
      "step": 2690
    },
    {
      "epoch": 0.48604860486048607,
      "grad_norm": 0.24746304750442505,
      "learning_rate": 9.883405368076088e-05,
      "loss": 0.2133,
      "step": 2700
    },
    {
      "epoch": 0.4878487848784879,
      "grad_norm": 0.26449674367904663,
      "learning_rate": 9.881146072416109e-05,
      "loss": 0.2127,
      "step": 2710
    },
    {
      "epoch": 0.4896489648964896,
      "grad_norm": 0.24275057017803192,
      "learning_rate": 9.878865359950632e-05,
      "loss": 0.2069,
      "step": 2720
    },
    {
      "epoch": 0.49144914491449143,
      "grad_norm": 0.23177023231983185,
      "learning_rate": 9.876563240686646e-05,
      "loss": 0.2211,
      "step": 2730
    },
    {
      "epoch": 0.49324932493249324,
      "grad_norm": 0.25169867277145386,
      "learning_rate": 9.874239724725065e-05,
      "loss": 0.2097,
      "step": 2740
    },
    {
      "epoch": 0.49504950495049505,
      "grad_norm": 0.2850533425807953,
      "learning_rate": 9.871894822260688e-05,
      "loss": 0.2191,
      "step": 2750
    },
    {
      "epoch": 0.49684968496849685,
      "grad_norm": 0.2404979169368744,
      "learning_rate": 9.869528543582144e-05,
      "loss": 0.2172,
      "step": 2760
    },
    {
      "epoch": 0.49864986498649866,
      "grad_norm": 0.26478737592697144,
      "learning_rate": 9.867140899071859e-05,
      "loss": 0.2063,
      "step": 2770
    },
    {
      "epoch": 0.5004500450045004,
      "grad_norm": 0.2462010383605957,
      "learning_rate": 9.864731899206004e-05,
      "loss": 0.2111,
      "step": 2780
    },
    {
      "epoch": 0.5022502250225023,
      "grad_norm": 0.23906828463077545,
      "learning_rate": 9.86230155455445e-05,
      "loss": 0.2088,
      "step": 2790
    },
    {
      "epoch": 0.504050405040504,
      "grad_norm": 0.22404444217681885,
      "learning_rate": 9.85984987578072e-05,
      "loss": 0.2133,
      "step": 2800
    },
    {
      "epoch": 0.5058505850585059,
      "grad_norm": 0.2274763286113739,
      "learning_rate": 9.857376873641943e-05,
      "loss": 0.2108,
      "step": 2810
    },
    {
      "epoch": 0.5076507650765076,
      "grad_norm": 0.2331087440252304,
      "learning_rate": 9.854882558988815e-05,
      "loss": 0.2125,
      "step": 2820
    },
    {
      "epoch": 0.5094509450945095,
      "grad_norm": 0.26106613874435425,
      "learning_rate": 9.852366942765534e-05,
      "loss": 0.2157,
      "step": 2830
    },
    {
      "epoch": 0.5112511251125113,
      "grad_norm": 0.24179741740226746,
      "learning_rate": 9.849830036009768e-05,
      "loss": 0.2161,
      "step": 2840
    },
    {
      "epoch": 0.513051305130513,
      "grad_norm": 0.2262210249900818,
      "learning_rate": 9.847271849852599e-05,
      "loss": 0.2097,
      "step": 2850
    },
    {
      "epoch": 0.5148514851485149,
      "grad_norm": 0.24393846094608307,
      "learning_rate": 9.844692395518477e-05,
      "loss": 0.2114,
      "step": 2860
    },
    {
      "epoch": 0.5166516651665166,
      "grad_norm": 0.25844648480415344,
      "learning_rate": 9.842091684325168e-05,
      "loss": 0.2087,
      "step": 2870
    },
    {
      "epoch": 0.5184518451845185,
      "grad_norm": 0.22355547547340393,
      "learning_rate": 9.839469727683705e-05,
      "loss": 0.2164,
      "step": 2880
    },
    {
      "epoch": 0.5202520252025202,
      "grad_norm": 0.2595849335193634,
      "learning_rate": 9.836826537098342e-05,
      "loss": 0.2147,
      "step": 2890
    },
    {
      "epoch": 0.5220522052205221,
      "grad_norm": 0.2303878515958786,
      "learning_rate": 9.834162124166499e-05,
      "loss": 0.2136,
      "step": 2900
    },
    {
      "epoch": 0.5238523852385238,
      "grad_norm": 0.2602064311504364,
      "learning_rate": 9.83147650057871e-05,
      "loss": 0.2181,
      "step": 2910
    },
    {
      "epoch": 0.5256525652565257,
      "grad_norm": 0.25662630796432495,
      "learning_rate": 9.828769678118578e-05,
      "loss": 0.2148,
      "step": 2920
    },
    {
      "epoch": 0.5274527452745275,
      "grad_norm": 0.22981633245944977,
      "learning_rate": 9.826041668662715e-05,
      "loss": 0.2075,
      "step": 2930
    },
    {
      "epoch": 0.5292529252925292,
      "grad_norm": 0.2448118031024933,
      "learning_rate": 9.8232924841807e-05,
      "loss": 0.2148,
      "step": 2940
    },
    {
      "epoch": 0.5310531053105311,
      "grad_norm": 0.24901655316352844,
      "learning_rate": 9.820522136735015e-05,
      "loss": 0.2147,
      "step": 2950
    },
    {
      "epoch": 0.5328532853285328,
      "grad_norm": 0.23731693625450134,
      "learning_rate": 9.817730638481002e-05,
      "loss": 0.2134,
      "step": 2960
    },
    {
      "epoch": 0.5346534653465347,
      "grad_norm": 0.2552334666252136,
      "learning_rate": 9.814918001666802e-05,
      "loss": 0.2136,
      "step": 2970
    },
    {
      "epoch": 0.5364536453645364,
      "grad_norm": 0.24413378536701202,
      "learning_rate": 9.81208423863331e-05,
      "loss": 0.2079,
      "step": 2980
    },
    {
      "epoch": 0.5382538253825383,
      "grad_norm": 0.21073882281780243,
      "learning_rate": 9.80922936181411e-05,
      "loss": 0.2089,
      "step": 2990
    },
    {
      "epoch": 0.54005400540054,
      "grad_norm": 0.22997449338436127,
      "learning_rate": 9.80635338373543e-05,
      "loss": 0.2123,
      "step": 3000
    },
    {
      "epoch": 0.54005400540054,
      "eval_loss": 0.21293717622756958,
      "eval_runtime": 316.7601,
      "eval_samples_per_second": 124.7,
      "eval_steps_per_second": 3.899,
      "step": 3000
    },
    {
      "epoch": 0.5418541854185418,
      "grad_norm": 0.2304617166519165,
      "learning_rate": 9.803456317016079e-05,
      "loss": 0.2151,
      "step": 3010
    },
    {
      "epoch": 0.5436543654365437,
      "grad_norm": 0.24742980301380157,
      "learning_rate": 9.8005381743674e-05,
      "loss": 0.2104,
      "step": 3020
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 0.516119658946991,
      "learning_rate": 9.797598968593211e-05,
      "loss": 0.198,
      "step": 3030
    },
    {
      "epoch": 0.5472547254725473,
      "grad_norm": 0.2393358200788498,
      "learning_rate": 9.794638712589742e-05,
      "loss": 0.2157,
      "step": 3040
    },
    {
      "epoch": 0.549054905490549,
      "grad_norm": 0.23356829583644867,
      "learning_rate": 9.791657419345589e-05,
      "loss": 0.2063,
      "step": 3050
    },
    {
      "epoch": 0.5508550855085509,
      "grad_norm": 0.2521313726902008,
      "learning_rate": 9.78865510194165e-05,
      "loss": 0.216,
      "step": 3060
    },
    {
      "epoch": 0.5526552655265526,
      "grad_norm": 0.2588180899620056,
      "learning_rate": 9.785631773551072e-05,
      "loss": 0.2149,
      "step": 3070
    },
    {
      "epoch": 0.5544554455445545,
      "grad_norm": 0.25503742694854736,
      "learning_rate": 9.782587447439187e-05,
      "loss": 0.2163,
      "step": 3080
    },
    {
      "epoch": 0.5562556255625563,
      "grad_norm": 0.24655157327651978,
      "learning_rate": 9.779522136963464e-05,
      "loss": 0.2124,
      "step": 3090
    },
    {
      "epoch": 0.558055805580558,
      "grad_norm": 0.224728524684906,
      "learning_rate": 9.776435855573437e-05,
      "loss": 0.212,
      "step": 3100
    },
    {
      "epoch": 0.5598559855985599,
      "grad_norm": 0.2219727486371994,
      "learning_rate": 9.77332861681066e-05,
      "loss": 0.2067,
      "step": 3110
    },
    {
      "epoch": 0.5616561656165616,
      "grad_norm": 0.2079688310623169,
      "learning_rate": 9.770200434308635e-05,
      "loss": 0.203,
      "step": 3120
    },
    {
      "epoch": 0.5634563456345635,
      "grad_norm": 0.2543124556541443,
      "learning_rate": 9.767051321792763e-05,
      "loss": 0.2138,
      "step": 3130
    },
    {
      "epoch": 0.5652565256525652,
      "grad_norm": 0.2526974380016327,
      "learning_rate": 9.763881293080277e-05,
      "loss": 0.2144,
      "step": 3140
    },
    {
      "epoch": 0.5670567056705671,
      "grad_norm": 0.25811222195625305,
      "learning_rate": 9.76069036208018e-05,
      "loss": 0.2169,
      "step": 3150
    },
    {
      "epoch": 0.5688568856885688,
      "grad_norm": 0.2649664878845215,
      "learning_rate": 9.757478542793192e-05,
      "loss": 0.2157,
      "step": 3160
    },
    {
      "epoch": 0.5706570657065707,
      "grad_norm": 0.22160731256008148,
      "learning_rate": 9.75424584931168e-05,
      "loss": 0.2035,
      "step": 3170
    },
    {
      "epoch": 0.5724572457245725,
      "grad_norm": 0.22985722124576569,
      "learning_rate": 9.750992295819603e-05,
      "loss": 0.2106,
      "step": 3180
    },
    {
      "epoch": 0.5742574257425742,
      "grad_norm": 0.23979565501213074,
      "learning_rate": 9.747717896592443e-05,
      "loss": 0.2092,
      "step": 3190
    },
    {
      "epoch": 0.5760576057605761,
      "grad_norm": 0.2651786208152771,
      "learning_rate": 9.744422665997148e-05,
      "loss": 0.2097,
      "step": 3200
    },
    {
      "epoch": 0.5778577857785778,
      "grad_norm": 0.22866341471672058,
      "learning_rate": 9.74110661849207e-05,
      "loss": 0.215,
      "step": 3210
    },
    {
      "epoch": 0.5796579657965797,
      "grad_norm": 0.23052644729614258,
      "learning_rate": 9.737769768626892e-05,
      "loss": 0.2104,
      "step": 3220
    },
    {
      "epoch": 0.5814581458145814,
      "grad_norm": 0.23537978529930115,
      "learning_rate": 9.734412131042574e-05,
      "loss": 0.207,
      "step": 3230
    },
    {
      "epoch": 0.5832583258325833,
      "grad_norm": 0.23548007011413574,
      "learning_rate": 9.731033720471287e-05,
      "loss": 0.2061,
      "step": 3240
    },
    {
      "epoch": 0.585058505850585,
      "grad_norm": 0.23433077335357666,
      "learning_rate": 9.727634551736347e-05,
      "loss": 0.2054,
      "step": 3250
    },
    {
      "epoch": 0.5868586858685868,
      "grad_norm": 0.24446631968021393,
      "learning_rate": 9.724214639752143e-05,
      "loss": 0.2063,
      "step": 3260
    },
    {
      "epoch": 0.5886588658865887,
      "grad_norm": 0.24442283809185028,
      "learning_rate": 9.720773999524087e-05,
      "loss": 0.2157,
      "step": 3270
    },
    {
      "epoch": 0.5904590459045904,
      "grad_norm": 0.24321478605270386,
      "learning_rate": 9.717312646148535e-05,
      "loss": 0.2129,
      "step": 3280
    },
    {
      "epoch": 0.5922592259225923,
      "grad_norm": 0.22498056292533875,
      "learning_rate": 9.713830594812726e-05,
      "loss": 0.2054,
      "step": 3290
    },
    {
      "epoch": 0.594059405940594,
      "grad_norm": 0.23608307540416718,
      "learning_rate": 9.710327860794715e-05,
      "loss": 0.2048,
      "step": 3300
    },
    {
      "epoch": 0.5958595859585959,
      "grad_norm": 0.23400336503982544,
      "learning_rate": 9.706804459463304e-05,
      "loss": 0.2058,
      "step": 3310
    },
    {
      "epoch": 0.5976597659765976,
      "grad_norm": 0.20508521795272827,
      "learning_rate": 9.703260406277978e-05,
      "loss": 0.2023,
      "step": 3320
    },
    {
      "epoch": 0.5994599459945995,
      "grad_norm": 0.24868269264698029,
      "learning_rate": 9.699695716788834e-05,
      "loss": 0.2163,
      "step": 3330
    },
    {
      "epoch": 0.6012601260126013,
      "grad_norm": 0.21883746981620789,
      "learning_rate": 9.696110406636517e-05,
      "loss": 0.2037,
      "step": 3340
    },
    {
      "epoch": 0.603060306030603,
      "grad_norm": 0.24629046022891998,
      "learning_rate": 9.692504491552141e-05,
      "loss": 0.2041,
      "step": 3350
    },
    {
      "epoch": 0.6048604860486049,
      "grad_norm": 0.2192843109369278,
      "learning_rate": 9.688877987357236e-05,
      "loss": 0.2096,
      "step": 3360
    },
    {
      "epoch": 0.6066606660666066,
      "grad_norm": 0.23917289078235626,
      "learning_rate": 9.68523090996367e-05,
      "loss": 0.207,
      "step": 3370
    },
    {
      "epoch": 0.6084608460846085,
      "grad_norm": 0.23239530622959137,
      "learning_rate": 9.681563275373568e-05,
      "loss": 0.2063,
      "step": 3380
    },
    {
      "epoch": 0.6102610261026102,
      "grad_norm": 0.2358783483505249,
      "learning_rate": 9.677875099679268e-05,
      "loss": 0.207,
      "step": 3390
    },
    {
      "epoch": 0.6120612061206121,
      "grad_norm": 0.23683814704418182,
      "learning_rate": 9.674166399063223e-05,
      "loss": 0.206,
      "step": 3400
    },
    {
      "epoch": 0.6138613861386139,
      "grad_norm": 0.24462026357650757,
      "learning_rate": 9.670437189797947e-05,
      "loss": 0.2127,
      "step": 3410
    },
    {
      "epoch": 0.6156615661566157,
      "grad_norm": 0.23222774267196655,
      "learning_rate": 9.666687488245945e-05,
      "loss": 0.2051,
      "step": 3420
    },
    {
      "epoch": 0.6174617461746175,
      "grad_norm": 0.24366077780723572,
      "learning_rate": 9.662917310859624e-05,
      "loss": 0.2197,
      "step": 3430
    },
    {
      "epoch": 0.6192619261926192,
      "grad_norm": 0.21413196623325348,
      "learning_rate": 9.659126674181239e-05,
      "loss": 0.2096,
      "step": 3440
    },
    {
      "epoch": 0.6210621062106211,
      "grad_norm": 0.26152798533439636,
      "learning_rate": 9.655315594842813e-05,
      "loss": 0.2097,
      "step": 3450
    },
    {
      "epoch": 0.6228622862286228,
      "grad_norm": 0.24256277084350586,
      "learning_rate": 9.651484089566063e-05,
      "loss": 0.2085,
      "step": 3460
    },
    {
      "epoch": 0.6246624662466247,
      "grad_norm": 0.2187686711549759,
      "learning_rate": 9.647632175162329e-05,
      "loss": 0.2184,
      "step": 3470
    },
    {
      "epoch": 0.6264626462646264,
      "grad_norm": 0.2674267292022705,
      "learning_rate": 9.643759868532496e-05,
      "loss": 0.2167,
      "step": 3480
    },
    {
      "epoch": 0.6282628262826283,
      "grad_norm": 0.23166342079639435,
      "learning_rate": 9.639867186666928e-05,
      "loss": 0.2053,
      "step": 3490
    },
    {
      "epoch": 0.6300630063006301,
      "grad_norm": 0.23329995572566986,
      "learning_rate": 9.635954146645387e-05,
      "loss": 0.2043,
      "step": 3500
    },
    {
      "epoch": 0.6300630063006301,
      "eval_loss": 0.20939837396144867,
      "eval_runtime": 316.6645,
      "eval_samples_per_second": 124.738,
      "eval_steps_per_second": 3.9,
      "step": 3500
    },
    {
      "epoch": 0.6318631863186318,
      "grad_norm": 0.23624008893966675,
      "learning_rate": 9.632020765636957e-05,
      "loss": 0.2084,
      "step": 3510
    },
    {
      "epoch": 0.6336633663366337,
      "grad_norm": 0.2378198802471161,
      "learning_rate": 9.628067060899976e-05,
      "loss": 0.202,
      "step": 3520
    },
    {
      "epoch": 0.6354635463546354,
      "grad_norm": 0.23220627009868622,
      "learning_rate": 9.624093049781952e-05,
      "loss": 0.214,
      "step": 3530
    },
    {
      "epoch": 0.6372637263726373,
      "grad_norm": 0.22800645232200623,
      "learning_rate": 9.620098749719489e-05,
      "loss": 0.2054,
      "step": 3540
    },
    {
      "epoch": 0.639063906390639,
      "grad_norm": 0.22299763560295105,
      "learning_rate": 9.616084178238218e-05,
      "loss": 0.207,
      "step": 3550
    },
    {
      "epoch": 0.6408640864086409,
      "grad_norm": 0.24686147272586823,
      "learning_rate": 9.612049352952708e-05,
      "loss": 0.2125,
      "step": 3560
    },
    {
      "epoch": 0.6426642664266426,
      "grad_norm": 0.2612927258014679,
      "learning_rate": 9.607994291566398e-05,
      "loss": 0.2128,
      "step": 3570
    },
    {
      "epoch": 0.6444644464446445,
      "grad_norm": 0.2308240830898285,
      "learning_rate": 9.603919011871516e-05,
      "loss": 0.2036,
      "step": 3580
    },
    {
      "epoch": 0.6462646264626463,
      "grad_norm": 0.26948824524879456,
      "learning_rate": 9.599823531748999e-05,
      "loss": 0.2149,
      "step": 3590
    },
    {
      "epoch": 0.648064806480648,
      "grad_norm": 0.26148882508277893,
      "learning_rate": 9.595707869168422e-05,
      "loss": 0.2037,
      "step": 3600
    },
    {
      "epoch": 0.6498649864986499,
      "grad_norm": 0.21003858745098114,
      "learning_rate": 9.591572042187905e-05,
      "loss": 0.2054,
      "step": 3610
    },
    {
      "epoch": 0.6516651665166516,
      "grad_norm": 0.24826359748840332,
      "learning_rate": 9.587416068954049e-05,
      "loss": 0.2071,
      "step": 3620
    },
    {
      "epoch": 0.6534653465346535,
      "grad_norm": 0.23808856308460236,
      "learning_rate": 9.583239967701849e-05,
      "loss": 0.2106,
      "step": 3630
    },
    {
      "epoch": 0.6552655265526552,
      "grad_norm": 0.22825875878334045,
      "learning_rate": 9.579043756754614e-05,
      "loss": 0.2098,
      "step": 3640
    },
    {
      "epoch": 0.6570657065706571,
      "grad_norm": 0.2592133581638336,
      "learning_rate": 9.574827454523887e-05,
      "loss": 0.2048,
      "step": 3650
    },
    {
      "epoch": 0.6588658865886589,
      "grad_norm": 0.22834962606430054,
      "learning_rate": 9.570591079509366e-05,
      "loss": 0.1986,
      "step": 3660
    },
    {
      "epoch": 0.6606660666066607,
      "grad_norm": 0.22950910031795502,
      "learning_rate": 9.56633465029882e-05,
      "loss": 0.2082,
      "step": 3670
    },
    {
      "epoch": 0.6624662466246625,
      "grad_norm": 0.24665984511375427,
      "learning_rate": 9.56205818556801e-05,
      "loss": 0.209,
      "step": 3680
    },
    {
      "epoch": 0.6642664266426642,
      "grad_norm": 0.2195744514465332,
      "learning_rate": 9.557761704080606e-05,
      "loss": 0.2003,
      "step": 3690
    },
    {
      "epoch": 0.6660666066606661,
      "grad_norm": 0.22199589014053345,
      "learning_rate": 9.553445224688104e-05,
      "loss": 0.2109,
      "step": 3700
    },
    {
      "epoch": 0.6678667866786678,
      "grad_norm": 0.23590803146362305,
      "learning_rate": 9.549108766329748e-05,
      "loss": 0.2089,
      "step": 3710
    },
    {
      "epoch": 0.6696669666966697,
      "grad_norm": 0.23883379995822906,
      "learning_rate": 9.544752348032432e-05,
      "loss": 0.2045,
      "step": 3720
    },
    {
      "epoch": 0.6714671467146714,
      "grad_norm": 0.2331724613904953,
      "learning_rate": 9.540375988910644e-05,
      "loss": 0.2166,
      "step": 3730
    },
    {
      "epoch": 0.6732673267326733,
      "grad_norm": 0.23776279389858246,
      "learning_rate": 9.53597970816635e-05,
      "loss": 0.2047,
      "step": 3740
    },
    {
      "epoch": 0.6750675067506751,
      "grad_norm": 0.22620773315429688,
      "learning_rate": 9.531563525088932e-05,
      "loss": 0.211,
      "step": 3750
    },
    {
      "epoch": 0.6768676867686768,
      "grad_norm": 0.23019500076770782,
      "learning_rate": 9.527127459055098e-05,
      "loss": 0.2086,
      "step": 3760
    },
    {
      "epoch": 0.6786678667866787,
      "grad_norm": 0.22121918201446533,
      "learning_rate": 9.522671529528795e-05,
      "loss": 0.2056,
      "step": 3770
    },
    {
      "epoch": 0.6804680468046804,
      "grad_norm": 0.23837274312973022,
      "learning_rate": 9.518195756061122e-05,
      "loss": 0.2163,
      "step": 3780
    },
    {
      "epoch": 0.6822682268226823,
      "grad_norm": 0.22653600573539734,
      "learning_rate": 9.513700158290246e-05,
      "loss": 0.2054,
      "step": 3790
    },
    {
      "epoch": 0.684068406840684,
      "grad_norm": 0.2548377811908722,
      "learning_rate": 9.509184755941321e-05,
      "loss": 0.2128,
      "step": 3800
    },
    {
      "epoch": 0.6858685868586859,
      "grad_norm": 0.2513207793235779,
      "learning_rate": 9.504649568826392e-05,
      "loss": 0.2043,
      "step": 3810
    },
    {
      "epoch": 0.6876687668766877,
      "grad_norm": 0.21407152712345123,
      "learning_rate": 9.500094616844315e-05,
      "loss": 0.1962,
      "step": 3820
    },
    {
      "epoch": 0.6894689468946895,
      "grad_norm": 0.24092818796634674,
      "learning_rate": 9.495519919980666e-05,
      "loss": 0.2079,
      "step": 3830
    },
    {
      "epoch": 0.6912691269126913,
      "grad_norm": 0.21626578271389008,
      "learning_rate": 9.490925498307657e-05,
      "loss": 0.2057,
      "step": 3840
    },
    {
      "epoch": 0.693069306930693,
      "grad_norm": 0.22738076746463776,
      "learning_rate": 9.486311371984045e-05,
      "loss": 0.21,
      "step": 3850
    },
    {
      "epoch": 0.6948694869486949,
      "grad_norm": 0.23304280638694763,
      "learning_rate": 9.481677561255042e-05,
      "loss": 0.2022,
      "step": 3860
    },
    {
      "epoch": 0.6966696669666966,
      "grad_norm": 0.238111212849617,
      "learning_rate": 9.477024086452231e-05,
      "loss": 0.2006,
      "step": 3870
    },
    {
      "epoch": 0.6984698469846985,
      "grad_norm": 0.25205790996551514,
      "learning_rate": 9.472350967993473e-05,
      "loss": 0.2075,
      "step": 3880
    },
    {
      "epoch": 0.7002700270027002,
      "grad_norm": 0.23783251643180847,
      "learning_rate": 9.467658226382822e-05,
      "loss": 0.2082,
      "step": 3890
    },
    {
      "epoch": 0.7020702070207021,
      "grad_norm": 0.23737800121307373,
      "learning_rate": 9.462945882210426e-05,
      "loss": 0.2069,
      "step": 3900
    },
    {
      "epoch": 0.7038703870387039,
      "grad_norm": 0.24336326122283936,
      "learning_rate": 9.458213956152446e-05,
      "loss": 0.213,
      "step": 3910
    },
    {
      "epoch": 0.7056705670567057,
      "grad_norm": 0.2440769374370575,
      "learning_rate": 9.453462468970961e-05,
      "loss": 0.2059,
      "step": 3920
    },
    {
      "epoch": 0.7074707470747075,
      "grad_norm": 0.23939993977546692,
      "learning_rate": 9.448691441513877e-05,
      "loss": 0.2121,
      "step": 3930
    },
    {
      "epoch": 0.7092709270927092,
      "grad_norm": 0.24011321365833282,
      "learning_rate": 9.443900894714838e-05,
      "loss": 0.2106,
      "step": 3940
    },
    {
      "epoch": 0.7110711071107111,
      "grad_norm": 0.2655102014541626,
      "learning_rate": 9.439090849593131e-05,
      "loss": 0.2092,
      "step": 3950
    },
    {
      "epoch": 0.7128712871287128,
      "grad_norm": 0.24547246098518372,
      "learning_rate": 9.434261327253593e-05,
      "loss": 0.2112,
      "step": 3960
    },
    {
      "epoch": 0.7146714671467147,
      "grad_norm": 0.24898996949195862,
      "learning_rate": 9.429412348886521e-05,
      "loss": 0.2073,
      "step": 3970
    },
    {
      "epoch": 0.7164716471647165,
      "grad_norm": 0.23754185438156128,
      "learning_rate": 9.424543935767584e-05,
      "loss": 0.2131,
      "step": 3980
    },
    {
      "epoch": 0.7182718271827183,
      "grad_norm": 0.24194017052650452,
      "learning_rate": 9.419656109257715e-05,
      "loss": 0.2001,
      "step": 3990
    },
    {
      "epoch": 0.7200720072007201,
      "grad_norm": 0.23280216753482819,
      "learning_rate": 9.414748890803034e-05,
      "loss": 0.2049,
      "step": 4000
    },
    {
      "epoch": 0.7200720072007201,
      "eval_loss": 0.2067154198884964,
      "eval_runtime": 316.7752,
      "eval_samples_per_second": 124.694,
      "eval_steps_per_second": 3.899,
      "step": 4000
    },
    {
      "epoch": 0.7218721872187218,
      "grad_norm": 0.22471250593662262,
      "learning_rate": 9.409822301934743e-05,
      "loss": 0.2087,
      "step": 4010
    },
    {
      "epoch": 0.7236723672367237,
      "grad_norm": 0.22987623512744904,
      "learning_rate": 9.404876364269033e-05,
      "loss": 0.2106,
      "step": 4020
    },
    {
      "epoch": 0.7254725472547254,
      "grad_norm": 0.23821160197257996,
      "learning_rate": 9.399911099506995e-05,
      "loss": 0.2107,
      "step": 4030
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 0.24363891780376434,
      "learning_rate": 9.394926529434519e-05,
      "loss": 0.2081,
      "step": 4040
    },
    {
      "epoch": 0.729072907290729,
      "grad_norm": 0.2459542602300644,
      "learning_rate": 9.389922675922196e-05,
      "loss": 0.1997,
      "step": 4050
    },
    {
      "epoch": 0.7308730873087309,
      "grad_norm": 0.2538906931877136,
      "learning_rate": 9.384899560925234e-05,
      "loss": 0.2037,
      "step": 4060
    },
    {
      "epoch": 0.7326732673267327,
      "grad_norm": 0.22958360612392426,
      "learning_rate": 9.379857206483346e-05,
      "loss": 0.203,
      "step": 4070
    },
    {
      "epoch": 0.7344734473447345,
      "grad_norm": 0.2241031527519226,
      "learning_rate": 9.37479563472067e-05,
      "loss": 0.2014,
      "step": 4080
    },
    {
      "epoch": 0.7362736273627363,
      "grad_norm": 0.24287284910678864,
      "learning_rate": 9.369714867845653e-05,
      "loss": 0.2062,
      "step": 4090
    },
    {
      "epoch": 0.738073807380738,
      "grad_norm": 0.22298087179660797,
      "learning_rate": 9.364614928150972e-05,
      "loss": 0.2059,
      "step": 4100
    },
    {
      "epoch": 0.7398739873987399,
      "grad_norm": 0.22433818876743317,
      "learning_rate": 9.359495838013423e-05,
      "loss": 0.2084,
      "step": 4110
    },
    {
      "epoch": 0.7416741674167416,
      "grad_norm": 0.23716357350349426,
      "learning_rate": 9.354357619893829e-05,
      "loss": 0.199,
      "step": 4120
    },
    {
      "epoch": 0.7434743474347435,
      "grad_norm": 0.22843913733959198,
      "learning_rate": 9.34920029633694e-05,
      "loss": 0.2087,
      "step": 4130
    },
    {
      "epoch": 0.7452745274527453,
      "grad_norm": 0.2484230101108551,
      "learning_rate": 9.344023889971333e-05,
      "loss": 0.2148,
      "step": 4140
    },
    {
      "epoch": 0.7470747074707471,
      "grad_norm": 0.23069673776626587,
      "learning_rate": 9.338828423509317e-05,
      "loss": 0.2031,
      "step": 4150
    },
    {
      "epoch": 0.7488748874887489,
      "grad_norm": 0.23127132654190063,
      "learning_rate": 9.333613919746826e-05,
      "loss": 0.2046,
      "step": 4160
    },
    {
      "epoch": 0.7506750675067507,
      "grad_norm": 0.24298977851867676,
      "learning_rate": 9.328380401563331e-05,
      "loss": 0.2069,
      "step": 4170
    },
    {
      "epoch": 0.7524752475247525,
      "grad_norm": 0.22152893245220184,
      "learning_rate": 9.323127891921718e-05,
      "loss": 0.2033,
      "step": 4180
    },
    {
      "epoch": 0.7542754275427542,
      "grad_norm": 0.24840903282165527,
      "learning_rate": 9.317856413868215e-05,
      "loss": 0.2166,
      "step": 4190
    },
    {
      "epoch": 0.7560756075607561,
      "grad_norm": 0.26208052039146423,
      "learning_rate": 9.312565990532269e-05,
      "loss": 0.198,
      "step": 4200
    },
    {
      "epoch": 0.7578757875787578,
      "grad_norm": 0.2249654233455658,
      "learning_rate": 9.307256645126456e-05,
      "loss": 0.202,
      "step": 4210
    },
    {
      "epoch": 0.7596759675967597,
      "grad_norm": 0.2731562554836273,
      "learning_rate": 9.301928400946373e-05,
      "loss": 0.1953,
      "step": 4220
    },
    {
      "epoch": 0.7614761476147615,
      "grad_norm": 0.2380090057849884,
      "learning_rate": 9.29658128137054e-05,
      "loss": 0.2049,
      "step": 4230
    },
    {
      "epoch": 0.7632763276327633,
      "grad_norm": 0.2641489505767822,
      "learning_rate": 9.291215309860298e-05,
      "loss": 0.2104,
      "step": 4240
    },
    {
      "epoch": 0.7650765076507651,
      "grad_norm": 0.19881705939769745,
      "learning_rate": 9.285830509959695e-05,
      "loss": 0.2089,
      "step": 4250
    },
    {
      "epoch": 0.7668766876687669,
      "grad_norm": 0.23701301217079163,
      "learning_rate": 9.280426905295404e-05,
      "loss": 0.2053,
      "step": 4260
    },
    {
      "epoch": 0.7686768676867687,
      "grad_norm": 0.24661783874034882,
      "learning_rate": 9.275004519576597e-05,
      "loss": 0.2065,
      "step": 4270
    },
    {
      "epoch": 0.7704770477047704,
      "grad_norm": 0.20124076306819916,
      "learning_rate": 9.269563376594858e-05,
      "loss": 0.2045,
      "step": 4280
    },
    {
      "epoch": 0.7722772277227723,
      "grad_norm": 0.2237536907196045,
      "learning_rate": 9.264103500224064e-05,
      "loss": 0.1963,
      "step": 4290
    },
    {
      "epoch": 0.774077407740774,
      "grad_norm": 0.22383403778076172,
      "learning_rate": 9.258624914420296e-05,
      "loss": 0.2022,
      "step": 4300
    },
    {
      "epoch": 0.7758775877587759,
      "grad_norm": 0.22563587129116058,
      "learning_rate": 9.253127643221719e-05,
      "loss": 0.1941,
      "step": 4310
    },
    {
      "epoch": 0.7776777677767777,
      "grad_norm": 0.22393293678760529,
      "learning_rate": 9.247611710748486e-05,
      "loss": 0.2039,
      "step": 4320
    },
    {
      "epoch": 0.7794779477947795,
      "grad_norm": 0.24167528748512268,
      "learning_rate": 9.242077141202627e-05,
      "loss": 0.2091,
      "step": 4330
    },
    {
      "epoch": 0.7812781278127813,
      "grad_norm": 0.21461306512355804,
      "learning_rate": 9.23652395886795e-05,
      "loss": 0.2076,
      "step": 4340
    },
    {
      "epoch": 0.783078307830783,
      "grad_norm": 0.22967225313186646,
      "learning_rate": 9.230952188109928e-05,
      "loss": 0.2049,
      "step": 4350
    },
    {
      "epoch": 0.7848784878487849,
      "grad_norm": 0.22706805169582367,
      "learning_rate": 9.225361853375587e-05,
      "loss": 0.2014,
      "step": 4360
    },
    {
      "epoch": 0.7866786678667866,
      "grad_norm": 0.226139098405838,
      "learning_rate": 9.219752979193414e-05,
      "loss": 0.2038,
      "step": 4370
    },
    {
      "epoch": 0.7884788478847885,
      "grad_norm": 0.23592673242092133,
      "learning_rate": 9.214125590173236e-05,
      "loss": 0.1958,
      "step": 4380
    },
    {
      "epoch": 0.7902790279027903,
      "grad_norm": 0.2482786774635315,
      "learning_rate": 9.208479711006122e-05,
      "loss": 0.2022,
      "step": 4390
    },
    {
      "epoch": 0.7920792079207921,
      "grad_norm": 0.25264087319374084,
      "learning_rate": 9.202815366464261e-05,
      "loss": 0.2042,
      "step": 4400
    },
    {
      "epoch": 0.7938793879387939,
      "grad_norm": 0.2268824428319931,
      "learning_rate": 9.197132581400867e-05,
      "loss": 0.2044,
      "step": 4410
    },
    {
      "epoch": 0.7956795679567957,
      "grad_norm": 0.2456209510564804,
      "learning_rate": 9.191431380750066e-05,
      "loss": 0.2062,
      "step": 4420
    },
    {
      "epoch": 0.7974797479747975,
      "grad_norm": 0.2131347805261612,
      "learning_rate": 9.185711789526785e-05,
      "loss": 0.2024,
      "step": 4430
    },
    {
      "epoch": 0.7992799279927992,
      "grad_norm": 0.23927991092205048,
      "learning_rate": 9.179973832826637e-05,
      "loss": 0.2031,
      "step": 4440
    },
    {
      "epoch": 0.8010801080108011,
      "grad_norm": 0.21817928552627563,
      "learning_rate": 9.174217535825823e-05,
      "loss": 0.2028,
      "step": 4450
    },
    {
      "epoch": 0.8028802880288028,
      "grad_norm": 0.23236271739006042,
      "learning_rate": 9.168442923781013e-05,
      "loss": 0.2019,
      "step": 4460
    },
    {
      "epoch": 0.8046804680468047,
      "grad_norm": 0.23999303579330444,
      "learning_rate": 9.162650022029236e-05,
      "loss": 0.1998,
      "step": 4470
    },
    {
      "epoch": 0.8064806480648065,
      "grad_norm": 0.23286929726600647,
      "learning_rate": 9.156838855987769e-05,
      "loss": 0.2024,
      "step": 4480
    },
    {
      "epoch": 0.8082808280828083,
      "grad_norm": 0.21921072900295258,
      "learning_rate": 9.15100945115403e-05,
      "loss": 0.2013,
      "step": 4490
    },
    {
      "epoch": 0.8100810081008101,
      "grad_norm": 0.22894969582557678,
      "learning_rate": 9.145161833105461e-05,
      "loss": 0.2037,
      "step": 4500
    },
    {
      "epoch": 0.8100810081008101,
      "eval_loss": 0.20348024368286133,
      "eval_runtime": 316.783,
      "eval_samples_per_second": 124.691,
      "eval_steps_per_second": 3.899,
      "step": 4500
    },
    {
      "epoch": 0.8118811881188119,
      "grad_norm": 0.23053114116191864,
      "learning_rate": 9.139296027499416e-05,
      "loss": 0.2051,
      "step": 4510
    },
    {
      "epoch": 0.8136813681368137,
      "grad_norm": 0.22433440387248993,
      "learning_rate": 9.133412060073052e-05,
      "loss": 0.2032,
      "step": 4520
    },
    {
      "epoch": 0.8154815481548154,
      "grad_norm": 0.21147237718105316,
      "learning_rate": 9.127509956643213e-05,
      "loss": 0.2062,
      "step": 4530
    },
    {
      "epoch": 0.8172817281728173,
      "grad_norm": 0.23975451290607452,
      "learning_rate": 9.121589743106315e-05,
      "loss": 0.2049,
      "step": 4540
    },
    {
      "epoch": 0.819081908190819,
      "grad_norm": 0.2609047591686249,
      "learning_rate": 9.115651445438242e-05,
      "loss": 0.2077,
      "step": 4550
    },
    {
      "epoch": 0.8208820882088209,
      "grad_norm": 0.2259020209312439,
      "learning_rate": 9.109695089694217e-05,
      "loss": 0.1997,
      "step": 4560
    },
    {
      "epoch": 0.8226822682268227,
      "grad_norm": 0.22624799609184265,
      "learning_rate": 9.1037207020087e-05,
      "loss": 0.2006,
      "step": 4570
    },
    {
      "epoch": 0.8244824482448245,
      "grad_norm": 0.23482918739318848,
      "learning_rate": 9.097728308595268e-05,
      "loss": 0.1992,
      "step": 4580
    },
    {
      "epoch": 0.8262826282628263,
      "grad_norm": 0.22943072021007538,
      "learning_rate": 9.0917179357465e-05,
      "loss": 0.2034,
      "step": 4590
    },
    {
      "epoch": 0.828082808280828,
      "grad_norm": 0.2206421196460724,
      "learning_rate": 9.085689609833865e-05,
      "loss": 0.2012,
      "step": 4600
    },
    {
      "epoch": 0.8298829882988299,
      "grad_norm": 0.21219399571418762,
      "learning_rate": 9.079643357307603e-05,
      "loss": 0.1999,
      "step": 4610
    },
    {
      "epoch": 0.8316831683168316,
      "grad_norm": 0.2205626219511032,
      "learning_rate": 9.073579204696609e-05,
      "loss": 0.2016,
      "step": 4620
    },
    {
      "epoch": 0.8334833483348335,
      "grad_norm": 0.23510605096817017,
      "learning_rate": 9.067497178608317e-05,
      "loss": 0.2093,
      "step": 4630
    },
    {
      "epoch": 0.8352835283528353,
      "grad_norm": 0.21196968853473663,
      "learning_rate": 9.061397305728588e-05,
      "loss": 0.1866,
      "step": 4640
    },
    {
      "epoch": 0.8370837083708371,
      "grad_norm": 0.22035059332847595,
      "learning_rate": 9.055279612821582e-05,
      "loss": 0.2018,
      "step": 4650
    },
    {
      "epoch": 0.8388838883888389,
      "grad_norm": 0.22121018171310425,
      "learning_rate": 9.049144126729653e-05,
      "loss": 0.2033,
      "step": 4660
    },
    {
      "epoch": 0.8406840684068407,
      "grad_norm": 0.22482182085514069,
      "learning_rate": 9.042990874373221e-05,
      "loss": 0.2059,
      "step": 4670
    },
    {
      "epoch": 0.8424842484248425,
      "grad_norm": 0.24872317910194397,
      "learning_rate": 9.036819882750661e-05,
      "loss": 0.1942,
      "step": 4680
    },
    {
      "epoch": 0.8442844284428442,
      "grad_norm": 0.23382668197155,
      "learning_rate": 9.030631178938183e-05,
      "loss": 0.2002,
      "step": 4690
    },
    {
      "epoch": 0.8460846084608461,
      "grad_norm": 0.23061996698379517,
      "learning_rate": 9.024424790089707e-05,
      "loss": 0.201,
      "step": 4700
    },
    {
      "epoch": 0.8478847884788479,
      "grad_norm": 0.21398621797561646,
      "learning_rate": 9.018200743436753e-05,
      "loss": 0.2045,
      "step": 4710
    },
    {
      "epoch": 0.8496849684968497,
      "grad_norm": 0.24778422713279724,
      "learning_rate": 9.01195906628832e-05,
      "loss": 0.2088,
      "step": 4720
    },
    {
      "epoch": 0.8514851485148515,
      "grad_norm": 0.2234611064195633,
      "learning_rate": 9.005699786030757e-05,
      "loss": 0.1934,
      "step": 4730
    },
    {
      "epoch": 0.8532853285328533,
      "grad_norm": 0.23318789899349213,
      "learning_rate": 8.999422930127653e-05,
      "loss": 0.1994,
      "step": 4740
    },
    {
      "epoch": 0.8550855085508551,
      "grad_norm": 0.2610333263874054,
      "learning_rate": 8.993128526119711e-05,
      "loss": 0.2049,
      "step": 4750
    },
    {
      "epoch": 0.8568856885688569,
      "grad_norm": 0.23346495628356934,
      "learning_rate": 8.986816601624635e-05,
      "loss": 0.1955,
      "step": 4760
    },
    {
      "epoch": 0.8586858685868587,
      "grad_norm": 0.24535135924816132,
      "learning_rate": 8.980487184336997e-05,
      "loss": 0.2039,
      "step": 4770
    },
    {
      "epoch": 0.8604860486048604,
      "grad_norm": 0.25145038962364197,
      "learning_rate": 8.97414030202812e-05,
      "loss": 0.2044,
      "step": 4780
    },
    {
      "epoch": 0.8622862286228623,
      "grad_norm": 0.25232475996017456,
      "learning_rate": 8.967775982545963e-05,
      "loss": 0.206,
      "step": 4790
    },
    {
      "epoch": 0.8640864086408641,
      "grad_norm": 0.2068834900856018,
      "learning_rate": 8.961394253814992e-05,
      "loss": 0.1945,
      "step": 4800
    },
    {
      "epoch": 0.8658865886588659,
      "grad_norm": 0.20895422995090485,
      "learning_rate": 8.954995143836056e-05,
      "loss": 0.1926,
      "step": 4810
    },
    {
      "epoch": 0.8676867686768677,
      "grad_norm": 0.2502649426460266,
      "learning_rate": 8.948578680686268e-05,
      "loss": 0.1962,
      "step": 4820
    },
    {
      "epoch": 0.8694869486948695,
      "grad_norm": 0.21781963109970093,
      "learning_rate": 8.942144892518885e-05,
      "loss": 0.204,
      "step": 4830
    },
    {
      "epoch": 0.8712871287128713,
      "grad_norm": 0.21508321166038513,
      "learning_rate": 8.935693807563174e-05,
      "loss": 0.2031,
      "step": 4840
    },
    {
      "epoch": 0.873087308730873,
      "grad_norm": 0.2252320647239685,
      "learning_rate": 8.9292254541243e-05,
      "loss": 0.2094,
      "step": 4850
    },
    {
      "epoch": 0.8748874887488749,
      "grad_norm": 0.24418379366397858,
      "learning_rate": 8.922739860583189e-05,
      "loss": 0.2007,
      "step": 4860
    },
    {
      "epoch": 0.8766876687668766,
      "grad_norm": 0.23919308185577393,
      "learning_rate": 8.916237055396419e-05,
      "loss": 0.2006,
      "step": 4870
    },
    {
      "epoch": 0.8784878487848785,
      "grad_norm": 0.2438650280237198,
      "learning_rate": 8.909717067096082e-05,
      "loss": 0.2022,
      "step": 4880
    },
    {
      "epoch": 0.8802880288028803,
      "grad_norm": 0.2446826845407486,
      "learning_rate": 8.903179924289665e-05,
      "loss": 0.2016,
      "step": 4890
    },
    {
      "epoch": 0.8820882088208821,
      "grad_norm": 0.23306594789028168,
      "learning_rate": 8.896625655659921e-05,
      "loss": 0.2028,
      "step": 4900
    },
    {
      "epoch": 0.8838883888388839,
      "grad_norm": 0.24346444010734558,
      "learning_rate": 8.890054289964748e-05,
      "loss": 0.1971,
      "step": 4910
    },
    {
      "epoch": 0.8856885688568857,
      "grad_norm": 0.23788951337337494,
      "learning_rate": 8.883465856037064e-05,
      "loss": 0.2016,
      "step": 4920
    },
    {
      "epoch": 0.8874887488748875,
      "grad_norm": 0.21506880223751068,
      "learning_rate": 8.876860382784664e-05,
      "loss": 0.2047,
      "step": 4930
    },
    {
      "epoch": 0.8892889288928892,
      "grad_norm": 0.23275813460350037,
      "learning_rate": 8.870237899190117e-05,
      "loss": 0.2072,
      "step": 4940
    },
    {
      "epoch": 0.8910891089108911,
      "grad_norm": 0.21245914697647095,
      "learning_rate": 8.863598434310625e-05,
      "loss": 0.2065,
      "step": 4950
    },
    {
      "epoch": 0.8928892889288929,
      "grad_norm": 0.19870716333389282,
      "learning_rate": 8.856942017277896e-05,
      "loss": 0.2016,
      "step": 4960
    },
    {
      "epoch": 0.8946894689468947,
      "grad_norm": 0.25991544127464294,
      "learning_rate": 8.850268677298019e-05,
      "loss": 0.2003,
      "step": 4970
    },
    {
      "epoch": 0.8964896489648965,
      "grad_norm": 0.26473942399024963,
      "learning_rate": 8.843578443651338e-05,
      "loss": 0.2065,
      "step": 4980
    },
    {
      "epoch": 0.8982898289828983,
      "grad_norm": 0.2286319136619568,
      "learning_rate": 8.836871345692314e-05,
      "loss": 0.2028,
      "step": 4990
    },
    {
      "epoch": 0.9000900090009001,
      "grad_norm": 0.2224671095609665,
      "learning_rate": 8.830147412849412e-05,
      "loss": 0.1989,
      "step": 5000
    },
    {
      "epoch": 0.9000900090009001,
      "eval_loss": 0.20156477391719818,
      "eval_runtime": 316.5593,
      "eval_samples_per_second": 124.779,
      "eval_steps_per_second": 3.901,
      "step": 5000
    },
    {
      "epoch": 0.9018901890189019,
      "grad_norm": 0.22737281024456024,
      "learning_rate": 8.823406674624957e-05,
      "loss": 0.2052,
      "step": 5010
    },
    {
      "epoch": 0.9036903690369037,
      "grad_norm": 0.22713065147399902,
      "learning_rate": 8.816649160595006e-05,
      "loss": 0.2093,
      "step": 5020
    },
    {
      "epoch": 0.9054905490549054,
      "grad_norm": 0.23764027655124664,
      "learning_rate": 8.809874900409235e-05,
      "loss": 0.2029,
      "step": 5030
    },
    {
      "epoch": 0.9072907290729073,
      "grad_norm": 0.22733670473098755,
      "learning_rate": 8.803083923790783e-05,
      "loss": 0.2063,
      "step": 5040
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 0.2285180687904358,
      "learning_rate": 8.796276260536142e-05,
      "loss": 0.1979,
      "step": 5050
    },
    {
      "epoch": 0.9108910891089109,
      "grad_norm": 0.20729659497737885,
      "learning_rate": 8.789451940515022e-05,
      "loss": 0.1958,
      "step": 5060
    },
    {
      "epoch": 0.9126912691269127,
      "grad_norm": 0.24628256261348724,
      "learning_rate": 8.782610993670211e-05,
      "loss": 0.2006,
      "step": 5070
    },
    {
      "epoch": 0.9144914491449145,
      "grad_norm": 0.22549910843372345,
      "learning_rate": 8.775753450017451e-05,
      "loss": 0.2067,
      "step": 5080
    },
    {
      "epoch": 0.9162916291629163,
      "grad_norm": 0.24328148365020752,
      "learning_rate": 8.768879339645311e-05,
      "loss": 0.2065,
      "step": 5090
    },
    {
      "epoch": 0.918091809180918,
      "grad_norm": 0.23623110353946686,
      "learning_rate": 8.761988692715042e-05,
      "loss": 0.208,
      "step": 5100
    },
    {
      "epoch": 0.9198919891989199,
      "grad_norm": 0.2114161103963852,
      "learning_rate": 8.755081539460452e-05,
      "loss": 0.2006,
      "step": 5110
    },
    {
      "epoch": 0.9216921692169217,
      "grad_norm": 0.20341260731220245,
      "learning_rate": 8.748157910187779e-05,
      "loss": 0.2087,
      "step": 5120
    },
    {
      "epoch": 0.9234923492349235,
      "grad_norm": 0.21291473507881165,
      "learning_rate": 8.741217835275549e-05,
      "loss": 0.2008,
      "step": 5130
    },
    {
      "epoch": 0.9252925292529253,
      "grad_norm": 0.2388085275888443,
      "learning_rate": 8.734261345174443e-05,
      "loss": 0.2003,
      "step": 5140
    },
    {
      "epoch": 0.9270927092709271,
      "grad_norm": 0.23811635375022888,
      "learning_rate": 8.727288470407171e-05,
      "loss": 0.1993,
      "step": 5150
    },
    {
      "epoch": 0.9288928892889289,
      "grad_norm": 0.219865083694458,
      "learning_rate": 8.720299241568328e-05,
      "loss": 0.2019,
      "step": 5160
    },
    {
      "epoch": 0.9306930693069307,
      "grad_norm": 0.2213873565196991,
      "learning_rate": 8.713293689324273e-05,
      "loss": 0.2051,
      "step": 5170
    },
    {
      "epoch": 0.9324932493249325,
      "grad_norm": 0.22098961472511292,
      "learning_rate": 8.706271844412979e-05,
      "loss": 0.1958,
      "step": 5180
    },
    {
      "epoch": 0.9342934293429342,
      "grad_norm": 0.20789465308189392,
      "learning_rate": 8.699233737643907e-05,
      "loss": 0.2022,
      "step": 5190
    },
    {
      "epoch": 0.9360936093609361,
      "grad_norm": 0.2272273749113083,
      "learning_rate": 8.692179399897872e-05,
      "loss": 0.1983,
      "step": 5200
    },
    {
      "epoch": 0.9378937893789379,
      "grad_norm": 0.222502201795578,
      "learning_rate": 8.685108862126907e-05,
      "loss": 0.1865,
      "step": 5210
    },
    {
      "epoch": 0.9396939693969397,
      "grad_norm": 0.21560728549957275,
      "learning_rate": 8.67802215535412e-05,
      "loss": 0.2117,
      "step": 5220
    },
    {
      "epoch": 0.9414941494149415,
      "grad_norm": 0.2211407870054245,
      "learning_rate": 8.670919310673564e-05,
      "loss": 0.2045,
      "step": 5230
    },
    {
      "epoch": 0.9432943294329433,
      "grad_norm": 0.22916315495967865,
      "learning_rate": 8.6638003592501e-05,
      "loss": 0.1944,
      "step": 5240
    },
    {
      "epoch": 0.9450945094509451,
      "grad_norm": 0.2298874706029892,
      "learning_rate": 8.656665332319266e-05,
      "loss": 0.1969,
      "step": 5250
    },
    {
      "epoch": 0.946894689468947,
      "grad_norm": 0.2478565275669098,
      "learning_rate": 8.649514261187124e-05,
      "loss": 0.1962,
      "step": 5260
    },
    {
      "epoch": 0.9486948694869487,
      "grad_norm": 0.24320486187934875,
      "learning_rate": 8.642347177230137e-05,
      "loss": 0.1959,
      "step": 5270
    },
    {
      "epoch": 0.9504950495049505,
      "grad_norm": 0.22357295453548431,
      "learning_rate": 8.635164111895029e-05,
      "loss": 0.1996,
      "step": 5280
    },
    {
      "epoch": 0.9522952295229523,
      "grad_norm": 0.21875126659870148,
      "learning_rate": 8.627965096698643e-05,
      "loss": 0.2044,
      "step": 5290
    },
    {
      "epoch": 0.9540954095409541,
      "grad_norm": 0.24637655913829803,
      "learning_rate": 8.6207501632278e-05,
      "loss": 0.2003,
      "step": 5300
    },
    {
      "epoch": 0.9558955895589559,
      "grad_norm": 0.21574169397354126,
      "learning_rate": 8.613519343139175e-05,
      "loss": 0.2009,
      "step": 5310
    },
    {
      "epoch": 0.9576957695769577,
      "grad_norm": 0.23259055614471436,
      "learning_rate": 8.60627266815914e-05,
      "loss": 0.1998,
      "step": 5320
    },
    {
      "epoch": 0.9594959495949595,
      "grad_norm": 0.20665207505226135,
      "learning_rate": 8.599010170083635e-05,
      "loss": 0.198,
      "step": 5330
    },
    {
      "epoch": 0.9612961296129613,
      "grad_norm": 0.2046847939491272,
      "learning_rate": 8.591731880778025e-05,
      "loss": 0.2031,
      "step": 5340
    },
    {
      "epoch": 0.963096309630963,
      "grad_norm": 0.21842117607593536,
      "learning_rate": 8.584437832176966e-05,
      "loss": 0.2028,
      "step": 5350
    },
    {
      "epoch": 0.9648964896489649,
      "grad_norm": 0.22107627987861633,
      "learning_rate": 8.577128056284255e-05,
      "loss": 0.196,
      "step": 5360
    },
    {
      "epoch": 0.9666966696669667,
      "grad_norm": 0.2062399685382843,
      "learning_rate": 8.569802585172699e-05,
      "loss": 0.1978,
      "step": 5370
    },
    {
      "epoch": 0.9684968496849685,
      "grad_norm": 0.21717259287834167,
      "learning_rate": 8.562461450983968e-05,
      "loss": 0.2034,
      "step": 5380
    },
    {
      "epoch": 0.9702970297029703,
      "grad_norm": 0.22800122201442719,
      "learning_rate": 8.555104685928458e-05,
      "loss": 0.2085,
      "step": 5390
    },
    {
      "epoch": 0.9720972097209721,
      "grad_norm": 0.21371755003929138,
      "learning_rate": 8.547732322285146e-05,
      "loss": 0.1987,
      "step": 5400
    },
    {
      "epoch": 0.9738973897389739,
      "grad_norm": 0.2408205270767212,
      "learning_rate": 8.540344392401451e-05,
      "loss": 0.1926,
      "step": 5410
    },
    {
      "epoch": 0.9756975697569757,
      "grad_norm": 0.22534720599651337,
      "learning_rate": 8.532940928693092e-05,
      "loss": 0.197,
      "step": 5420
    },
    {
      "epoch": 0.9774977497749775,
      "grad_norm": 0.24241024255752563,
      "learning_rate": 8.525521963643946e-05,
      "loss": 0.1965,
      "step": 5430
    },
    {
      "epoch": 0.9792979297929792,
      "grad_norm": 0.21398070454597473,
      "learning_rate": 8.5180875298059e-05,
      "loss": 0.2021,
      "step": 5440
    },
    {
      "epoch": 0.9810981098109811,
      "grad_norm": 0.23988649249076843,
      "learning_rate": 8.510637659798719e-05,
      "loss": 0.1987,
      "step": 5450
    },
    {
      "epoch": 0.9828982898289829,
      "grad_norm": 0.20920424163341522,
      "learning_rate": 8.503172386309892e-05,
      "loss": 0.2005,
      "step": 5460
    },
    {
      "epoch": 0.9846984698469847,
      "grad_norm": 0.2365211397409439,
      "learning_rate": 8.495691742094498e-05,
      "loss": 0.1953,
      "step": 5470
    },
    {
      "epoch": 0.9864986498649865,
      "grad_norm": 0.2141592800617218,
      "learning_rate": 8.48819575997505e-05,
      "loss": 0.199,
      "step": 5480
    },
    {
      "epoch": 0.9882988298829883,
      "grad_norm": 0.2417260706424713,
      "learning_rate": 8.480684472841366e-05,
      "loss": 0.202,
      "step": 5490
    },
    {
      "epoch": 0.9900990099009901,
      "grad_norm": 0.225415900349617,
      "learning_rate": 8.473157913650412e-05,
      "loss": 0.1978,
      "step": 5500
    },
    {
      "epoch": 0.9900990099009901,
      "eval_loss": 0.19931472837924957,
      "eval_runtime": 316.8266,
      "eval_samples_per_second": 124.674,
      "eval_steps_per_second": 3.898,
      "step": 5500
    },
    {
      "epoch": 0.991899189918992,
      "grad_norm": 0.23154836893081665,
      "learning_rate": 8.465616115426167e-05,
      "loss": 0.191,
      "step": 5510
    },
    {
      "epoch": 0.9936993699369937,
      "grad_norm": 0.20643921196460724,
      "learning_rate": 8.458059111259471e-05,
      "loss": 0.203,
      "step": 5520
    },
    {
      "epoch": 0.9954995499549955,
      "grad_norm": 0.21065416932106018,
      "learning_rate": 8.450486934307882e-05,
      "loss": 0.1916,
      "step": 5530
    },
    {
      "epoch": 0.9972997299729973,
      "grad_norm": 0.2460058629512787,
      "learning_rate": 8.442899617795531e-05,
      "loss": 0.2035,
      "step": 5540
    },
    {
      "epoch": 0.9990999099909991,
      "grad_norm": 0.22961121797561646,
      "learning_rate": 8.43529719501298e-05,
      "loss": 0.1951,
      "step": 5550
    },
    {
      "epoch": 1.0009000900090008,
      "grad_norm": 0.2122998684644699,
      "learning_rate": 8.427679699317067e-05,
      "loss": 0.1915,
      "step": 5560
    },
    {
      "epoch": 1.0027002700270027,
      "grad_norm": 0.2124055176973343,
      "learning_rate": 8.420047164130766e-05,
      "loss": 0.1782,
      "step": 5570
    },
    {
      "epoch": 1.0045004500450045,
      "grad_norm": 0.22698654234409332,
      "learning_rate": 8.412399622943045e-05,
      "loss": 0.1818,
      "step": 5580
    },
    {
      "epoch": 1.0063006300630064,
      "grad_norm": 0.2033182829618454,
      "learning_rate": 8.404737109308705e-05,
      "loss": 0.1791,
      "step": 5590
    },
    {
      "epoch": 1.008100810081008,
      "grad_norm": 0.2149023413658142,
      "learning_rate": 8.397059656848242e-05,
      "loss": 0.1831,
      "step": 5600
    },
    {
      "epoch": 1.00990099009901,
      "grad_norm": 0.22377514839172363,
      "learning_rate": 8.389367299247707e-05,
      "loss": 0.1791,
      "step": 5610
    },
    {
      "epoch": 1.0117011701170118,
      "grad_norm": 0.24366538226604462,
      "learning_rate": 8.38166007025854e-05,
      "loss": 0.1795,
      "step": 5620
    },
    {
      "epoch": 1.0135013501350134,
      "grad_norm": 0.2169315218925476,
      "learning_rate": 8.373938003697435e-05,
      "loss": 0.1816,
      "step": 5630
    },
    {
      "epoch": 1.0153015301530153,
      "grad_norm": 0.238911971449852,
      "learning_rate": 8.366201133446189e-05,
      "loss": 0.1787,
      "step": 5640
    },
    {
      "epoch": 1.0171017101710171,
      "grad_norm": 0.24937152862548828,
      "learning_rate": 8.358449493451554e-05,
      "loss": 0.1777,
      "step": 5650
    },
    {
      "epoch": 1.018901890189019,
      "grad_norm": 0.21741554141044617,
      "learning_rate": 8.350683117725082e-05,
      "loss": 0.1825,
      "step": 5660
    },
    {
      "epoch": 1.0207020702070206,
      "grad_norm": 0.27232933044433594,
      "learning_rate": 8.342902040342983e-05,
      "loss": 0.1826,
      "step": 5670
    },
    {
      "epoch": 1.0225022502250225,
      "grad_norm": 0.19997097551822662,
      "learning_rate": 8.335106295445974e-05,
      "loss": 0.1785,
      "step": 5680
    },
    {
      "epoch": 1.0243024302430244,
      "grad_norm": 0.2182328850030899,
      "learning_rate": 8.327295917239126e-05,
      "loss": 0.1803,
      "step": 5690
    },
    {
      "epoch": 1.026102610261026,
      "grad_norm": 0.22710135579109192,
      "learning_rate": 8.319470939991718e-05,
      "loss": 0.178,
      "step": 5700
    },
    {
      "epoch": 1.0279027902790279,
      "grad_norm": 0.22037293016910553,
      "learning_rate": 8.311631398037083e-05,
      "loss": 0.1743,
      "step": 5710
    },
    {
      "epoch": 1.0297029702970297,
      "grad_norm": 0.2453022301197052,
      "learning_rate": 8.30377732577246e-05,
      "loss": 0.1851,
      "step": 5720
    },
    {
      "epoch": 1.0315031503150316,
      "grad_norm": 0.2103194147348404,
      "learning_rate": 8.295908757658837e-05,
      "loss": 0.1722,
      "step": 5730
    },
    {
      "epoch": 1.0333033303330332,
      "grad_norm": 0.24787963926792145,
      "learning_rate": 8.288025728220816e-05,
      "loss": 0.1821,
      "step": 5740
    },
    {
      "epoch": 1.035103510351035,
      "grad_norm": 0.23751315474510193,
      "learning_rate": 8.280128272046439e-05,
      "loss": 0.1746,
      "step": 5750
    },
    {
      "epoch": 1.036903690369037,
      "grad_norm": 0.23362411558628082,
      "learning_rate": 8.272216423787056e-05,
      "loss": 0.1796,
      "step": 5760
    },
    {
      "epoch": 1.0387038703870386,
      "grad_norm": 0.2425726354122162,
      "learning_rate": 8.264290218157157e-05,
      "loss": 0.1815,
      "step": 5770
    },
    {
      "epoch": 1.0405040504050405,
      "grad_norm": 0.20776617527008057,
      "learning_rate": 8.256349689934233e-05,
      "loss": 0.1815,
      "step": 5780
    },
    {
      "epoch": 1.0423042304230423,
      "grad_norm": 0.2245650589466095,
      "learning_rate": 8.248394873958616e-05,
      "loss": 0.1837,
      "step": 5790
    },
    {
      "epoch": 1.0441044104410442,
      "grad_norm": 0.21586057543754578,
      "learning_rate": 8.240425805133328e-05,
      "loss": 0.1796,
      "step": 5800
    },
    {
      "epoch": 1.0459045904590458,
      "grad_norm": 0.21698030829429626,
      "learning_rate": 8.232442518423924e-05,
      "loss": 0.1871,
      "step": 5810
    },
    {
      "epoch": 1.0477047704770477,
      "grad_norm": 0.23718494176864624,
      "learning_rate": 8.22444504885835e-05,
      "loss": 0.1811,
      "step": 5820
    },
    {
      "epoch": 1.0495049504950495,
      "grad_norm": 0.24667838215827942,
      "learning_rate": 8.216433431526774e-05,
      "loss": 0.1812,
      "step": 5830
    },
    {
      "epoch": 1.0513051305130514,
      "grad_norm": 0.2536092698574066,
      "learning_rate": 8.208407701581443e-05,
      "loss": 0.1794,
      "step": 5840
    },
    {
      "epoch": 1.053105310531053,
      "grad_norm": 0.21370790898799896,
      "learning_rate": 8.200367894236525e-05,
      "loss": 0.1876,
      "step": 5850
    },
    {
      "epoch": 1.054905490549055,
      "grad_norm": 0.21642319858074188,
      "learning_rate": 8.192314044767955e-05,
      "loss": 0.1841,
      "step": 5860
    },
    {
      "epoch": 1.0567056705670568,
      "grad_norm": 0.23614123463630676,
      "learning_rate": 8.184246188513281e-05,
      "loss": 0.1865,
      "step": 5870
    },
    {
      "epoch": 1.0585058505850584,
      "grad_norm": 0.21579112112522125,
      "learning_rate": 8.176164360871501e-05,
      "loss": 0.1782,
      "step": 5880
    },
    {
      "epoch": 1.0603060306030603,
      "grad_norm": 0.248478963971138,
      "learning_rate": 8.168068597302929e-05,
      "loss": 0.1829,
      "step": 5890
    },
    {
      "epoch": 1.0621062106210621,
      "grad_norm": 0.21805238723754883,
      "learning_rate": 8.159958933329011e-05,
      "loss": 0.181,
      "step": 5900
    },
    {
      "epoch": 1.063906390639064,
      "grad_norm": 0.24151034653186798,
      "learning_rate": 8.151835404532191e-05,
      "loss": 0.1804,
      "step": 5910
    },
    {
      "epoch": 1.0657065706570656,
      "grad_norm": 0.2288019061088562,
      "learning_rate": 8.143698046555743e-05,
      "loss": 0.1872,
      "step": 5920
    },
    {
      "epoch": 1.0675067506750675,
      "grad_norm": 0.22423410415649414,
      "learning_rate": 8.135546895103622e-05,
      "loss": 0.1767,
      "step": 5930
    },
    {
      "epoch": 1.0693069306930694,
      "grad_norm": 0.2205565720796585,
      "learning_rate": 8.127381985940302e-05,
      "loss": 0.1841,
      "step": 5940
    },
    {
      "epoch": 1.071107110711071,
      "grad_norm": 0.2462858110666275,
      "learning_rate": 8.119203354890624e-05,
      "loss": 0.1861,
      "step": 5950
    },
    {
      "epoch": 1.0729072907290729,
      "grad_norm": 0.24020881950855255,
      "learning_rate": 8.111011037839628e-05,
      "loss": 0.1802,
      "step": 5960
    },
    {
      "epoch": 1.0747074707470747,
      "grad_norm": 0.22531883418560028,
      "learning_rate": 8.102805070732417e-05,
      "loss": 0.1815,
      "step": 5970
    },
    {
      "epoch": 1.0765076507650766,
      "grad_norm": 0.2602084279060364,
      "learning_rate": 8.094585489573975e-05,
      "loss": 0.1898,
      "step": 5980
    },
    {
      "epoch": 1.0783078307830782,
      "grad_norm": 0.20193356275558472,
      "learning_rate": 8.086352330429019e-05,
      "loss": 0.1844,
      "step": 5990
    },
    {
      "epoch": 1.08010801080108,
      "grad_norm": 0.2624191343784332,
      "learning_rate": 8.078105629421851e-05,
      "loss": 0.1799,
      "step": 6000
    },
    {
      "epoch": 1.08010801080108,
      "eval_loss": 0.19844526052474976,
      "eval_runtime": 316.856,
      "eval_samples_per_second": 124.662,
      "eval_steps_per_second": 3.898,
      "step": 6000
    },
    {
      "epoch": 1.081908190819082,
      "grad_norm": 0.26000428199768066,
      "learning_rate": 8.069845422736184e-05,
      "loss": 0.1844,
      "step": 6010
    },
    {
      "epoch": 1.0837083708370838,
      "grad_norm": 0.2690500020980835,
      "learning_rate": 8.061571746614986e-05,
      "loss": 0.1843,
      "step": 6020
    },
    {
      "epoch": 1.0855085508550855,
      "grad_norm": 0.22099098563194275,
      "learning_rate": 8.053284637360332e-05,
      "loss": 0.1817,
      "step": 6030
    },
    {
      "epoch": 1.0873087308730873,
      "grad_norm": 0.23016442358493805,
      "learning_rate": 8.04498413133323e-05,
      "loss": 0.1831,
      "step": 6040
    },
    {
      "epoch": 1.0891089108910892,
      "grad_norm": 0.20191094279289246,
      "learning_rate": 8.036670264953477e-05,
      "loss": 0.1763,
      "step": 6050
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 0.23867644369602203,
      "learning_rate": 8.028343074699478e-05,
      "loss": 0.1835,
      "step": 6060
    },
    {
      "epoch": 1.0927092709270927,
      "grad_norm": 0.218905508518219,
      "learning_rate": 8.020002597108112e-05,
      "loss": 0.1766,
      "step": 6070
    },
    {
      "epoch": 1.0945094509450946,
      "grad_norm": 0.212873175740242,
      "learning_rate": 8.01164886877455e-05,
      "loss": 0.1785,
      "step": 6080
    },
    {
      "epoch": 1.0963096309630962,
      "grad_norm": 0.23665957152843475,
      "learning_rate": 8.003281926352105e-05,
      "loss": 0.1767,
      "step": 6090
    },
    {
      "epoch": 1.098109810981098,
      "grad_norm": 0.22037798166275024,
      "learning_rate": 7.994901806552068e-05,
      "loss": 0.1924,
      "step": 6100
    },
    {
      "epoch": 1.0999099909991,
      "grad_norm": 0.23455555737018585,
      "learning_rate": 7.986508546143552e-05,
      "loss": 0.1896,
      "step": 6110
    },
    {
      "epoch": 1.1017101710171018,
      "grad_norm": 0.22522200644016266,
      "learning_rate": 7.97810218195332e-05,
      "loss": 0.1823,
      "step": 6120
    },
    {
      "epoch": 1.1035103510351034,
      "grad_norm": 0.24012596905231476,
      "learning_rate": 7.969682750865634e-05,
      "loss": 0.1809,
      "step": 6130
    },
    {
      "epoch": 1.1053105310531053,
      "grad_norm": 0.2297692596912384,
      "learning_rate": 7.961250289822089e-05,
      "loss": 0.181,
      "step": 6140
    },
    {
      "epoch": 1.1071107110711071,
      "grad_norm": 0.25783854722976685,
      "learning_rate": 7.95280483582145e-05,
      "loss": 0.1831,
      "step": 6150
    },
    {
      "epoch": 1.108910891089109,
      "grad_norm": 0.23703411221504211,
      "learning_rate": 7.944346425919491e-05,
      "loss": 0.1909,
      "step": 6160
    },
    {
      "epoch": 1.1107110711071106,
      "grad_norm": 0.25322505831718445,
      "learning_rate": 7.935875097228834e-05,
      "loss": 0.1847,
      "step": 6170
    },
    {
      "epoch": 1.1125112511251125,
      "grad_norm": 0.24147309362888336,
      "learning_rate": 7.927390886918778e-05,
      "loss": 0.1879,
      "step": 6180
    },
    {
      "epoch": 1.1143114311431144,
      "grad_norm": 0.24996539950370789,
      "learning_rate": 7.918893832215149e-05,
      "loss": 0.1804,
      "step": 6190
    },
    {
      "epoch": 1.116111611161116,
      "grad_norm": 0.23054106533527374,
      "learning_rate": 7.910383970400128e-05,
      "loss": 0.1868,
      "step": 6200
    },
    {
      "epoch": 1.1179117911791179,
      "grad_norm": 0.25044700503349304,
      "learning_rate": 7.901861338812088e-05,
      "loss": 0.1828,
      "step": 6210
    },
    {
      "epoch": 1.1197119711971197,
      "grad_norm": 0.23547938466072083,
      "learning_rate": 7.893325974845431e-05,
      "loss": 0.1842,
      "step": 6220
    },
    {
      "epoch": 1.1215121512151216,
      "grad_norm": 0.24082323908805847,
      "learning_rate": 7.884777915950425e-05,
      "loss": 0.1899,
      "step": 6230
    },
    {
      "epoch": 1.1233123312331232,
      "grad_norm": 0.22680528461933136,
      "learning_rate": 7.876217199633043e-05,
      "loss": 0.183,
      "step": 6240
    },
    {
      "epoch": 1.125112511251125,
      "grad_norm": 0.24876077473163605,
      "learning_rate": 7.867643863454785e-05,
      "loss": 0.1786,
      "step": 6250
    },
    {
      "epoch": 1.126912691269127,
      "grad_norm": 0.2264471799135208,
      "learning_rate": 7.859057945032533e-05,
      "loss": 0.186,
      "step": 6260
    },
    {
      "epoch": 1.1287128712871288,
      "grad_norm": 0.252796471118927,
      "learning_rate": 7.850459482038368e-05,
      "loss": 0.1896,
      "step": 6270
    },
    {
      "epoch": 1.1305130513051305,
      "grad_norm": 0.24498094618320465,
      "learning_rate": 7.841848512199419e-05,
      "loss": 0.1856,
      "step": 6280
    },
    {
      "epoch": 1.1323132313231323,
      "grad_norm": 0.20089095830917358,
      "learning_rate": 7.833225073297683e-05,
      "loss": 0.182,
      "step": 6290
    },
    {
      "epoch": 1.1341134113411342,
      "grad_norm": 0.22718565165996552,
      "learning_rate": 7.824589203169873e-05,
      "loss": 0.1882,
      "step": 6300
    },
    {
      "epoch": 1.1359135913591358,
      "grad_norm": 0.21958692371845245,
      "learning_rate": 7.815940939707244e-05,
      "loss": 0.1811,
      "step": 6310
    },
    {
      "epoch": 1.1377137713771377,
      "grad_norm": 0.2255156934261322,
      "learning_rate": 7.807280320855431e-05,
      "loss": 0.1887,
      "step": 6320
    },
    {
      "epoch": 1.1395139513951396,
      "grad_norm": 0.24252045154571533,
      "learning_rate": 7.798607384614274e-05,
      "loss": 0.1788,
      "step": 6330
    },
    {
      "epoch": 1.1413141314131412,
      "grad_norm": 0.24444131553173065,
      "learning_rate": 7.789922169037661e-05,
      "loss": 0.1839,
      "step": 6340
    },
    {
      "epoch": 1.143114311431143,
      "grad_norm": 0.2577359080314636,
      "learning_rate": 7.781224712233364e-05,
      "loss": 0.1775,
      "step": 6350
    },
    {
      "epoch": 1.144914491449145,
      "grad_norm": 0.24814701080322266,
      "learning_rate": 7.772515052362853e-05,
      "loss": 0.1857,
      "step": 6360
    },
    {
      "epoch": 1.1467146714671468,
      "grad_norm": 0.2166299968957901,
      "learning_rate": 7.763793227641152e-05,
      "loss": 0.177,
      "step": 6370
    },
    {
      "epoch": 1.1485148514851484,
      "grad_norm": 0.22626471519470215,
      "learning_rate": 7.755059276336651e-05,
      "loss": 0.187,
      "step": 6380
    },
    {
      "epoch": 1.1503150315031503,
      "grad_norm": 0.20939259231090546,
      "learning_rate": 7.746313236770958e-05,
      "loss": 0.1807,
      "step": 6390
    },
    {
      "epoch": 1.1521152115211521,
      "grad_norm": 0.23045918345451355,
      "learning_rate": 7.737555147318709e-05,
      "loss": 0.1774,
      "step": 6400
    },
    {
      "epoch": 1.153915391539154,
      "grad_norm": 0.21168984472751617,
      "learning_rate": 7.728785046407416e-05,
      "loss": 0.1863,
      "step": 6410
    },
    {
      "epoch": 1.1557155715571557,
      "grad_norm": 0.22225093841552734,
      "learning_rate": 7.720002972517295e-05,
      "loss": 0.1834,
      "step": 6420
    },
    {
      "epoch": 1.1575157515751575,
      "grad_norm": 0.2526393532752991,
      "learning_rate": 7.711208964181093e-05,
      "loss": 0.1875,
      "step": 6430
    },
    {
      "epoch": 1.1593159315931594,
      "grad_norm": 0.23065048456192017,
      "learning_rate": 7.702403059983922e-05,
      "loss": 0.184,
      "step": 6440
    },
    {
      "epoch": 1.161116111611161,
      "grad_norm": 0.23032861948013306,
      "learning_rate": 7.693585298563087e-05,
      "loss": 0.1892,
      "step": 6450
    },
    {
      "epoch": 1.1629162916291629,
      "grad_norm": 0.22203806042671204,
      "learning_rate": 7.684755718607921e-05,
      "loss": 0.1805,
      "step": 6460
    },
    {
      "epoch": 1.1647164716471647,
      "grad_norm": 0.2457210272550583,
      "learning_rate": 7.675914358859612e-05,
      "loss": 0.1833,
      "step": 6470
    },
    {
      "epoch": 1.1665166516651666,
      "grad_norm": 0.22170527279376984,
      "learning_rate": 7.667061258111032e-05,
      "loss": 0.1846,
      "step": 6480
    },
    {
      "epoch": 1.1683168316831682,
      "grad_norm": 0.25271642208099365,
      "learning_rate": 7.658196455206572e-05,
      "loss": 0.1781,
      "step": 6490
    },
    {
      "epoch": 1.17011701170117,
      "grad_norm": 0.22583577036857605,
      "learning_rate": 7.649319989041965e-05,
      "loss": 0.1847,
      "step": 6500
    },
    {
      "epoch": 1.17011701170117,
      "eval_loss": 0.1971111297607422,
      "eval_runtime": 316.7166,
      "eval_samples_per_second": 124.717,
      "eval_steps_per_second": 3.899,
      "step": 6500
    },
    {
      "epoch": 1.171917191719172,
      "grad_norm": 0.2250930666923523,
      "learning_rate": 7.64043189856412e-05,
      "loss": 0.1798,
      "step": 6510
    },
    {
      "epoch": 1.1737173717371738,
      "grad_norm": 0.2210041880607605,
      "learning_rate": 7.63153222277095e-05,
      "loss": 0.1794,
      "step": 6520
    },
    {
      "epoch": 1.1755175517551755,
      "grad_norm": 0.23946473002433777,
      "learning_rate": 7.622621000711196e-05,
      "loss": 0.1767,
      "step": 6530
    },
    {
      "epoch": 1.1773177317731773,
      "grad_norm": 0.2456904947757721,
      "learning_rate": 7.613698271484268e-05,
      "loss": 0.179,
      "step": 6540
    },
    {
      "epoch": 1.1791179117911792,
      "grad_norm": 0.2379496842622757,
      "learning_rate": 7.604764074240056e-05,
      "loss": 0.1845,
      "step": 6550
    },
    {
      "epoch": 1.1809180918091808,
      "grad_norm": 0.24855934083461761,
      "learning_rate": 7.595818448178778e-05,
      "loss": 0.1826,
      "step": 6560
    },
    {
      "epoch": 1.1827182718271827,
      "grad_norm": 0.2715936601161957,
      "learning_rate": 7.586861432550788e-05,
      "loss": 0.1862,
      "step": 6570
    },
    {
      "epoch": 1.1845184518451846,
      "grad_norm": 0.26532432436943054,
      "learning_rate": 7.57789306665642e-05,
      "loss": 0.1755,
      "step": 6580
    },
    {
      "epoch": 1.1863186318631862,
      "grad_norm": 0.250596821308136,
      "learning_rate": 7.568913389845809e-05,
      "loss": 0.1826,
      "step": 6590
    },
    {
      "epoch": 1.188118811881188,
      "grad_norm": 0.250918984413147,
      "learning_rate": 7.559922441518714e-05,
      "loss": 0.182,
      "step": 6600
    },
    {
      "epoch": 1.18991899189919,
      "grad_norm": 0.23598971962928772,
      "learning_rate": 7.550920261124354e-05,
      "loss": 0.1893,
      "step": 6610
    },
    {
      "epoch": 1.1917191719171918,
      "grad_norm": 0.23270264267921448,
      "learning_rate": 7.541906888161225e-05,
      "loss": 0.1793,
      "step": 6620
    },
    {
      "epoch": 1.1935193519351934,
      "grad_norm": 0.21534624695777893,
      "learning_rate": 7.532882362176941e-05,
      "loss": 0.1867,
      "step": 6630
    },
    {
      "epoch": 1.1953195319531953,
      "grad_norm": 0.22311323881149292,
      "learning_rate": 7.523846722768044e-05,
      "loss": 0.1846,
      "step": 6640
    },
    {
      "epoch": 1.1971197119711972,
      "grad_norm": 0.230670765042305,
      "learning_rate": 7.51480000957984e-05,
      "loss": 0.1887,
      "step": 6650
    },
    {
      "epoch": 1.198919891989199,
      "grad_norm": 0.26517319679260254,
      "learning_rate": 7.505742262306223e-05,
      "loss": 0.1826,
      "step": 6660
    },
    {
      "epoch": 1.2007200720072007,
      "grad_norm": 0.22027923166751862,
      "learning_rate": 7.496673520689504e-05,
      "loss": 0.1798,
      "step": 6670
    },
    {
      "epoch": 1.2025202520252025,
      "grad_norm": 0.2303256094455719,
      "learning_rate": 7.487593824520229e-05,
      "loss": 0.1889,
      "step": 6680
    },
    {
      "epoch": 1.2043204320432044,
      "grad_norm": 0.24294430017471313,
      "learning_rate": 7.478503213637009e-05,
      "loss": 0.1866,
      "step": 6690
    },
    {
      "epoch": 1.206120612061206,
      "grad_norm": 0.32139959931373596,
      "learning_rate": 7.469401727926347e-05,
      "loss": 0.1762,
      "step": 6700
    },
    {
      "epoch": 1.2079207920792079,
      "grad_norm": 0.24487176537513733,
      "learning_rate": 7.460289407322464e-05,
      "loss": 0.1833,
      "step": 6710
    },
    {
      "epoch": 1.2097209720972097,
      "grad_norm": 0.22972148656845093,
      "learning_rate": 7.451166291807111e-05,
      "loss": 0.1712,
      "step": 6720
    },
    {
      "epoch": 1.2115211521152116,
      "grad_norm": 0.23121051490306854,
      "learning_rate": 7.442032421409417e-05,
      "loss": 0.1858,
      "step": 6730
    },
    {
      "epoch": 1.2133213321332132,
      "grad_norm": 0.24040502309799194,
      "learning_rate": 7.432887836205689e-05,
      "loss": 0.1769,
      "step": 6740
    },
    {
      "epoch": 1.215121512151215,
      "grad_norm": 0.2681249678134918,
      "learning_rate": 7.423732576319248e-05,
      "loss": 0.1892,
      "step": 6750
    },
    {
      "epoch": 1.216921692169217,
      "grad_norm": 0.24310873448848724,
      "learning_rate": 7.414566681920257e-05,
      "loss": 0.1831,
      "step": 6760
    },
    {
      "epoch": 1.2187218721872188,
      "grad_norm": 0.21631544828414917,
      "learning_rate": 7.40539019322554e-05,
      "loss": 0.1829,
      "step": 6770
    },
    {
      "epoch": 1.2205220522052205,
      "grad_norm": 0.23234550654888153,
      "learning_rate": 7.396203150498396e-05,
      "loss": 0.1817,
      "step": 6780
    },
    {
      "epoch": 1.2223222322232223,
      "grad_norm": 0.20846319198608398,
      "learning_rate": 7.387005594048439e-05,
      "loss": 0.1802,
      "step": 6790
    },
    {
      "epoch": 1.2241224122412242,
      "grad_norm": 0.2412704974412918,
      "learning_rate": 7.377797564231417e-05,
      "loss": 0.186,
      "step": 6800
    },
    {
      "epoch": 1.2259225922592258,
      "grad_norm": 0.21988070011138916,
      "learning_rate": 7.368579101449022e-05,
      "loss": 0.1791,
      "step": 6810
    },
    {
      "epoch": 1.2277227722772277,
      "grad_norm": 0.23824061453342438,
      "learning_rate": 7.359350246148729e-05,
      "loss": 0.1886,
      "step": 6820
    },
    {
      "epoch": 1.2295229522952296,
      "grad_norm": 0.25290706753730774,
      "learning_rate": 7.35011103882361e-05,
      "loss": 0.1866,
      "step": 6830
    },
    {
      "epoch": 1.2313231323132312,
      "grad_norm": 0.2265482097864151,
      "learning_rate": 7.340861520012156e-05,
      "loss": 0.1805,
      "step": 6840
    },
    {
      "epoch": 1.233123312331233,
      "grad_norm": 0.23921525478363037,
      "learning_rate": 7.331601730298106e-05,
      "loss": 0.1843,
      "step": 6850
    },
    {
      "epoch": 1.234923492349235,
      "grad_norm": 0.22793932259082794,
      "learning_rate": 7.322331710310259e-05,
      "loss": 0.1783,
      "step": 6860
    },
    {
      "epoch": 1.2367236723672368,
      "grad_norm": 0.20772512257099152,
      "learning_rate": 7.313051500722305e-05,
      "loss": 0.1747,
      "step": 6870
    },
    {
      "epoch": 1.2385238523852384,
      "grad_norm": 0.2482481747865677,
      "learning_rate": 7.303761142252644e-05,
      "loss": 0.1769,
      "step": 6880
    },
    {
      "epoch": 1.2403240324032403,
      "grad_norm": 0.25542712211608887,
      "learning_rate": 7.294460675664198e-05,
      "loss": 0.1849,
      "step": 6890
    },
    {
      "epoch": 1.2421242124212422,
      "grad_norm": 0.24340622127056122,
      "learning_rate": 7.285150141764247e-05,
      "loss": 0.1829,
      "step": 6900
    },
    {
      "epoch": 1.243924392439244,
      "grad_norm": 0.2532971501350403,
      "learning_rate": 7.27582958140424e-05,
      "loss": 0.1879,
      "step": 6910
    },
    {
      "epoch": 1.2457245724572457,
      "grad_norm": 0.23622813820838928,
      "learning_rate": 7.26649903547962e-05,
      "loss": 0.181,
      "step": 6920
    },
    {
      "epoch": 1.2475247524752475,
      "grad_norm": 0.24392195045948029,
      "learning_rate": 7.257158544929643e-05,
      "loss": 0.1919,
      "step": 6930
    },
    {
      "epoch": 1.2493249324932494,
      "grad_norm": 0.2264585793018341,
      "learning_rate": 7.247808150737197e-05,
      "loss": 0.1889,
      "step": 6940
    },
    {
      "epoch": 1.251125112511251,
      "grad_norm": 0.2780439555644989,
      "learning_rate": 7.238447893928628e-05,
      "loss": 0.1788,
      "step": 6950
    },
    {
      "epoch": 1.2529252925292529,
      "grad_norm": 0.22765761613845825,
      "learning_rate": 7.22907781557355e-05,
      "loss": 0.1838,
      "step": 6960
    },
    {
      "epoch": 1.2547254725472547,
      "grad_norm": 0.21058151125907898,
      "learning_rate": 7.219697956784674e-05,
      "loss": 0.1786,
      "step": 6970
    },
    {
      "epoch": 1.2565256525652564,
      "grad_norm": 0.23801380395889282,
      "learning_rate": 7.210308358717628e-05,
      "loss": 0.1828,
      "step": 6980
    },
    {
      "epoch": 1.2583258325832583,
      "grad_norm": 0.22969532012939453,
      "learning_rate": 7.20090906257076e-05,
      "loss": 0.1907,
      "step": 6990
    },
    {
      "epoch": 1.2601260126012601,
      "grad_norm": 0.2403048872947693,
      "learning_rate": 7.191500109584987e-05,
      "loss": 0.1786,
      "step": 7000
    },
    {
      "epoch": 1.2601260126012601,
      "eval_loss": 0.19585826992988586,
      "eval_runtime": 316.6586,
      "eval_samples_per_second": 124.74,
      "eval_steps_per_second": 3.9,
      "step": 7000
    },
    {
      "epoch": 1.261926192619262,
      "grad_norm": 0.2004324346780777,
      "learning_rate": 7.182081541043583e-05,
      "loss": 0.1834,
      "step": 7010
    },
    {
      "epoch": 1.2637263726372638,
      "grad_norm": 0.29517194628715515,
      "learning_rate": 7.172653398272018e-05,
      "loss": 0.1834,
      "step": 7020
    },
    {
      "epoch": 1.2655265526552655,
      "grad_norm": 0.24052181839942932,
      "learning_rate": 7.163215722637769e-05,
      "loss": 0.1855,
      "step": 7030
    },
    {
      "epoch": 1.2673267326732673,
      "grad_norm": 0.23728212714195251,
      "learning_rate": 7.153768555550142e-05,
      "loss": 0.1783,
      "step": 7040
    },
    {
      "epoch": 1.2691269126912692,
      "grad_norm": 0.2532604932785034,
      "learning_rate": 7.144311938460087e-05,
      "loss": 0.1797,
      "step": 7050
    },
    {
      "epoch": 1.2709270927092708,
      "grad_norm": 0.22831276059150696,
      "learning_rate": 7.134845912860014e-05,
      "loss": 0.1801,
      "step": 7060
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 0.23795835673809052,
      "learning_rate": 7.125370520283618e-05,
      "loss": 0.1755,
      "step": 7070
    },
    {
      "epoch": 1.2745274527452746,
      "grad_norm": 0.20966097712516785,
      "learning_rate": 7.115885802305695e-05,
      "loss": 0.1793,
      "step": 7080
    },
    {
      "epoch": 1.2763276327632762,
      "grad_norm": 0.2260133922100067,
      "learning_rate": 7.106391800541953e-05,
      "loss": 0.184,
      "step": 7090
    },
    {
      "epoch": 1.278127812781278,
      "grad_norm": 0.25013142824172974,
      "learning_rate": 7.096888556648839e-05,
      "loss": 0.1843,
      "step": 7100
    },
    {
      "epoch": 1.27992799279928,
      "grad_norm": 0.24078087508678436,
      "learning_rate": 7.087376112323343e-05,
      "loss": 0.1833,
      "step": 7110
    },
    {
      "epoch": 1.2817281728172818,
      "grad_norm": 0.23307190835475922,
      "learning_rate": 7.077854509302835e-05,
      "loss": 0.1758,
      "step": 7120
    },
    {
      "epoch": 1.2835283528352837,
      "grad_norm": 0.26093363761901855,
      "learning_rate": 7.068323789364862e-05,
      "loss": 0.1842,
      "step": 7130
    },
    {
      "epoch": 1.2853285328532853,
      "grad_norm": 0.21735729277133942,
      "learning_rate": 7.058783994326973e-05,
      "loss": 0.1775,
      "step": 7140
    },
    {
      "epoch": 1.2871287128712872,
      "grad_norm": 0.22697408497333527,
      "learning_rate": 7.04923516604654e-05,
      "loss": 0.1781,
      "step": 7150
    },
    {
      "epoch": 1.288928892889289,
      "grad_norm": 0.22739244997501373,
      "learning_rate": 7.039677346420566e-05,
      "loss": 0.1823,
      "step": 7160
    },
    {
      "epoch": 1.2907290729072907,
      "grad_norm": 0.2101743519306183,
      "learning_rate": 7.030110577385506e-05,
      "loss": 0.1816,
      "step": 7170
    },
    {
      "epoch": 1.2925292529252925,
      "grad_norm": 0.20637665688991547,
      "learning_rate": 7.020534900917084e-05,
      "loss": 0.1723,
      "step": 7180
    },
    {
      "epoch": 1.2943294329432944,
      "grad_norm": 0.22030319273471832,
      "learning_rate": 7.010950359030104e-05,
      "loss": 0.1806,
      "step": 7190
    },
    {
      "epoch": 1.296129612961296,
      "grad_norm": 0.2452118992805481,
      "learning_rate": 7.00135699377827e-05,
      "loss": 0.1926,
      "step": 7200
    },
    {
      "epoch": 1.297929792979298,
      "grad_norm": 0.23682254552841187,
      "learning_rate": 6.991754847253996e-05,
      "loss": 0.175,
      "step": 7210
    },
    {
      "epoch": 1.2997299729972998,
      "grad_norm": 0.20729398727416992,
      "learning_rate": 6.982143961588235e-05,
      "loss": 0.1722,
      "step": 7220
    },
    {
      "epoch": 1.3015301530153014,
      "grad_norm": 0.22707481682300568,
      "learning_rate": 6.972524378950272e-05,
      "loss": 0.1829,
      "step": 7230
    },
    {
      "epoch": 1.3033303330333033,
      "grad_norm": 0.2163422405719757,
      "learning_rate": 6.96289614154756e-05,
      "loss": 0.1926,
      "step": 7240
    },
    {
      "epoch": 1.3051305130513051,
      "grad_norm": 0.23931069672107697,
      "learning_rate": 6.953259291625522e-05,
      "loss": 0.1819,
      "step": 7250
    },
    {
      "epoch": 1.306930693069307,
      "grad_norm": 0.21093328297138214,
      "learning_rate": 6.943613871467374e-05,
      "loss": 0.1813,
      "step": 7260
    },
    {
      "epoch": 1.3087308730873088,
      "grad_norm": 0.2282516360282898,
      "learning_rate": 6.93395992339393e-05,
      "loss": 0.1759,
      "step": 7270
    },
    {
      "epoch": 1.3105310531053105,
      "grad_norm": 0.23230674862861633,
      "learning_rate": 6.924297489763424e-05,
      "loss": 0.1795,
      "step": 7280
    },
    {
      "epoch": 1.3123312331233123,
      "grad_norm": 0.25473982095718384,
      "learning_rate": 6.914626612971326e-05,
      "loss": 0.1823,
      "step": 7290
    },
    {
      "epoch": 1.3141314131413142,
      "grad_norm": 0.2207656353712082,
      "learning_rate": 6.904947335450142e-05,
      "loss": 0.1802,
      "step": 7300
    },
    {
      "epoch": 1.3159315931593158,
      "grad_norm": 0.21763615310192108,
      "learning_rate": 6.895259699669248e-05,
      "loss": 0.1773,
      "step": 7310
    },
    {
      "epoch": 1.3177317731773177,
      "grad_norm": 0.21983377635478973,
      "learning_rate": 6.885563748134682e-05,
      "loss": 0.1812,
      "step": 7320
    },
    {
      "epoch": 1.3195319531953196,
      "grad_norm": 0.26121819019317627,
      "learning_rate": 6.875859523388983e-05,
      "loss": 0.1841,
      "step": 7330
    },
    {
      "epoch": 1.3213321332133212,
      "grad_norm": 0.24714189767837524,
      "learning_rate": 6.866147068010975e-05,
      "loss": 0.185,
      "step": 7340
    },
    {
      "epoch": 1.323132313231323,
      "grad_norm": 0.22980383038520813,
      "learning_rate": 6.856426424615606e-05,
      "loss": 0.1806,
      "step": 7350
    },
    {
      "epoch": 1.324932493249325,
      "grad_norm": 0.236937016248703,
      "learning_rate": 6.846697635853747e-05,
      "loss": 0.1778,
      "step": 7360
    },
    {
      "epoch": 1.3267326732673268,
      "grad_norm": 0.24340665340423584,
      "learning_rate": 6.836960744412005e-05,
      "loss": 0.1823,
      "step": 7370
    },
    {
      "epoch": 1.3285328532853287,
      "grad_norm": 0.2363894283771515,
      "learning_rate": 6.827215793012544e-05,
      "loss": 0.1763,
      "step": 7380
    },
    {
      "epoch": 1.3303330333033303,
      "grad_norm": 0.2244805246591568,
      "learning_rate": 6.817462824412885e-05,
      "loss": 0.1785,
      "step": 7390
    },
    {
      "epoch": 1.3321332133213322,
      "grad_norm": 0.22540463507175446,
      "learning_rate": 6.807701881405737e-05,
      "loss": 0.1839,
      "step": 7400
    },
    {
      "epoch": 1.333933393339334,
      "grad_norm": 0.22316743433475494,
      "learning_rate": 6.797933006818788e-05,
      "loss": 0.1861,
      "step": 7410
    },
    {
      "epoch": 1.3357335733573357,
      "grad_norm": 0.24465790390968323,
      "learning_rate": 6.78815624351453e-05,
      "loss": 0.1795,
      "step": 7420
    },
    {
      "epoch": 1.3375337533753375,
      "grad_norm": 0.2298559844493866,
      "learning_rate": 6.778371634390069e-05,
      "loss": 0.186,
      "step": 7430
    },
    {
      "epoch": 1.3393339333933394,
      "grad_norm": 0.24516062438488007,
      "learning_rate": 6.768579222376936e-05,
      "loss": 0.1736,
      "step": 7440
    },
    {
      "epoch": 1.341134113411341,
      "grad_norm": 0.22531560063362122,
      "learning_rate": 6.758779050440898e-05,
      "loss": 0.1812,
      "step": 7450
    },
    {
      "epoch": 1.342934293429343,
      "grad_norm": 0.23208829760551453,
      "learning_rate": 6.748971161581765e-05,
      "loss": 0.1804,
      "step": 7460
    },
    {
      "epoch": 1.3447344734473448,
      "grad_norm": 0.2110729217529297,
      "learning_rate": 6.739155598833219e-05,
      "loss": 0.1789,
      "step": 7470
    },
    {
      "epoch": 1.3465346534653464,
      "grad_norm": 0.27054262161254883,
      "learning_rate": 6.7293324052626e-05,
      "loss": 0.1807,
      "step": 7480
    },
    {
      "epoch": 1.3483348334833483,
      "grad_norm": 0.2376764565706253,
      "learning_rate": 6.71950162397073e-05,
      "loss": 0.1777,
      "step": 7490
    },
    {
      "epoch": 1.3501350135013501,
      "grad_norm": 0.26110804080963135,
      "learning_rate": 6.709663298091732e-05,
      "loss": 0.176,
      "step": 7500
    },
    {
      "epoch": 1.3501350135013501,
      "eval_loss": 0.19398465752601624,
      "eval_runtime": 316.8441,
      "eval_samples_per_second": 124.667,
      "eval_steps_per_second": 3.898,
      "step": 7500
    },
    {
      "epoch": 1.351935193519352,
      "grad_norm": 0.22991929948329926,
      "learning_rate": 6.699817470792828e-05,
      "loss": 0.1765,
      "step": 7510
    },
    {
      "epoch": 1.3537353735373538,
      "grad_norm": 0.2591189742088318,
      "learning_rate": 6.689964185274149e-05,
      "loss": 0.1805,
      "step": 7520
    },
    {
      "epoch": 1.3555355535553555,
      "grad_norm": 0.21185347437858582,
      "learning_rate": 6.680103484768557e-05,
      "loss": 0.1764,
      "step": 7530
    },
    {
      "epoch": 1.3573357335733574,
      "grad_norm": 0.2659972310066223,
      "learning_rate": 6.670235412541443e-05,
      "loss": 0.177,
      "step": 7540
    },
    {
      "epoch": 1.3591359135913592,
      "grad_norm": 0.2165699601173401,
      "learning_rate": 6.660360011890546e-05,
      "loss": 0.176,
      "step": 7550
    },
    {
      "epoch": 1.3609360936093609,
      "grad_norm": 0.21779635548591614,
      "learning_rate": 6.650477326145756e-05,
      "loss": 0.183,
      "step": 7560
    },
    {
      "epoch": 1.3627362736273627,
      "grad_norm": 0.2157849371433258,
      "learning_rate": 6.640587398668935e-05,
      "loss": 0.1834,
      "step": 7570
    },
    {
      "epoch": 1.3645364536453646,
      "grad_norm": 0.23944181203842163,
      "learning_rate": 6.63069027285371e-05,
      "loss": 0.18,
      "step": 7580
    },
    {
      "epoch": 1.3663366336633662,
      "grad_norm": 0.27753934264183044,
      "learning_rate": 6.6207859921253e-05,
      "loss": 0.1814,
      "step": 7590
    },
    {
      "epoch": 1.368136813681368,
      "grad_norm": 0.22352878749370575,
      "learning_rate": 6.61087459994031e-05,
      "loss": 0.1886,
      "step": 7600
    },
    {
      "epoch": 1.36993699369937,
      "grad_norm": 0.2485703080892563,
      "learning_rate": 6.600956139786553e-05,
      "loss": 0.181,
      "step": 7610
    },
    {
      "epoch": 1.3717371737173718,
      "grad_norm": 0.2122000902891159,
      "learning_rate": 6.591030655182853e-05,
      "loss": 0.1774,
      "step": 7620
    },
    {
      "epoch": 1.3735373537353737,
      "grad_norm": 0.2260238081216812,
      "learning_rate": 6.581098189678851e-05,
      "loss": 0.1793,
      "step": 7630
    },
    {
      "epoch": 1.3753375337533753,
      "grad_norm": 0.22620567679405212,
      "learning_rate": 6.571158786854821e-05,
      "loss": 0.1814,
      "step": 7640
    },
    {
      "epoch": 1.3771377137713772,
      "grad_norm": 0.21126610040664673,
      "learning_rate": 6.561212490321479e-05,
      "loss": 0.1818,
      "step": 7650
    },
    {
      "epoch": 1.378937893789379,
      "grad_norm": 0.23091119527816772,
      "learning_rate": 6.551259343719783e-05,
      "loss": 0.1764,
      "step": 7660
    },
    {
      "epoch": 1.3807380738073807,
      "grad_norm": 0.22657497227191925,
      "learning_rate": 6.541299390720745e-05,
      "loss": 0.1769,
      "step": 7670
    },
    {
      "epoch": 1.3825382538253825,
      "grad_norm": 0.22140631079673767,
      "learning_rate": 6.531332675025245e-05,
      "loss": 0.1822,
      "step": 7680
    },
    {
      "epoch": 1.3843384338433844,
      "grad_norm": 0.2223970741033554,
      "learning_rate": 6.52135924036384e-05,
      "loss": 0.1787,
      "step": 7690
    },
    {
      "epoch": 1.386138613861386,
      "grad_norm": 0.2704010009765625,
      "learning_rate": 6.511379130496559e-05,
      "loss": 0.1814,
      "step": 7700
    },
    {
      "epoch": 1.387938793879388,
      "grad_norm": 0.2250993549823761,
      "learning_rate": 6.501392389212721e-05,
      "loss": 0.1788,
      "step": 7710
    },
    {
      "epoch": 1.3897389738973898,
      "grad_norm": 0.21789966523647308,
      "learning_rate": 6.491399060330748e-05,
      "loss": 0.1847,
      "step": 7720
    },
    {
      "epoch": 1.3915391539153914,
      "grad_norm": 0.22837704420089722,
      "learning_rate": 6.481399187697958e-05,
      "loss": 0.1815,
      "step": 7730
    },
    {
      "epoch": 1.3933393339333933,
      "grad_norm": 0.2565630376338959,
      "learning_rate": 6.471392815190383e-05,
      "loss": 0.1794,
      "step": 7740
    },
    {
      "epoch": 1.3951395139513951,
      "grad_norm": 0.23311659693717957,
      "learning_rate": 6.461379986712581e-05,
      "loss": 0.1765,
      "step": 7750
    },
    {
      "epoch": 1.396939693969397,
      "grad_norm": 0.2445913553237915,
      "learning_rate": 6.451360746197425e-05,
      "loss": 0.1774,
      "step": 7760
    },
    {
      "epoch": 1.3987398739873989,
      "grad_norm": 0.31027066707611084,
      "learning_rate": 6.44133513760593e-05,
      "loss": 0.1782,
      "step": 7770
    },
    {
      "epoch": 1.4005400540054005,
      "grad_norm": 0.23085008561611176,
      "learning_rate": 6.431303204927052e-05,
      "loss": 0.1775,
      "step": 7780
    },
    {
      "epoch": 1.4023402340234024,
      "grad_norm": 0.2608686685562134,
      "learning_rate": 6.421264992177491e-05,
      "loss": 0.1821,
      "step": 7790
    },
    {
      "epoch": 1.4041404140414042,
      "grad_norm": 0.24065348505973816,
      "learning_rate": 6.411220543401504e-05,
      "loss": 0.1831,
      "step": 7800
    },
    {
      "epoch": 1.4059405940594059,
      "grad_norm": 0.24278388917446136,
      "learning_rate": 6.401169902670708e-05,
      "loss": 0.1806,
      "step": 7810
    },
    {
      "epoch": 1.4077407740774077,
      "grad_norm": 0.24295078217983246,
      "learning_rate": 6.391113114083889e-05,
      "loss": 0.1884,
      "step": 7820
    },
    {
      "epoch": 1.4095409540954096,
      "grad_norm": 0.22885636985301971,
      "learning_rate": 6.38105022176681e-05,
      "loss": 0.179,
      "step": 7830
    },
    {
      "epoch": 1.4113411341134112,
      "grad_norm": 0.22197085618972778,
      "learning_rate": 6.370981269872011e-05,
      "loss": 0.1814,
      "step": 7840
    },
    {
      "epoch": 1.413141314131413,
      "grad_norm": 0.2216714322566986,
      "learning_rate": 6.360906302578624e-05,
      "loss": 0.181,
      "step": 7850
    },
    {
      "epoch": 1.414941494149415,
      "grad_norm": 0.2222881019115448,
      "learning_rate": 6.35082536409217e-05,
      "loss": 0.1733,
      "step": 7860
    },
    {
      "epoch": 1.4167416741674168,
      "grad_norm": 0.23007462918758392,
      "learning_rate": 6.340738498644374e-05,
      "loss": 0.1792,
      "step": 7870
    },
    {
      "epoch": 1.4185418541854187,
      "grad_norm": 0.21849915385246277,
      "learning_rate": 6.33064575049296e-05,
      "loss": 0.1772,
      "step": 7880
    },
    {
      "epoch": 1.4203420342034203,
      "grad_norm": 0.21427130699157715,
      "learning_rate": 6.32054716392147e-05,
      "loss": 0.1754,
      "step": 7890
    },
    {
      "epoch": 1.4221422142214222,
      "grad_norm": 0.2525210380554199,
      "learning_rate": 6.31044278323906e-05,
      "loss": 0.1876,
      "step": 7900
    },
    {
      "epoch": 1.423942394239424,
      "grad_norm": 0.22392630577087402,
      "learning_rate": 6.30033265278031e-05,
      "loss": 0.1775,
      "step": 7910
    },
    {
      "epoch": 1.4257425742574257,
      "grad_norm": 0.23999398946762085,
      "learning_rate": 6.290216816905025e-05,
      "loss": 0.1758,
      "step": 7920
    },
    {
      "epoch": 1.4275427542754275,
      "grad_norm": 0.20164230465888977,
      "learning_rate": 6.280095319998046e-05,
      "loss": 0.1791,
      "step": 7930
    },
    {
      "epoch": 1.4293429342934294,
      "grad_norm": 0.19857601821422577,
      "learning_rate": 6.269968206469054e-05,
      "loss": 0.1815,
      "step": 7940
    },
    {
      "epoch": 1.431143114311431,
      "grad_norm": 0.23204778134822845,
      "learning_rate": 6.259835520752366e-05,
      "loss": 0.1746,
      "step": 7950
    },
    {
      "epoch": 1.432943294329433,
      "grad_norm": 0.22686919569969177,
      "learning_rate": 6.249697307306762e-05,
      "loss": 0.165,
      "step": 7960
    },
    {
      "epoch": 1.4347434743474348,
      "grad_norm": 0.25087347626686096,
      "learning_rate": 6.239553610615264e-05,
      "loss": 0.1827,
      "step": 7970
    },
    {
      "epoch": 1.4365436543654364,
      "grad_norm": 0.2231862097978592,
      "learning_rate": 6.229404475184951e-05,
      "loss": 0.1752,
      "step": 7980
    },
    {
      "epoch": 1.4383438343834383,
      "grad_norm": 0.2217341512441635,
      "learning_rate": 6.219249945546779e-05,
      "loss": 0.1891,
      "step": 7990
    },
    {
      "epoch": 1.4401440144014401,
      "grad_norm": 0.23345151543617249,
      "learning_rate": 6.209090066255358e-05,
      "loss": 0.1753,
      "step": 8000
    },
    {
      "epoch": 1.4401440144014401,
      "eval_loss": 0.19260630011558533,
      "eval_runtime": 316.7857,
      "eval_samples_per_second": 124.69,
      "eval_steps_per_second": 3.899,
      "step": 8000
    },
    {
      "epoch": 1.441944194419442,
      "grad_norm": 0.2599453628063202,
      "learning_rate": 6.198924881888778e-05,
      "loss": 0.1825,
      "step": 8010
    },
    {
      "epoch": 1.4437443744374439,
      "grad_norm": 0.2313985824584961,
      "learning_rate": 6.188754437048403e-05,
      "loss": 0.182,
      "step": 8020
    },
    {
      "epoch": 1.4455445544554455,
      "grad_norm": 0.2610337734222412,
      "learning_rate": 6.17857877635868e-05,
      "loss": 0.1786,
      "step": 8030
    },
    {
      "epoch": 1.4473447344734474,
      "grad_norm": 0.22947362065315247,
      "learning_rate": 6.168397944466938e-05,
      "loss": 0.1797,
      "step": 8040
    },
    {
      "epoch": 1.4491449144914492,
      "grad_norm": 0.21054796874523163,
      "learning_rate": 6.158211986043201e-05,
      "loss": 0.1758,
      "step": 8050
    },
    {
      "epoch": 1.4509450945094509,
      "grad_norm": 0.22567300498485565,
      "learning_rate": 6.14802094577998e-05,
      "loss": 0.182,
      "step": 8060
    },
    {
      "epoch": 1.4527452745274527,
      "grad_norm": 0.22439338266849518,
      "learning_rate": 6.137824868392089e-05,
      "loss": 0.1789,
      "step": 8070
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 0.26348599791526794,
      "learning_rate": 6.12762379861644e-05,
      "loss": 0.1771,
      "step": 8080
    },
    {
      "epoch": 1.4563456345634562,
      "grad_norm": 0.2478642463684082,
      "learning_rate": 6.117417781211851e-05,
      "loss": 0.1766,
      "step": 8090
    },
    {
      "epoch": 1.458145814581458,
      "grad_norm": 0.23607996106147766,
      "learning_rate": 6.107206860958846e-05,
      "loss": 0.1765,
      "step": 8100
    },
    {
      "epoch": 1.45994599459946,
      "grad_norm": 0.22292464971542358,
      "learning_rate": 6.096991082659467e-05,
      "loss": 0.1772,
      "step": 8110
    },
    {
      "epoch": 1.4617461746174618,
      "grad_norm": 0.22889690101146698,
      "learning_rate": 6.086770491137065e-05,
      "loss": 0.1881,
      "step": 8120
    },
    {
      "epoch": 1.4635463546354637,
      "grad_norm": 0.24164429306983948,
      "learning_rate": 6.0765451312361145e-05,
      "loss": 0.1799,
      "step": 8130
    },
    {
      "epoch": 1.4653465346534653,
      "grad_norm": 0.252493679523468,
      "learning_rate": 6.0663150478220066e-05,
      "loss": 0.1791,
      "step": 8140
    },
    {
      "epoch": 1.4671467146714672,
      "grad_norm": 0.23360615968704224,
      "learning_rate": 6.056080285780865e-05,
      "loss": 0.1761,
      "step": 8150
    },
    {
      "epoch": 1.468946894689469,
      "grad_norm": 0.23525089025497437,
      "learning_rate": 6.045840890019335e-05,
      "loss": 0.1812,
      "step": 8160
    },
    {
      "epoch": 1.4707470747074707,
      "grad_norm": 0.2045946568250656,
      "learning_rate": 6.035596905464396e-05,
      "loss": 0.1828,
      "step": 8170
    },
    {
      "epoch": 1.4725472547254725,
      "grad_norm": 0.26069697737693787,
      "learning_rate": 6.025348377063163e-05,
      "loss": 0.1842,
      "step": 8180
    },
    {
      "epoch": 1.4743474347434744,
      "grad_norm": 0.24355357885360718,
      "learning_rate": 6.015095349782682e-05,
      "loss": 0.1816,
      "step": 8190
    },
    {
      "epoch": 1.476147614761476,
      "grad_norm": 0.2247459888458252,
      "learning_rate": 6.004837868609747e-05,
      "loss": 0.1798,
      "step": 8200
    },
    {
      "epoch": 1.477947794779478,
      "grad_norm": 0.22285807132720947,
      "learning_rate": 5.994575978550687e-05,
      "loss": 0.1756,
      "step": 8210
    },
    {
      "epoch": 1.4797479747974798,
      "grad_norm": 0.23576833307743073,
      "learning_rate": 5.9843097246311795e-05,
      "loss": 0.1825,
      "step": 8220
    },
    {
      "epoch": 1.4815481548154814,
      "grad_norm": 0.24282455444335938,
      "learning_rate": 5.974039151896047e-05,
      "loss": 0.1826,
      "step": 8230
    },
    {
      "epoch": 1.4833483348334833,
      "grad_norm": 0.2508341073989868,
      "learning_rate": 5.9637643054090654e-05,
      "loss": 0.1853,
      "step": 8240
    },
    {
      "epoch": 1.4851485148514851,
      "grad_norm": 0.2589671015739441,
      "learning_rate": 5.9534852302527555e-05,
      "loss": 0.1784,
      "step": 8250
    },
    {
      "epoch": 1.486948694869487,
      "grad_norm": 0.247102752327919,
      "learning_rate": 5.943201971528196e-05,
      "loss": 0.1813,
      "step": 8260
    },
    {
      "epoch": 1.4887488748874889,
      "grad_norm": 0.2179850935935974,
      "learning_rate": 5.9329145743548266e-05,
      "loss": 0.1739,
      "step": 8270
    },
    {
      "epoch": 1.4905490549054905,
      "grad_norm": 0.23342496156692505,
      "learning_rate": 5.922623083870236e-05,
      "loss": 0.1804,
      "step": 8280
    },
    {
      "epoch": 1.4923492349234924,
      "grad_norm": 0.22148431837558746,
      "learning_rate": 5.912327545229977e-05,
      "loss": 0.1706,
      "step": 8290
    },
    {
      "epoch": 1.4941494149414942,
      "grad_norm": 0.24201735854148865,
      "learning_rate": 5.902028003607367e-05,
      "loss": 0.1737,
      "step": 8300
    },
    {
      "epoch": 1.4959495949594959,
      "grad_norm": 0.20134377479553223,
      "learning_rate": 5.8917245041932836e-05,
      "loss": 0.181,
      "step": 8310
    },
    {
      "epoch": 1.4977497749774977,
      "grad_norm": 0.22484491765499115,
      "learning_rate": 5.88141709219597e-05,
      "loss": 0.1725,
      "step": 8320
    },
    {
      "epoch": 1.4995499549954996,
      "grad_norm": 0.23555487394332886,
      "learning_rate": 5.871105812840838e-05,
      "loss": 0.1807,
      "step": 8330
    },
    {
      "epoch": 1.5013501350135012,
      "grad_norm": 0.23870199918746948,
      "learning_rate": 5.860790711370268e-05,
      "loss": 0.1765,
      "step": 8340
    },
    {
      "epoch": 1.5031503150315033,
      "grad_norm": 0.25661754608154297,
      "learning_rate": 5.850471833043407e-05,
      "loss": 0.1783,
      "step": 8350
    },
    {
      "epoch": 1.504950495049505,
      "grad_norm": 0.20982691645622253,
      "learning_rate": 5.840149223135981e-05,
      "loss": 0.1781,
      "step": 8360
    },
    {
      "epoch": 1.5067506750675066,
      "grad_norm": 0.2435092180967331,
      "learning_rate": 5.829822926940082e-05,
      "loss": 0.1809,
      "step": 8370
    },
    {
      "epoch": 1.5085508550855087,
      "grad_norm": 0.2362465113401413,
      "learning_rate": 5.819492989763977e-05,
      "loss": 0.1774,
      "step": 8380
    },
    {
      "epoch": 1.5103510351035103,
      "grad_norm": 0.23774689435958862,
      "learning_rate": 5.809159456931914e-05,
      "loss": 0.1754,
      "step": 8390
    },
    {
      "epoch": 1.5121512151215122,
      "grad_norm": 0.2237645536661148,
      "learning_rate": 5.798822373783909e-05,
      "loss": 0.1743,
      "step": 8400
    },
    {
      "epoch": 1.513951395139514,
      "grad_norm": 0.22266927361488342,
      "learning_rate": 5.7884817856755623e-05,
      "loss": 0.1791,
      "step": 8410
    },
    {
      "epoch": 1.5157515751575157,
      "grad_norm": 0.22747738659381866,
      "learning_rate": 5.778137737977851e-05,
      "loss": 0.1736,
      "step": 8420
    },
    {
      "epoch": 1.5175517551755175,
      "grad_norm": 0.2175021916627884,
      "learning_rate": 5.767790276076932e-05,
      "loss": 0.1816,
      "step": 8430
    },
    {
      "epoch": 1.5193519351935194,
      "grad_norm": 0.23600275814533234,
      "learning_rate": 5.757439445373937e-05,
      "loss": 0.1779,
      "step": 8440
    },
    {
      "epoch": 1.521152115211521,
      "grad_norm": 0.23598510026931763,
      "learning_rate": 5.747085291284788e-05,
      "loss": 0.1765,
      "step": 8450
    },
    {
      "epoch": 1.522952295229523,
      "grad_norm": 0.254479318857193,
      "learning_rate": 5.7367278592399856e-05,
      "loss": 0.175,
      "step": 8460
    },
    {
      "epoch": 1.5247524752475248,
      "grad_norm": 0.2649979889392853,
      "learning_rate": 5.726367194684406e-05,
      "loss": 0.186,
      "step": 8470
    },
    {
      "epoch": 1.5265526552655264,
      "grad_norm": 0.2171381711959839,
      "learning_rate": 5.716003343077121e-05,
      "loss": 0.1724,
      "step": 8480
    },
    {
      "epoch": 1.5283528352835285,
      "grad_norm": 0.22726067900657654,
      "learning_rate": 5.705636349891176e-05,
      "loss": 0.1826,
      "step": 8490
    },
    {
      "epoch": 1.5301530153015301,
      "grad_norm": 0.23179955780506134,
      "learning_rate": 5.695266260613403e-05,
      "loss": 0.1761,
      "step": 8500
    },
    {
      "epoch": 1.5301530153015301,
      "eval_loss": 0.19091522693634033,
      "eval_runtime": 316.9922,
      "eval_samples_per_second": 124.609,
      "eval_steps_per_second": 3.896,
      "step": 8500
    },
    {
      "epoch": 1.531953195319532,
      "grad_norm": 0.22688142955303192,
      "learning_rate": 5.6848931207442226e-05,
      "loss": 0.1767,
      "step": 8510
    },
    {
      "epoch": 1.5337533753375339,
      "grad_norm": 0.2690766453742981,
      "learning_rate": 5.674516975797437e-05,
      "loss": 0.1764,
      "step": 8520
    },
    {
      "epoch": 1.5355535553555355,
      "grad_norm": 0.21918874979019165,
      "learning_rate": 5.664137871300033e-05,
      "loss": 0.1766,
      "step": 8530
    },
    {
      "epoch": 1.5373537353735374,
      "grad_norm": 0.2432364821434021,
      "learning_rate": 5.653755852791982e-05,
      "loss": 0.175,
      "step": 8540
    },
    {
      "epoch": 1.5391539153915392,
      "grad_norm": 0.2290085256099701,
      "learning_rate": 5.643370965826048e-05,
      "loss": 0.1801,
      "step": 8550
    },
    {
      "epoch": 1.5409540954095409,
      "grad_norm": 0.2172478586435318,
      "learning_rate": 5.6329832559675724e-05,
      "loss": 0.1721,
      "step": 8560
    },
    {
      "epoch": 1.5427542754275427,
      "grad_norm": 0.2200901210308075,
      "learning_rate": 5.622592768794287e-05,
      "loss": 0.1739,
      "step": 8570
    },
    {
      "epoch": 1.5445544554455446,
      "grad_norm": 0.21984605491161346,
      "learning_rate": 5.612199549896108e-05,
      "loss": 0.1715,
      "step": 8580
    },
    {
      "epoch": 1.5463546354635462,
      "grad_norm": 0.22938090562820435,
      "learning_rate": 5.601803644874938e-05,
      "loss": 0.1753,
      "step": 8590
    },
    {
      "epoch": 1.5481548154815483,
      "grad_norm": 0.22557909786701202,
      "learning_rate": 5.5914050993444656e-05,
      "loss": 0.1804,
      "step": 8600
    },
    {
      "epoch": 1.54995499549955,
      "grad_norm": 0.21756428480148315,
      "learning_rate": 5.581003958929965e-05,
      "loss": 0.181,
      "step": 8610
    },
    {
      "epoch": 1.5517551755175516,
      "grad_norm": 0.233368381857872,
      "learning_rate": 5.5706002692680956e-05,
      "loss": 0.1755,
      "step": 8620
    },
    {
      "epoch": 1.5535553555355537,
      "grad_norm": 0.23162107169628143,
      "learning_rate": 5.560194076006703e-05,
      "loss": 0.1691,
      "step": 8630
    },
    {
      "epoch": 1.5553555355535553,
      "grad_norm": 0.23339669406414032,
      "learning_rate": 5.5497854248046155e-05,
      "loss": 0.1804,
      "step": 8640
    },
    {
      "epoch": 1.5571557155715572,
      "grad_norm": 0.24053777754306793,
      "learning_rate": 5.539374361331446e-05,
      "loss": 0.1772,
      "step": 8650
    },
    {
      "epoch": 1.558955895589559,
      "grad_norm": 0.2338114082813263,
      "learning_rate": 5.5289609312673964e-05,
      "loss": 0.1846,
      "step": 8660
    },
    {
      "epoch": 1.5607560756075607,
      "grad_norm": 0.2203431874513626,
      "learning_rate": 5.518545180303049e-05,
      "loss": 0.1786,
      "step": 8670
    },
    {
      "epoch": 1.5625562556255626,
      "grad_norm": 0.23004686832427979,
      "learning_rate": 5.508127154139164e-05,
      "loss": 0.1746,
      "step": 8680
    },
    {
      "epoch": 1.5643564356435644,
      "grad_norm": 0.23968762159347534,
      "learning_rate": 5.497706898486495e-05,
      "loss": 0.1795,
      "step": 8690
    },
    {
      "epoch": 1.566156615661566,
      "grad_norm": 0.22501949965953827,
      "learning_rate": 5.487284459065573e-05,
      "loss": 0.1742,
      "step": 8700
    },
    {
      "epoch": 1.567956795679568,
      "grad_norm": 0.2260780781507492,
      "learning_rate": 5.476859881606509e-05,
      "loss": 0.1775,
      "step": 8710
    },
    {
      "epoch": 1.5697569756975698,
      "grad_norm": 0.2229728400707245,
      "learning_rate": 5.4664332118487914e-05,
      "loss": 0.1751,
      "step": 8720
    },
    {
      "epoch": 1.5715571557155714,
      "grad_norm": 0.2324976921081543,
      "learning_rate": 5.4560044955411016e-05,
      "loss": 0.1831,
      "step": 8730
    },
    {
      "epoch": 1.5733573357335735,
      "grad_norm": 0.21976949274539948,
      "learning_rate": 5.445573778441089e-05,
      "loss": 0.1778,
      "step": 8740
    },
    {
      "epoch": 1.5751575157515751,
      "grad_norm": 0.25582215189933777,
      "learning_rate": 5.4351411063151835e-05,
      "loss": 0.1748,
      "step": 8750
    },
    {
      "epoch": 1.576957695769577,
      "grad_norm": 0.25923460721969604,
      "learning_rate": 5.424706524938399e-05,
      "loss": 0.178,
      "step": 8760
    },
    {
      "epoch": 1.5787578757875789,
      "grad_norm": 0.23745518922805786,
      "learning_rate": 5.4142700800941215e-05,
      "loss": 0.1723,
      "step": 8770
    },
    {
      "epoch": 1.5805580558055805,
      "grad_norm": 0.2572418451309204,
      "learning_rate": 5.403831817573911e-05,
      "loss": 0.1772,
      "step": 8780
    },
    {
      "epoch": 1.5823582358235824,
      "grad_norm": 0.2146551012992859,
      "learning_rate": 5.393391783177311e-05,
      "loss": 0.1816,
      "step": 8790
    },
    {
      "epoch": 1.5841584158415842,
      "grad_norm": 0.22277669608592987,
      "learning_rate": 5.3829500227116317e-05,
      "loss": 0.1797,
      "step": 8800
    },
    {
      "epoch": 1.5859585958595859,
      "grad_norm": 0.2399885654449463,
      "learning_rate": 5.372506581991759e-05,
      "loss": 0.1754,
      "step": 8810
    },
    {
      "epoch": 1.5877587758775877,
      "grad_norm": 0.2302442491054535,
      "learning_rate": 5.362061506839952e-05,
      "loss": 0.1735,
      "step": 8820
    },
    {
      "epoch": 1.5895589558955896,
      "grad_norm": 0.21585726737976074,
      "learning_rate": 5.351614843085644e-05,
      "loss": 0.1802,
      "step": 8830
    },
    {
      "epoch": 1.5913591359135912,
      "grad_norm": 0.23325729370117188,
      "learning_rate": 5.34116663656523e-05,
      "loss": 0.1798,
      "step": 8840
    },
    {
      "epoch": 1.5931593159315933,
      "grad_norm": 0.22822868824005127,
      "learning_rate": 5.3307169331218856e-05,
      "loss": 0.1781,
      "step": 8850
    },
    {
      "epoch": 1.594959495949595,
      "grad_norm": 0.22216089069843292,
      "learning_rate": 5.320265778605343e-05,
      "loss": 0.1694,
      "step": 8860
    },
    {
      "epoch": 1.5967596759675966,
      "grad_norm": 0.23559026420116425,
      "learning_rate": 5.3098132188717084e-05,
      "loss": 0.1772,
      "step": 8870
    },
    {
      "epoch": 1.5985598559855987,
      "grad_norm": 0.21736763417720795,
      "learning_rate": 5.299359299783254e-05,
      "loss": 0.1695,
      "step": 8880
    },
    {
      "epoch": 1.6003600360036003,
      "grad_norm": 0.22772124409675598,
      "learning_rate": 5.288904067208211e-05,
      "loss": 0.173,
      "step": 8890
    },
    {
      "epoch": 1.6021602160216022,
      "grad_norm": 0.23040778934955597,
      "learning_rate": 5.278447567020578e-05,
      "loss": 0.1708,
      "step": 8900
    },
    {
      "epoch": 1.603960396039604,
      "grad_norm": 0.22176136076450348,
      "learning_rate": 5.267989845099914e-05,
      "loss": 0.1737,
      "step": 8910
    },
    {
      "epoch": 1.6057605760576057,
      "grad_norm": 0.22674013674259186,
      "learning_rate": 5.257530947331141e-05,
      "loss": 0.182,
      "step": 8920
    },
    {
      "epoch": 1.6075607560756076,
      "grad_norm": 0.2312164455652237,
      "learning_rate": 5.2470709196043347e-05,
      "loss": 0.1726,
      "step": 8930
    },
    {
      "epoch": 1.6093609360936094,
      "grad_norm": 0.21699413657188416,
      "learning_rate": 5.236609807814533e-05,
      "loss": 0.173,
      "step": 8940
    },
    {
      "epoch": 1.611161116111611,
      "grad_norm": 0.21089667081832886,
      "learning_rate": 5.226147657861532e-05,
      "loss": 0.1766,
      "step": 8950
    },
    {
      "epoch": 1.612961296129613,
      "grad_norm": 0.24083516001701355,
      "learning_rate": 5.215684515649676e-05,
      "loss": 0.1774,
      "step": 8960
    },
    {
      "epoch": 1.6147614761476148,
      "grad_norm": 0.23547235131263733,
      "learning_rate": 5.2052204270876705e-05,
      "loss": 0.1786,
      "step": 8970
    },
    {
      "epoch": 1.6165616561656164,
      "grad_norm": 0.25413864850997925,
      "learning_rate": 5.194755438088369e-05,
      "loss": 0.1794,
      "step": 8980
    },
    {
      "epoch": 1.6183618361836185,
      "grad_norm": 0.2175319343805313,
      "learning_rate": 5.1842895945685754e-05,
      "loss": 0.1761,
      "step": 8990
    },
    {
      "epoch": 1.6201620162016201,
      "grad_norm": 0.2392716258764267,
      "learning_rate": 5.173822942448847e-05,
      "loss": 0.1765,
      "step": 9000
    },
    {
      "epoch": 1.6201620162016201,
      "eval_loss": 0.18955256044864655,
      "eval_runtime": 316.6128,
      "eval_samples_per_second": 124.758,
      "eval_steps_per_second": 3.901,
      "step": 9000
    },
    {
      "epoch": 1.621962196219622,
      "grad_norm": 0.216875359416008,
      "learning_rate": 5.1633555276532855e-05,
      "loss": 0.1825,
      "step": 9010
    },
    {
      "epoch": 1.6237623762376239,
      "grad_norm": 0.22273580729961395,
      "learning_rate": 5.152887396109338e-05,
      "loss": 0.1761,
      "step": 9020
    },
    {
      "epoch": 1.6255625562556255,
      "grad_norm": 0.22385063767433167,
      "learning_rate": 5.1424185937476e-05,
      "loss": 0.1731,
      "step": 9030
    },
    {
      "epoch": 1.6273627362736274,
      "grad_norm": 0.21351182460784912,
      "learning_rate": 5.131949166501608e-05,
      "loss": 0.1767,
      "step": 9040
    },
    {
      "epoch": 1.6291629162916292,
      "grad_norm": 0.21825958788394928,
      "learning_rate": 5.121479160307643e-05,
      "loss": 0.1731,
      "step": 9050
    },
    {
      "epoch": 1.6309630963096309,
      "grad_norm": 0.24239714443683624,
      "learning_rate": 5.1110086211045197e-05,
      "loss": 0.1814,
      "step": 9060
    },
    {
      "epoch": 1.6327632763276327,
      "grad_norm": 0.22111618518829346,
      "learning_rate": 5.100537594833398e-05,
      "loss": 0.1794,
      "step": 9070
    },
    {
      "epoch": 1.6345634563456346,
      "grad_norm": 0.2437933385372162,
      "learning_rate": 5.0900661274375736e-05,
      "loss": 0.1856,
      "step": 9080
    },
    {
      "epoch": 1.6363636363636362,
      "grad_norm": 0.22507014870643616,
      "learning_rate": 5.0795942648622765e-05,
      "loss": 0.1746,
      "step": 9090
    },
    {
      "epoch": 1.6381638163816383,
      "grad_norm": 0.24766439199447632,
      "learning_rate": 5.069122053054469e-05,
      "loss": 0.1774,
      "step": 9100
    },
    {
      "epoch": 1.63996399639964,
      "grad_norm": 0.2164902687072754,
      "learning_rate": 5.0586495379626507e-05,
      "loss": 0.1866,
      "step": 9110
    },
    {
      "epoch": 1.6417641764176416,
      "grad_norm": 0.21443720161914825,
      "learning_rate": 5.0481767655366466e-05,
      "loss": 0.1685,
      "step": 9120
    },
    {
      "epoch": 1.6435643564356437,
      "grad_norm": 0.22410601377487183,
      "learning_rate": 5.037703781727414e-05,
      "loss": 0.1772,
      "step": 9130
    },
    {
      "epoch": 1.6453645364536453,
      "grad_norm": 0.24402761459350586,
      "learning_rate": 5.027230632486836e-05,
      "loss": 0.178,
      "step": 9140
    },
    {
      "epoch": 1.6471647164716472,
      "grad_norm": 0.23479455709457397,
      "learning_rate": 5.016757363767521e-05,
      "loss": 0.1764,
      "step": 9150
    },
    {
      "epoch": 1.648964896489649,
      "grad_norm": 0.21962401270866394,
      "learning_rate": 5.006284021522608e-05,
      "loss": 0.1775,
      "step": 9160
    },
    {
      "epoch": 1.6507650765076507,
      "grad_norm": 0.23410122096538544,
      "learning_rate": 4.995810651705549e-05,
      "loss": 0.1786,
      "step": 9170
    },
    {
      "epoch": 1.6525652565256526,
      "grad_norm": 0.2425415962934494,
      "learning_rate": 4.985337300269921e-05,
      "loss": 0.1794,
      "step": 9180
    },
    {
      "epoch": 1.6543654365436544,
      "grad_norm": 0.2427809089422226,
      "learning_rate": 4.974864013169223e-05,
      "loss": 0.1768,
      "step": 9190
    },
    {
      "epoch": 1.656165616561656,
      "grad_norm": 0.22023914754390717,
      "learning_rate": 4.964390836356671e-05,
      "loss": 0.1766,
      "step": 9200
    },
    {
      "epoch": 1.657965796579658,
      "grad_norm": 0.234934002161026,
      "learning_rate": 4.953917815784991e-05,
      "loss": 0.182,
      "step": 9210
    },
    {
      "epoch": 1.6597659765976598,
      "grad_norm": 0.24076220393180847,
      "learning_rate": 4.943444997406234e-05,
      "loss": 0.1722,
      "step": 9220
    },
    {
      "epoch": 1.6615661566156614,
      "grad_norm": 0.22638706862926483,
      "learning_rate": 4.9329724271715513e-05,
      "loss": 0.1758,
      "step": 9230
    },
    {
      "epoch": 1.6633663366336635,
      "grad_norm": 0.2540174126625061,
      "learning_rate": 4.922500151031016e-05,
      "loss": 0.1769,
      "step": 9240
    },
    {
      "epoch": 1.6651665166516652,
      "grad_norm": 0.23635563254356384,
      "learning_rate": 4.912028214933409e-05,
      "loss": 0.179,
      "step": 9250
    },
    {
      "epoch": 1.666966696669667,
      "grad_norm": 0.2214985191822052,
      "learning_rate": 4.9015566648260124e-05,
      "loss": 0.1753,
      "step": 9260
    },
    {
      "epoch": 1.6687668766876689,
      "grad_norm": 0.22514405846595764,
      "learning_rate": 4.891085546654422e-05,
      "loss": 0.1738,
      "step": 9270
    },
    {
      "epoch": 1.6705670567056705,
      "grad_norm": 0.22651265561580658,
      "learning_rate": 4.880614906362336e-05,
      "loss": 0.1783,
      "step": 9280
    },
    {
      "epoch": 1.6723672367236724,
      "grad_norm": 0.2172718048095703,
      "learning_rate": 4.870144789891355e-05,
      "loss": 0.1717,
      "step": 9290
    },
    {
      "epoch": 1.6741674167416742,
      "grad_norm": 0.22589460015296936,
      "learning_rate": 4.859675243180781e-05,
      "loss": 0.1756,
      "step": 9300
    },
    {
      "epoch": 1.6759675967596759,
      "grad_norm": 0.22840650379657745,
      "learning_rate": 4.84920631216742e-05,
      "loss": 0.1751,
      "step": 9310
    },
    {
      "epoch": 1.6777677767776777,
      "grad_norm": 0.2702333927154541,
      "learning_rate": 4.83873804278537e-05,
      "loss": 0.1784,
      "step": 9320
    },
    {
      "epoch": 1.6795679567956796,
      "grad_norm": 0.2575435936450958,
      "learning_rate": 4.8282704809658305e-05,
      "loss": 0.1768,
      "step": 9330
    },
    {
      "epoch": 1.6813681368136812,
      "grad_norm": 0.2501317262649536,
      "learning_rate": 4.817803672636898e-05,
      "loss": 0.1826,
      "step": 9340
    },
    {
      "epoch": 1.6831683168316833,
      "grad_norm": 0.2151915729045868,
      "learning_rate": 4.8073376637233566e-05,
      "loss": 0.1801,
      "step": 9350
    },
    {
      "epoch": 1.684968496849685,
      "grad_norm": 0.24850359559059143,
      "learning_rate": 4.79687250014649e-05,
      "loss": 0.176,
      "step": 9360
    },
    {
      "epoch": 1.6867686768676866,
      "grad_norm": 0.23857606947422028,
      "learning_rate": 4.786408227823867e-05,
      "loss": 0.1786,
      "step": 9370
    },
    {
      "epoch": 1.6885688568856887,
      "grad_norm": 0.24597570300102234,
      "learning_rate": 4.77594489266915e-05,
      "loss": 0.1723,
      "step": 9380
    },
    {
      "epoch": 1.6903690369036903,
      "grad_norm": 0.21031427383422852,
      "learning_rate": 4.7654825405918874e-05,
      "loss": 0.1767,
      "step": 9390
    },
    {
      "epoch": 1.6921692169216922,
      "grad_norm": 0.2448267936706543,
      "learning_rate": 4.755021217497314e-05,
      "loss": 0.18,
      "step": 9400
    },
    {
      "epoch": 1.693969396939694,
      "grad_norm": 0.23764324188232422,
      "learning_rate": 4.7445609692861495e-05,
      "loss": 0.1789,
      "step": 9410
    },
    {
      "epoch": 1.6957695769576957,
      "grad_norm": 0.22222575545310974,
      "learning_rate": 4.734101841854402e-05,
      "loss": 0.1738,
      "step": 9420
    },
    {
      "epoch": 1.6975697569756976,
      "grad_norm": 0.23777905106544495,
      "learning_rate": 4.723643881093154e-05,
      "loss": 0.1689,
      "step": 9430
    },
    {
      "epoch": 1.6993699369936994,
      "grad_norm": 0.23201711475849152,
      "learning_rate": 4.713187132888375e-05,
      "loss": 0.1779,
      "step": 9440
    },
    {
      "epoch": 1.701170117011701,
      "grad_norm": 0.23893849551677704,
      "learning_rate": 4.7027316431207146e-05,
      "loss": 0.1766,
      "step": 9450
    },
    {
      "epoch": 1.702970297029703,
      "grad_norm": 0.2249612659215927,
      "learning_rate": 4.6922774576652956e-05,
      "loss": 0.1764,
      "step": 9460
    },
    {
      "epoch": 1.7047704770477048,
      "grad_norm": 0.22866615653038025,
      "learning_rate": 4.681824622391522e-05,
      "loss": 0.1743,
      "step": 9470
    },
    {
      "epoch": 1.7065706570657064,
      "grad_norm": 0.22281374037265778,
      "learning_rate": 4.671373183162874e-05,
      "loss": 0.173,
      "step": 9480
    },
    {
      "epoch": 1.7083708370837085,
      "grad_norm": 0.23539836704730988,
      "learning_rate": 4.6609231858367026e-05,
      "loss": 0.1742,
      "step": 9490
    },
    {
      "epoch": 1.7101710171017102,
      "grad_norm": 0.227435901761055,
      "learning_rate": 4.6504746762640386e-05,
      "loss": 0.1688,
      "step": 9500
    },
    {
      "epoch": 1.7101710171017102,
      "eval_loss": 0.1880747377872467,
      "eval_runtime": 317.1804,
      "eval_samples_per_second": 124.535,
      "eval_steps_per_second": 3.894,
      "step": 9500
    },
    {
      "epoch": 1.711971197119712,
      "grad_norm": 0.24906155467033386,
      "learning_rate": 4.640027700289376e-05,
      "loss": 0.1718,
      "step": 9510
    },
    {
      "epoch": 1.7137713771377139,
      "grad_norm": 0.21301382780075073,
      "learning_rate": 4.629582303750488e-05,
      "loss": 0.1734,
      "step": 9520
    },
    {
      "epoch": 1.7155715571557155,
      "grad_norm": 0.20678886771202087,
      "learning_rate": 4.619138532478218e-05,
      "loss": 0.1765,
      "step": 9530
    },
    {
      "epoch": 1.7173717371737174,
      "grad_norm": 0.23368880152702332,
      "learning_rate": 4.6086964322962695e-05,
      "loss": 0.1789,
      "step": 9540
    },
    {
      "epoch": 1.7191719171917192,
      "grad_norm": 0.25210657715797424,
      "learning_rate": 4.598256049021021e-05,
      "loss": 0.1738,
      "step": 9550
    },
    {
      "epoch": 1.7209720972097209,
      "grad_norm": 0.23559825122356415,
      "learning_rate": 4.58781742846132e-05,
      "loss": 0.1797,
      "step": 9560
    },
    {
      "epoch": 1.7227722772277227,
      "grad_norm": 0.23880474269390106,
      "learning_rate": 4.577380616418272e-05,
      "loss": 0.1763,
      "step": 9570
    },
    {
      "epoch": 1.7245724572457246,
      "grad_norm": 0.24170225858688354,
      "learning_rate": 4.566945658685052e-05,
      "loss": 0.1702,
      "step": 9580
    },
    {
      "epoch": 1.7263726372637263,
      "grad_norm": 0.23340678215026855,
      "learning_rate": 4.556512601046701e-05,
      "loss": 0.1734,
      "step": 9590
    },
    {
      "epoch": 1.7281728172817283,
      "grad_norm": 0.22668765485286713,
      "learning_rate": 4.546081489279917e-05,
      "loss": 0.1775,
      "step": 9600
    },
    {
      "epoch": 1.72997299729973,
      "grad_norm": 0.19922375679016113,
      "learning_rate": 4.535652369152865e-05,
      "loss": 0.1739,
      "step": 9610
    },
    {
      "epoch": 1.7317731773177316,
      "grad_norm": 0.23636847734451294,
      "learning_rate": 4.5252252864249714e-05,
      "loss": 0.1736,
      "step": 9620
    },
    {
      "epoch": 1.7335733573357337,
      "grad_norm": 0.23513418436050415,
      "learning_rate": 4.51480028684672e-05,
      "loss": 0.1786,
      "step": 9630
    },
    {
      "epoch": 1.7353735373537353,
      "grad_norm": 0.2274463027715683,
      "learning_rate": 4.504377416159457e-05,
      "loss": 0.1753,
      "step": 9640
    },
    {
      "epoch": 1.7371737173717372,
      "grad_norm": 0.24632039666175842,
      "learning_rate": 4.493956720095185e-05,
      "loss": 0.1735,
      "step": 9650
    },
    {
      "epoch": 1.738973897389739,
      "grad_norm": 0.23865923285484314,
      "learning_rate": 4.483538244376368e-05,
      "loss": 0.1741,
      "step": 9660
    },
    {
      "epoch": 1.7407740774077407,
      "grad_norm": 0.2405191957950592,
      "learning_rate": 4.473122034715729e-05,
      "loss": 0.1778,
      "step": 9670
    },
    {
      "epoch": 1.7425742574257426,
      "grad_norm": 0.2187766134738922,
      "learning_rate": 4.462708136816044e-05,
      "loss": 0.1705,
      "step": 9680
    },
    {
      "epoch": 1.7443744374437444,
      "grad_norm": 0.24439381062984467,
      "learning_rate": 4.452296596369948e-05,
      "loss": 0.1755,
      "step": 9690
    },
    {
      "epoch": 1.746174617461746,
      "grad_norm": 0.20013004541397095,
      "learning_rate": 4.4418874590597346e-05,
      "loss": 0.1822,
      "step": 9700
    },
    {
      "epoch": 1.747974797479748,
      "grad_norm": 0.24808885157108307,
      "learning_rate": 4.431480770557147e-05,
      "loss": 0.1785,
      "step": 9710
    },
    {
      "epoch": 1.7497749774977498,
      "grad_norm": 0.24364888668060303,
      "learning_rate": 4.421076576523191e-05,
      "loss": 0.1789,
      "step": 9720
    },
    {
      "epoch": 1.7515751575157514,
      "grad_norm": 0.25402018427848816,
      "learning_rate": 4.4106749226079236e-05,
      "loss": 0.1777,
      "step": 9730
    },
    {
      "epoch": 1.7533753375337535,
      "grad_norm": 0.2447977364063263,
      "learning_rate": 4.4002758544502564e-05,
      "loss": 0.1752,
      "step": 9740
    },
    {
      "epoch": 1.7551755175517552,
      "grad_norm": 0.22191032767295837,
      "learning_rate": 4.389879417677757e-05,
      "loss": 0.1767,
      "step": 9750
    },
    {
      "epoch": 1.756975697569757,
      "grad_norm": 0.2131752371788025,
      "learning_rate": 4.379485657906449e-05,
      "loss": 0.1739,
      "step": 9760
    },
    {
      "epoch": 1.7587758775877589,
      "grad_norm": 0.2193400263786316,
      "learning_rate": 4.369094620740604e-05,
      "loss": 0.1739,
      "step": 9770
    },
    {
      "epoch": 1.7605760576057605,
      "grad_norm": 0.2476017028093338,
      "learning_rate": 4.3587063517725565e-05,
      "loss": 0.1718,
      "step": 9780
    },
    {
      "epoch": 1.7623762376237624,
      "grad_norm": 0.23215621709823608,
      "learning_rate": 4.348320896582485e-05,
      "loss": 0.1779,
      "step": 9790
    },
    {
      "epoch": 1.7641764176417642,
      "grad_norm": 0.2233273833990097,
      "learning_rate": 4.337938300738232e-05,
      "loss": 0.1757,
      "step": 9800
    },
    {
      "epoch": 1.765976597659766,
      "grad_norm": 0.23734469711780548,
      "learning_rate": 4.32755860979509e-05,
      "loss": 0.1791,
      "step": 9810
    },
    {
      "epoch": 1.7677767776777678,
      "grad_norm": 0.2430730164051056,
      "learning_rate": 4.317181869295602e-05,
      "loss": 0.1792,
      "step": 9820
    },
    {
      "epoch": 1.7695769576957696,
      "grad_norm": 0.236197829246521,
      "learning_rate": 4.30680812476937e-05,
      "loss": 0.1741,
      "step": 9830
    },
    {
      "epoch": 1.7713771377137713,
      "grad_norm": 0.21487043797969818,
      "learning_rate": 4.2964374217328536e-05,
      "loss": 0.1787,
      "step": 9840
    },
    {
      "epoch": 1.7731773177317733,
      "grad_norm": 0.24639050662517548,
      "learning_rate": 4.286069805689158e-05,
      "loss": 0.1691,
      "step": 9850
    },
    {
      "epoch": 1.774977497749775,
      "grad_norm": 0.2267015129327774,
      "learning_rate": 4.275705322127851e-05,
      "loss": 0.1735,
      "step": 9860
    },
    {
      "epoch": 1.7767776777677766,
      "grad_norm": 0.2464321106672287,
      "learning_rate": 4.265344016524758e-05,
      "loss": 0.175,
      "step": 9870
    },
    {
      "epoch": 1.7785778577857787,
      "grad_norm": 0.23366591334342957,
      "learning_rate": 4.254985934341751e-05,
      "loss": 0.1778,
      "step": 9880
    },
    {
      "epoch": 1.7803780378037803,
      "grad_norm": 0.22827142477035522,
      "learning_rate": 4.2446311210265674e-05,
      "loss": 0.1726,
      "step": 9890
    },
    {
      "epoch": 1.7821782178217822,
      "grad_norm": 0.22453920543193817,
      "learning_rate": 4.2342796220126014e-05,
      "loss": 0.1776,
      "step": 9900
    },
    {
      "epoch": 1.783978397839784,
      "grad_norm": 0.22872792184352875,
      "learning_rate": 4.223931482718699e-05,
      "loss": 0.179,
      "step": 9910
    },
    {
      "epoch": 1.7857785778577857,
      "grad_norm": 0.21978744864463806,
      "learning_rate": 4.2135867485489714e-05,
      "loss": 0.1792,
      "step": 9920
    },
    {
      "epoch": 1.7875787578757876,
      "grad_norm": 0.2594098448753357,
      "learning_rate": 4.2032454648925855e-05,
      "loss": 0.1745,
      "step": 9930
    },
    {
      "epoch": 1.7893789378937894,
      "grad_norm": 0.24225327372550964,
      "learning_rate": 4.1929076771235695e-05,
      "loss": 0.1802,
      "step": 9940
    },
    {
      "epoch": 1.791179117911791,
      "grad_norm": 0.2371714562177658,
      "learning_rate": 4.182573430600615e-05,
      "loss": 0.1741,
      "step": 9950
    },
    {
      "epoch": 1.792979297929793,
      "grad_norm": 0.20169755816459656,
      "learning_rate": 4.172242770666871e-05,
      "loss": 0.1713,
      "step": 9960
    },
    {
      "epoch": 1.7947794779477948,
      "grad_norm": 0.2185191959142685,
      "learning_rate": 4.161915742649755e-05,
      "loss": 0.1717,
      "step": 9970
    },
    {
      "epoch": 1.7965796579657964,
      "grad_norm": 0.2104547917842865,
      "learning_rate": 4.151592391860746e-05,
      "loss": 0.1741,
      "step": 9980
    },
    {
      "epoch": 1.7983798379837985,
      "grad_norm": 0.235412135720253,
      "learning_rate": 4.141272763595188e-05,
      "loss": 0.1747,
      "step": 9990
    },
    {
      "epoch": 1.8001800180018002,
      "grad_norm": 0.22280338406562805,
      "learning_rate": 4.1309569031320944e-05,
      "loss": 0.1754,
      "step": 10000
    },
    {
      "epoch": 1.8001800180018002,
      "eval_loss": 0.18683438003063202,
      "eval_runtime": 316.6014,
      "eval_samples_per_second": 124.763,
      "eval_steps_per_second": 3.901,
      "step": 10000
    },
    {
      "epoch": 1.801980198019802,
      "grad_norm": 0.22414715588092804,
      "learning_rate": 4.120644855733945e-05,
      "loss": 0.174,
      "step": 10010
    },
    {
      "epoch": 1.8037803780378039,
      "grad_norm": 0.22347165644168854,
      "learning_rate": 4.1103366666464887e-05,
      "loss": 0.1722,
      "step": 10020
    },
    {
      "epoch": 1.8055805580558055,
      "grad_norm": 0.21889306604862213,
      "learning_rate": 4.100032381098547e-05,
      "loss": 0.18,
      "step": 10030
    },
    {
      "epoch": 1.8073807380738074,
      "grad_norm": 0.2106132060289383,
      "learning_rate": 4.089732044301814e-05,
      "loss": 0.1711,
      "step": 10040
    },
    {
      "epoch": 1.8091809180918093,
      "grad_norm": 0.22404809296131134,
      "learning_rate": 4.079435701450654e-05,
      "loss": 0.181,
      "step": 10050
    },
    {
      "epoch": 1.810981098109811,
      "grad_norm": 0.22029125690460205,
      "learning_rate": 4.0691433977219165e-05,
      "loss": 0.1774,
      "step": 10060
    },
    {
      "epoch": 1.8127812781278128,
      "grad_norm": 0.2192152589559555,
      "learning_rate": 4.058855178274715e-05,
      "loss": 0.1776,
      "step": 10070
    },
    {
      "epoch": 1.8145814581458146,
      "grad_norm": 0.253188818693161,
      "learning_rate": 4.048571088250256e-05,
      "loss": 0.1771,
      "step": 10080
    },
    {
      "epoch": 1.8163816381638163,
      "grad_norm": 0.23206916451454163,
      "learning_rate": 4.038291172771622e-05,
      "loss": 0.1675,
      "step": 10090
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 0.21173018217086792,
      "learning_rate": 4.028015476943573e-05,
      "loss": 0.1754,
      "step": 10100
    },
    {
      "epoch": 1.81998199819982,
      "grad_norm": 0.2386562079191208,
      "learning_rate": 4.0177440458523656e-05,
      "loss": 0.1845,
      "step": 10110
    },
    {
      "epoch": 1.8217821782178216,
      "grad_norm": 0.23119774460792542,
      "learning_rate": 4.0074769245655404e-05,
      "loss": 0.1742,
      "step": 10120
    },
    {
      "epoch": 1.8235823582358237,
      "grad_norm": 0.22666972875595093,
      "learning_rate": 3.9972141581317196e-05,
      "loss": 0.1758,
      "step": 10130
    },
    {
      "epoch": 1.8253825382538253,
      "grad_norm": 0.2450668066740036,
      "learning_rate": 3.986955791580431e-05,
      "loss": 0.1737,
      "step": 10140
    },
    {
      "epoch": 1.8271827182718272,
      "grad_norm": 0.24199508130550385,
      "learning_rate": 3.9767018699218905e-05,
      "loss": 0.169,
      "step": 10150
    },
    {
      "epoch": 1.828982898289829,
      "grad_norm": 0.25337550044059753,
      "learning_rate": 3.966452438146808e-05,
      "loss": 0.1747,
      "step": 10160
    },
    {
      "epoch": 1.8307830783078307,
      "grad_norm": 0.220970019698143,
      "learning_rate": 3.956207541226199e-05,
      "loss": 0.1709,
      "step": 10170
    },
    {
      "epoch": 1.8325832583258326,
      "grad_norm": 0.2154323011636734,
      "learning_rate": 3.945967224111184e-05,
      "loss": 0.1748,
      "step": 10180
    },
    {
      "epoch": 1.8343834383438344,
      "grad_norm": 0.24402233958244324,
      "learning_rate": 3.935731531732778e-05,
      "loss": 0.1728,
      "step": 10190
    },
    {
      "epoch": 1.836183618361836,
      "grad_norm": 0.23580339550971985,
      "learning_rate": 3.9255005090017156e-05,
      "loss": 0.176,
      "step": 10200
    },
    {
      "epoch": 1.837983798379838,
      "grad_norm": 0.26192957162857056,
      "learning_rate": 3.915274200808235e-05,
      "loss": 0.1744,
      "step": 10210
    },
    {
      "epoch": 1.8397839783978398,
      "grad_norm": 0.23620183765888214,
      "learning_rate": 3.9050526520218925e-05,
      "loss": 0.1713,
      "step": 10220
    },
    {
      "epoch": 1.8415841584158414,
      "grad_norm": 0.23975889384746552,
      "learning_rate": 3.894835907491363e-05,
      "loss": 0.1851,
      "step": 10230
    },
    {
      "epoch": 1.8433843384338435,
      "grad_norm": 0.20708821713924408,
      "learning_rate": 3.884624012044235e-05,
      "loss": 0.1759,
      "step": 10240
    },
    {
      "epoch": 1.8451845184518452,
      "grad_norm": 0.22034063935279846,
      "learning_rate": 3.874417010486829e-05,
      "loss": 0.1766,
      "step": 10250
    },
    {
      "epoch": 1.846984698469847,
      "grad_norm": 0.23165035247802734,
      "learning_rate": 3.864214947603989e-05,
      "loss": 0.1714,
      "step": 10260
    },
    {
      "epoch": 1.848784878487849,
      "grad_norm": 0.21247535943984985,
      "learning_rate": 3.8540178681588894e-05,
      "loss": 0.1762,
      "step": 10270
    },
    {
      "epoch": 1.8505850585058505,
      "grad_norm": 0.20738428831100464,
      "learning_rate": 3.8438258168928395e-05,
      "loss": 0.177,
      "step": 10280
    },
    {
      "epoch": 1.8523852385238524,
      "grad_norm": 0.2676404118537903,
      "learning_rate": 3.833638838525088e-05,
      "loss": 0.1805,
      "step": 10290
    },
    {
      "epoch": 1.8541854185418543,
      "grad_norm": 0.23339377343654633,
      "learning_rate": 3.823456977752623e-05,
      "loss": 0.1738,
      "step": 10300
    },
    {
      "epoch": 1.855985598559856,
      "grad_norm": 0.19044902920722961,
      "learning_rate": 3.81328027924998e-05,
      "loss": 0.1679,
      "step": 10310
    },
    {
      "epoch": 1.8577857785778578,
      "grad_norm": 0.23274962604045868,
      "learning_rate": 3.803108787669046e-05,
      "loss": 0.1678,
      "step": 10320
    },
    {
      "epoch": 1.8595859585958596,
      "grad_norm": 0.22667695581912994,
      "learning_rate": 3.792942547638856e-05,
      "loss": 0.1705,
      "step": 10330
    },
    {
      "epoch": 1.8613861386138613,
      "grad_norm": 0.23037481307983398,
      "learning_rate": 3.782781603765409e-05,
      "loss": 0.1787,
      "step": 10340
    },
    {
      "epoch": 1.8631863186318633,
      "grad_norm": 0.23116719722747803,
      "learning_rate": 3.7726260006314626e-05,
      "loss": 0.1772,
      "step": 10350
    },
    {
      "epoch": 1.864986498649865,
      "grad_norm": 0.22823503613471985,
      "learning_rate": 3.762475782796344e-05,
      "loss": 0.1681,
      "step": 10360
    },
    {
      "epoch": 1.8667866786678666,
      "grad_norm": 0.23249377310276031,
      "learning_rate": 3.752330994795749e-05,
      "loss": 0.176,
      "step": 10370
    },
    {
      "epoch": 1.8685868586858687,
      "grad_norm": 0.21173132956027985,
      "learning_rate": 3.7421916811415495e-05,
      "loss": 0.1736,
      "step": 10380
    },
    {
      "epoch": 1.8703870387038704,
      "grad_norm": 0.2286842167377472,
      "learning_rate": 3.7320578863215996e-05,
      "loss": 0.1665,
      "step": 10390
    },
    {
      "epoch": 1.8721872187218722,
      "grad_norm": 0.228562593460083,
      "learning_rate": 3.721929654799538e-05,
      "loss": 0.1794,
      "step": 10400
    },
    {
      "epoch": 1.873987398739874,
      "grad_norm": 0.21749845147132874,
      "learning_rate": 3.711807031014589e-05,
      "loss": 0.1807,
      "step": 10410
    },
    {
      "epoch": 1.8757875787578757,
      "grad_norm": 0.25057223439216614,
      "learning_rate": 3.70169005938138e-05,
      "loss": 0.1796,
      "step": 10420
    },
    {
      "epoch": 1.8775877587758776,
      "grad_norm": 0.20894350111484528,
      "learning_rate": 3.6915787842897354e-05,
      "loss": 0.1726,
      "step": 10430
    },
    {
      "epoch": 1.8793879387938794,
      "grad_norm": 0.22257515788078308,
      "learning_rate": 3.6814732501044795e-05,
      "loss": 0.1722,
      "step": 10440
    },
    {
      "epoch": 1.881188118811881,
      "grad_norm": 0.2550674378871918,
      "learning_rate": 3.6713735011652574e-05,
      "loss": 0.1733,
      "step": 10450
    },
    {
      "epoch": 1.882988298829883,
      "grad_norm": 0.21964597702026367,
      "learning_rate": 3.661279581786324e-05,
      "loss": 0.1773,
      "step": 10460
    },
    {
      "epoch": 1.8847884788478848,
      "grad_norm": 0.24217933416366577,
      "learning_rate": 3.6511915362563555e-05,
      "loss": 0.1759,
      "step": 10470
    },
    {
      "epoch": 1.8865886588658864,
      "grad_norm": 0.235865518450737,
      "learning_rate": 3.641109408838262e-05,
      "loss": 0.1698,
      "step": 10480
    },
    {
      "epoch": 1.8883888388838885,
      "grad_norm": 0.2442626804113388,
      "learning_rate": 3.631033243768978e-05,
      "loss": 0.1731,
      "step": 10490
    },
    {
      "epoch": 1.8901890189018902,
      "grad_norm": 0.21478553116321564,
      "learning_rate": 3.620963085259284e-05,
      "loss": 0.1743,
      "step": 10500
    },
    {
      "epoch": 1.8901890189018902,
      "eval_loss": 0.1854907125234604,
      "eval_runtime": 316.7952,
      "eval_samples_per_second": 124.686,
      "eval_steps_per_second": 3.898,
      "step": 10500
    },
    {
      "epoch": 1.891989198919892,
      "grad_norm": 0.24597007036209106,
      "learning_rate": 3.610898977493605e-05,
      "loss": 0.1785,
      "step": 10510
    },
    {
      "epoch": 1.893789378937894,
      "grad_norm": 0.21002976596355438,
      "learning_rate": 3.6008409646298136e-05,
      "loss": 0.1703,
      "step": 10520
    },
    {
      "epoch": 1.8955895589558955,
      "grad_norm": 0.2346259206533432,
      "learning_rate": 3.590789090799044e-05,
      "loss": 0.1685,
      "step": 10530
    },
    {
      "epoch": 1.8973897389738974,
      "grad_norm": 0.22118902206420898,
      "learning_rate": 3.580743400105493e-05,
      "loss": 0.1796,
      "step": 10540
    },
    {
      "epoch": 1.8991899189918993,
      "grad_norm": 0.21850363910198212,
      "learning_rate": 3.5707039366262266e-05,
      "loss": 0.1691,
      "step": 10550
    },
    {
      "epoch": 1.900990099009901,
      "grad_norm": 0.2250548154115677,
      "learning_rate": 3.56067074441099e-05,
      "loss": 0.1767,
      "step": 10560
    },
    {
      "epoch": 1.9027902790279028,
      "grad_norm": 0.2443554699420929,
      "learning_rate": 3.550643867482013e-05,
      "loss": 0.1714,
      "step": 10570
    },
    {
      "epoch": 1.9045904590459046,
      "grad_norm": 0.22002053260803223,
      "learning_rate": 3.540623349833812e-05,
      "loss": 0.1756,
      "step": 10580
    },
    {
      "epoch": 1.9063906390639063,
      "grad_norm": 0.22920344769954681,
      "learning_rate": 3.5306092354330045e-05,
      "loss": 0.1755,
      "step": 10590
    },
    {
      "epoch": 1.9081908190819084,
      "grad_norm": 0.22373726963996887,
      "learning_rate": 3.5206015682181114e-05,
      "loss": 0.1739,
      "step": 10600
    },
    {
      "epoch": 1.90999099909991,
      "grad_norm": 0.25252193212509155,
      "learning_rate": 3.510600392099366e-05,
      "loss": 0.1758,
      "step": 10610
    },
    {
      "epoch": 1.9117911791179116,
      "grad_norm": 0.24120710790157318,
      "learning_rate": 3.500605750958522e-05,
      "loss": 0.1704,
      "step": 10620
    },
    {
      "epoch": 1.9135913591359137,
      "grad_norm": 0.24715490639209747,
      "learning_rate": 3.4906176886486565e-05,
      "loss": 0.1751,
      "step": 10630
    },
    {
      "epoch": 1.9153915391539154,
      "grad_norm": 0.2468406707048416,
      "learning_rate": 3.480636248993984e-05,
      "loss": 0.1747,
      "step": 10640
    },
    {
      "epoch": 1.9171917191719172,
      "grad_norm": 0.21872608363628387,
      "learning_rate": 3.470661475789661e-05,
      "loss": 0.173,
      "step": 10650
    },
    {
      "epoch": 1.918991899189919,
      "grad_norm": 0.25274887681007385,
      "learning_rate": 3.460693412801591e-05,
      "loss": 0.1738,
      "step": 10660
    },
    {
      "epoch": 1.9207920792079207,
      "grad_norm": 0.23055987060070038,
      "learning_rate": 3.450732103766239e-05,
      "loss": 0.1713,
      "step": 10670
    },
    {
      "epoch": 1.9225922592259226,
      "grad_norm": 0.2517528533935547,
      "learning_rate": 3.440777592390434e-05,
      "loss": 0.1829,
      "step": 10680
    },
    {
      "epoch": 1.9243924392439244,
      "grad_norm": 0.22522194683551788,
      "learning_rate": 3.430829922351178e-05,
      "loss": 0.1724,
      "step": 10690
    },
    {
      "epoch": 1.926192619261926,
      "grad_norm": 0.24157972633838654,
      "learning_rate": 3.420889137295459e-05,
      "loss": 0.1731,
      "step": 10700
    },
    {
      "epoch": 1.927992799279928,
      "grad_norm": 0.2397865504026413,
      "learning_rate": 3.410955280840054e-05,
      "loss": 0.1688,
      "step": 10710
    },
    {
      "epoch": 1.9297929792979298,
      "grad_norm": 0.26058289408683777,
      "learning_rate": 3.401028396571337e-05,
      "loss": 0.1844,
      "step": 10720
    },
    {
      "epoch": 1.9315931593159315,
      "grad_norm": 0.2441859096288681,
      "learning_rate": 3.391108528045097e-05,
      "loss": 0.1813,
      "step": 10730
    },
    {
      "epoch": 1.9333933393339335,
      "grad_norm": 0.2270483523607254,
      "learning_rate": 3.381195718786335e-05,
      "loss": 0.1695,
      "step": 10740
    },
    {
      "epoch": 1.9351935193519352,
      "grad_norm": 0.21849800646305084,
      "learning_rate": 3.371290012289078e-05,
      "loss": 0.1754,
      "step": 10750
    },
    {
      "epoch": 1.936993699369937,
      "grad_norm": 0.22395899891853333,
      "learning_rate": 3.3613914520161935e-05,
      "loss": 0.1682,
      "step": 10760
    },
    {
      "epoch": 1.938793879387939,
      "grad_norm": 0.2503412067890167,
      "learning_rate": 3.351500081399187e-05,
      "loss": 0.1746,
      "step": 10770
    },
    {
      "epoch": 1.9405940594059405,
      "grad_norm": 0.20870137214660645,
      "learning_rate": 3.341615943838022e-05,
      "loss": 0.1731,
      "step": 10780
    },
    {
      "epoch": 1.9423942394239424,
      "grad_norm": 0.22328992187976837,
      "learning_rate": 3.331739082700931e-05,
      "loss": 0.1676,
      "step": 10790
    },
    {
      "epoch": 1.9441944194419443,
      "grad_norm": 0.22773022949695587,
      "learning_rate": 3.321869541324209e-05,
      "loss": 0.1693,
      "step": 10800
    },
    {
      "epoch": 1.945994599459946,
      "grad_norm": 0.2198358029127121,
      "learning_rate": 3.312007363012041e-05,
      "loss": 0.1701,
      "step": 10810
    },
    {
      "epoch": 1.9477947794779478,
      "grad_norm": 0.23153240978717804,
      "learning_rate": 3.302152591036309e-05,
      "loss": 0.1723,
      "step": 10820
    },
    {
      "epoch": 1.9495949594959496,
      "grad_norm": 0.19587822258472443,
      "learning_rate": 3.2923052686363874e-05,
      "loss": 0.1636,
      "step": 10830
    },
    {
      "epoch": 1.9513951395139513,
      "grad_norm": 0.21134057641029358,
      "learning_rate": 3.2824654390189744e-05,
      "loss": 0.171,
      "step": 10840
    },
    {
      "epoch": 1.9531953195319534,
      "grad_norm": 0.2305329144001007,
      "learning_rate": 3.272633145357891e-05,
      "loss": 0.1796,
      "step": 10850
    },
    {
      "epoch": 1.954995499549955,
      "grad_norm": 0.23215073347091675,
      "learning_rate": 3.262808430793886e-05,
      "loss": 0.1704,
      "step": 10860
    },
    {
      "epoch": 1.9567956795679566,
      "grad_norm": 0.22286607325077057,
      "learning_rate": 3.252991338434462e-05,
      "loss": 0.1725,
      "step": 10870
    },
    {
      "epoch": 1.9585958595859587,
      "grad_norm": 0.23076097667217255,
      "learning_rate": 3.2431819113536746e-05,
      "loss": 0.1722,
      "step": 10880
    },
    {
      "epoch": 1.9603960396039604,
      "grad_norm": 0.22146916389465332,
      "learning_rate": 3.233380192591946e-05,
      "loss": 0.1744,
      "step": 10890
    },
    {
      "epoch": 1.9621962196219622,
      "grad_norm": 0.21736310422420502,
      "learning_rate": 3.223586225155879e-05,
      "loss": 0.1651,
      "step": 10900
    },
    {
      "epoch": 1.963996399639964,
      "grad_norm": 0.2262491136789322,
      "learning_rate": 3.213800052018062e-05,
      "loss": 0.1763,
      "step": 10910
    },
    {
      "epoch": 1.9657965796579657,
      "grad_norm": 0.23157699406147003,
      "learning_rate": 3.204021716116891e-05,
      "loss": 0.1727,
      "step": 10920
    },
    {
      "epoch": 1.9675967596759676,
      "grad_norm": 0.23087245225906372,
      "learning_rate": 3.19425126035637e-05,
      "loss": 0.1754,
      "step": 10930
    },
    {
      "epoch": 1.9693969396939695,
      "grad_norm": 0.24254116415977478,
      "learning_rate": 3.184488727605929e-05,
      "loss": 0.1766,
      "step": 10940
    },
    {
      "epoch": 1.971197119711971,
      "grad_norm": 0.22418494522571564,
      "learning_rate": 3.174734160700235e-05,
      "loss": 0.1795,
      "step": 10950
    },
    {
      "epoch": 1.972997299729973,
      "grad_norm": 0.23182541131973267,
      "learning_rate": 3.1649876024390034e-05,
      "loss": 0.1759,
      "step": 10960
    },
    {
      "epoch": 1.9747974797479748,
      "grad_norm": 0.219981387257576,
      "learning_rate": 3.15524909558681e-05,
      "loss": 0.1744,
      "step": 10970
    },
    {
      "epoch": 1.9765976597659765,
      "grad_norm": 0.23748469352722168,
      "learning_rate": 3.1455186828729045e-05,
      "loss": 0.1782,
      "step": 10980
    },
    {
      "epoch": 1.9783978397839785,
      "grad_norm": 0.2480928748846054,
      "learning_rate": 3.135796406991022e-05,
      "loss": 0.1717,
      "step": 10990
    },
    {
      "epoch": 1.9801980198019802,
      "grad_norm": 0.24287985265254974,
      "learning_rate": 3.126082310599194e-05,
      "loss": 0.1749,
      "step": 11000
    },
    {
      "epoch": 1.9801980198019802,
      "eval_loss": 0.1843818873167038,
      "eval_runtime": 316.8757,
      "eval_samples_per_second": 124.655,
      "eval_steps_per_second": 3.897,
      "step": 11000
    },
    {
      "epoch": 1.981998199819982,
      "grad_norm": 0.2539142668247223,
      "learning_rate": 3.116376436319567e-05,
      "loss": 0.1726,
      "step": 11010
    },
    {
      "epoch": 1.983798379837984,
      "grad_norm": 0.24498192965984344,
      "learning_rate": 3.10667882673821e-05,
      "loss": 0.1782,
      "step": 11020
    },
    {
      "epoch": 1.9855985598559855,
      "grad_norm": 0.21446481347084045,
      "learning_rate": 3.096989524404926e-05,
      "loss": 0.1726,
      "step": 11030
    },
    {
      "epoch": 1.9873987398739874,
      "grad_norm": 0.2369292825460434,
      "learning_rate": 3.087308571833076e-05,
      "loss": 0.1747,
      "step": 11040
    },
    {
      "epoch": 1.9891989198919893,
      "grad_norm": 0.2085305154323578,
      "learning_rate": 3.0776360114993765e-05,
      "loss": 0.1733,
      "step": 11050
    },
    {
      "epoch": 1.990999099909991,
      "grad_norm": 0.2066744863986969,
      "learning_rate": 3.0679718858437256e-05,
      "loss": 0.1654,
      "step": 11060
    },
    {
      "epoch": 1.9927992799279928,
      "grad_norm": 0.24414604902267456,
      "learning_rate": 3.0583162372690185e-05,
      "loss": 0.1699,
      "step": 11070
    },
    {
      "epoch": 1.9945994599459946,
      "grad_norm": 0.2470005601644516,
      "learning_rate": 3.0486691081409457e-05,
      "loss": 0.1732,
      "step": 11080
    },
    {
      "epoch": 1.9963996399639963,
      "grad_norm": 0.205951526761055,
      "learning_rate": 3.0390305407878216e-05,
      "loss": 0.1694,
      "step": 11090
    },
    {
      "epoch": 1.9981998199819984,
      "grad_norm": 0.24321258068084717,
      "learning_rate": 3.029400577500401e-05,
      "loss": 0.1663,
      "step": 11100
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.21577133238315582,
      "learning_rate": 3.0197792605316743e-05,
      "loss": 0.1746,
      "step": 11110
    },
    {
      "epoch": 2.0018001800180016,
      "grad_norm": 0.2273331582546234,
      "learning_rate": 3.0101666320967025e-05,
      "loss": 0.1534,
      "step": 11120
    },
    {
      "epoch": 2.0036003600360037,
      "grad_norm": 0.2051791399717331,
      "learning_rate": 3.0005627343724286e-05,
      "loss": 0.1479,
      "step": 11130
    },
    {
      "epoch": 2.0054005400540054,
      "grad_norm": 0.2317444384098053,
      "learning_rate": 2.990967609497477e-05,
      "loss": 0.1508,
      "step": 11140
    },
    {
      "epoch": 2.007200720072007,
      "grad_norm": 0.2426849603652954,
      "learning_rate": 2.981381299571987e-05,
      "loss": 0.1581,
      "step": 11150
    },
    {
      "epoch": 2.009000900090009,
      "grad_norm": 0.23266533017158508,
      "learning_rate": 2.9718038466574228e-05,
      "loss": 0.1549,
      "step": 11160
    },
    {
      "epoch": 2.0108010801080107,
      "grad_norm": 0.24179843068122864,
      "learning_rate": 2.962235292776379e-05,
      "loss": 0.1511,
      "step": 11170
    },
    {
      "epoch": 2.012601260126013,
      "grad_norm": 0.23550844192504883,
      "learning_rate": 2.9526756799124133e-05,
      "loss": 0.1532,
      "step": 11180
    },
    {
      "epoch": 2.0144014401440145,
      "grad_norm": 0.23437879979610443,
      "learning_rate": 2.943125050009846e-05,
      "loss": 0.1495,
      "step": 11190
    },
    {
      "epoch": 2.016201620162016,
      "grad_norm": 0.2528984844684601,
      "learning_rate": 2.9335834449735878e-05,
      "loss": 0.1486,
      "step": 11200
    },
    {
      "epoch": 2.018001800180018,
      "grad_norm": 0.22736530005931854,
      "learning_rate": 2.9240509066689503e-05,
      "loss": 0.1503,
      "step": 11210
    },
    {
      "epoch": 2.01980198019802,
      "grad_norm": 0.2293500155210495,
      "learning_rate": 2.9145274769214613e-05,
      "loss": 0.1539,
      "step": 11220
    },
    {
      "epoch": 2.0216021602160215,
      "grad_norm": 0.23788899183273315,
      "learning_rate": 2.905013197516686e-05,
      "loss": 0.1511,
      "step": 11230
    },
    {
      "epoch": 2.0234023402340235,
      "grad_norm": 0.22980210185050964,
      "learning_rate": 2.8955081102000424e-05,
      "loss": 0.1506,
      "step": 11240
    },
    {
      "epoch": 2.025202520252025,
      "grad_norm": 0.2357473075389862,
      "learning_rate": 2.88601225667661e-05,
      "loss": 0.1526,
      "step": 11250
    },
    {
      "epoch": 2.027002700270027,
      "grad_norm": 0.2505234479904175,
      "learning_rate": 2.8765256786109584e-05,
      "loss": 0.1545,
      "step": 11260
    },
    {
      "epoch": 2.028802880288029,
      "grad_norm": 0.242188960313797,
      "learning_rate": 2.8670484176269642e-05,
      "loss": 0.1549,
      "step": 11270
    },
    {
      "epoch": 2.0306030603060305,
      "grad_norm": 0.24025578796863556,
      "learning_rate": 2.857580515307613e-05,
      "loss": 0.1587,
      "step": 11280
    },
    {
      "epoch": 2.032403240324032,
      "grad_norm": 0.24692627787590027,
      "learning_rate": 2.8481220131948362e-05,
      "loss": 0.1474,
      "step": 11290
    },
    {
      "epoch": 2.0342034203420343,
      "grad_norm": 0.2225147932767868,
      "learning_rate": 2.8386729527893196e-05,
      "loss": 0.1498,
      "step": 11300
    },
    {
      "epoch": 2.036003600360036,
      "grad_norm": 0.265508234500885,
      "learning_rate": 2.8292333755503154e-05,
      "loss": 0.15,
      "step": 11310
    },
    {
      "epoch": 2.037803780378038,
      "grad_norm": 0.24395142495632172,
      "learning_rate": 2.819803322895474e-05,
      "loss": 0.1515,
      "step": 11320
    },
    {
      "epoch": 2.0396039603960396,
      "grad_norm": 0.22850988805294037,
      "learning_rate": 2.810382836200653e-05,
      "loss": 0.1465,
      "step": 11330
    },
    {
      "epoch": 2.0414041404140413,
      "grad_norm": 0.23818254470825195,
      "learning_rate": 2.8009719567997373e-05,
      "loss": 0.1507,
      "step": 11340
    },
    {
      "epoch": 2.0432043204320434,
      "grad_norm": 0.26008203625679016,
      "learning_rate": 2.7915707259844602e-05,
      "loss": 0.1547,
      "step": 11350
    },
    {
      "epoch": 2.045004500450045,
      "grad_norm": 0.23854096233844757,
      "learning_rate": 2.782179185004215e-05,
      "loss": 0.1532,
      "step": 11360
    },
    {
      "epoch": 2.0468046804680466,
      "grad_norm": 0.23400427401065826,
      "learning_rate": 2.7727973750658852e-05,
      "loss": 0.1509,
      "step": 11370
    },
    {
      "epoch": 2.0486048604860487,
      "grad_norm": 0.26758620142936707,
      "learning_rate": 2.763425337333655e-05,
      "loss": 0.1527,
      "step": 11380
    },
    {
      "epoch": 2.0504050405040504,
      "grad_norm": 0.22492113709449768,
      "learning_rate": 2.754063112928832e-05,
      "loss": 0.1568,
      "step": 11390
    },
    {
      "epoch": 2.052205220522052,
      "grad_norm": 0.24658875167369843,
      "learning_rate": 2.744710742929667e-05,
      "loss": 0.1509,
      "step": 11400
    },
    {
      "epoch": 2.054005400540054,
      "grad_norm": 0.23840296268463135,
      "learning_rate": 2.7353682683711733e-05,
      "loss": 0.1473,
      "step": 11410
    },
    {
      "epoch": 2.0558055805580557,
      "grad_norm": 0.2345251888036728,
      "learning_rate": 2.7260357302449424e-05,
      "loss": 0.1491,
      "step": 11420
    },
    {
      "epoch": 2.057605760576058,
      "grad_norm": 0.2408932000398636,
      "learning_rate": 2.7167131694989735e-05,
      "loss": 0.1467,
      "step": 11430
    },
    {
      "epoch": 2.0594059405940595,
      "grad_norm": 0.23077572882175446,
      "learning_rate": 2.707400627037484e-05,
      "loss": 0.1497,
      "step": 11440
    },
    {
      "epoch": 2.061206120612061,
      "grad_norm": 0.24379636347293854,
      "learning_rate": 2.698098143720738e-05,
      "loss": 0.1584,
      "step": 11450
    },
    {
      "epoch": 2.063006300630063,
      "grad_norm": 0.24617084860801697,
      "learning_rate": 2.688805760364864e-05,
      "loss": 0.1508,
      "step": 11460
    },
    {
      "epoch": 2.064806480648065,
      "grad_norm": 0.2439338117837906,
      "learning_rate": 2.67952351774167e-05,
      "loss": 0.1484,
      "step": 11470
    },
    {
      "epoch": 2.0666066606660665,
      "grad_norm": 0.23497731983661652,
      "learning_rate": 2.6702514565784736e-05,
      "loss": 0.1513,
      "step": 11480
    },
    {
      "epoch": 2.0684068406840685,
      "grad_norm": 0.2414463311433792,
      "learning_rate": 2.660989617557923e-05,
      "loss": 0.154,
      "step": 11490
    },
    {
      "epoch": 2.07020702070207,
      "grad_norm": 0.26039570569992065,
      "learning_rate": 2.651738041317804e-05,
      "loss": 0.153,
      "step": 11500
    },
    {
      "epoch": 2.07020702070207,
      "eval_loss": 0.18740929663181305,
      "eval_runtime": 316.7479,
      "eval_samples_per_second": 124.705,
      "eval_steps_per_second": 3.899,
      "step": 11500
    },
    {
      "epoch": 2.072007200720072,
      "grad_norm": 0.234796404838562,
      "learning_rate": 2.642496768450887e-05,
      "loss": 0.1539,
      "step": 11510
    },
    {
      "epoch": 2.073807380738074,
      "grad_norm": 0.26146936416625977,
      "learning_rate": 2.6332658395047288e-05,
      "loss": 0.1478,
      "step": 11520
    },
    {
      "epoch": 2.0756075607560756,
      "grad_norm": 0.22781622409820557,
      "learning_rate": 2.6240452949814943e-05,
      "loss": 0.1547,
      "step": 11530
    },
    {
      "epoch": 2.077407740774077,
      "grad_norm": 0.23399363458156586,
      "learning_rate": 2.6148351753377932e-05,
      "loss": 0.1436,
      "step": 11540
    },
    {
      "epoch": 2.0792079207920793,
      "grad_norm": 0.24364106357097626,
      "learning_rate": 2.605635520984494e-05,
      "loss": 0.1502,
      "step": 11550
    },
    {
      "epoch": 2.081008100810081,
      "grad_norm": 0.25402867794036865,
      "learning_rate": 2.5964463722865395e-05,
      "loss": 0.1487,
      "step": 11560
    },
    {
      "epoch": 2.082808280828083,
      "grad_norm": 0.2538979947566986,
      "learning_rate": 2.5872677695627845e-05,
      "loss": 0.1517,
      "step": 11570
    },
    {
      "epoch": 2.0846084608460846,
      "grad_norm": 0.23577472567558289,
      "learning_rate": 2.5780997530858148e-05,
      "loss": 0.1525,
      "step": 11580
    },
    {
      "epoch": 2.0864086408640863,
      "grad_norm": 0.2475878745317459,
      "learning_rate": 2.568942363081756e-05,
      "loss": 0.1465,
      "step": 11590
    },
    {
      "epoch": 2.0882088208820884,
      "grad_norm": 0.24262413382530212,
      "learning_rate": 2.559795639730119e-05,
      "loss": 0.1481,
      "step": 11600
    },
    {
      "epoch": 2.09000900090009,
      "grad_norm": 0.23992377519607544,
      "learning_rate": 2.550659623163605e-05,
      "loss": 0.1489,
      "step": 11610
    },
    {
      "epoch": 2.0918091809180916,
      "grad_norm": 0.234824538230896,
      "learning_rate": 2.5415343534679426e-05,
      "loss": 0.1489,
      "step": 11620
    },
    {
      "epoch": 2.0936093609360937,
      "grad_norm": 0.23557302355766296,
      "learning_rate": 2.5324198706817053e-05,
      "loss": 0.1544,
      "step": 11630
    },
    {
      "epoch": 2.0954095409540954,
      "grad_norm": 0.2557460367679596,
      "learning_rate": 2.5233162147961382e-05,
      "loss": 0.1504,
      "step": 11640
    },
    {
      "epoch": 2.097209720972097,
      "grad_norm": 0.2310321182012558,
      "learning_rate": 2.51422342575498e-05,
      "loss": 0.1495,
      "step": 11650
    },
    {
      "epoch": 2.099009900990099,
      "grad_norm": 0.2676374316215515,
      "learning_rate": 2.5051415434542928e-05,
      "loss": 0.1483,
      "step": 11660
    },
    {
      "epoch": 2.1008100810081007,
      "grad_norm": 0.2387213259935379,
      "learning_rate": 2.4960706077422775e-05,
      "loss": 0.1558,
      "step": 11670
    },
    {
      "epoch": 2.102610261026103,
      "grad_norm": 0.23910881578922272,
      "learning_rate": 2.4870106584191093e-05,
      "loss": 0.1476,
      "step": 11680
    },
    {
      "epoch": 2.1044104410441045,
      "grad_norm": 0.24819831550121307,
      "learning_rate": 2.477961735236759e-05,
      "loss": 0.1523,
      "step": 11690
    },
    {
      "epoch": 2.106210621062106,
      "grad_norm": 0.26132410764694214,
      "learning_rate": 2.4689238778988183e-05,
      "loss": 0.1528,
      "step": 11700
    },
    {
      "epoch": 2.108010801080108,
      "grad_norm": 0.21437250077724457,
      "learning_rate": 2.4598971260603242e-05,
      "loss": 0.155,
      "step": 11710
    },
    {
      "epoch": 2.10981098109811,
      "grad_norm": 0.2427665740251541,
      "learning_rate": 2.4508815193275903e-05,
      "loss": 0.1531,
      "step": 11720
    },
    {
      "epoch": 2.1116111611161115,
      "grad_norm": 0.2635086178779602,
      "learning_rate": 2.4418770972580217e-05,
      "loss": 0.1533,
      "step": 11730
    },
    {
      "epoch": 2.1134113411341136,
      "grad_norm": 0.245396226644516,
      "learning_rate": 2.432883899359956e-05,
      "loss": 0.1512,
      "step": 11740
    },
    {
      "epoch": 2.115211521152115,
      "grad_norm": 0.2259093075990677,
      "learning_rate": 2.423901965092481e-05,
      "loss": 0.1557,
      "step": 11750
    },
    {
      "epoch": 2.117011701170117,
      "grad_norm": 0.24062369763851166,
      "learning_rate": 2.4149313338652634e-05,
      "loss": 0.152,
      "step": 11760
    },
    {
      "epoch": 2.118811881188119,
      "grad_norm": 0.24913033843040466,
      "learning_rate": 2.4059720450383777e-05,
      "loss": 0.1506,
      "step": 11770
    },
    {
      "epoch": 2.1206120612061206,
      "grad_norm": 0.264955073595047,
      "learning_rate": 2.397024137922126e-05,
      "loss": 0.1523,
      "step": 11780
    },
    {
      "epoch": 2.122412241224122,
      "grad_norm": 0.2517143487930298,
      "learning_rate": 2.388087651776877e-05,
      "loss": 0.1524,
      "step": 11790
    },
    {
      "epoch": 2.1242124212421243,
      "grad_norm": 0.27851414680480957,
      "learning_rate": 2.3791626258128885e-05,
      "loss": 0.1567,
      "step": 11800
    },
    {
      "epoch": 2.126012601260126,
      "grad_norm": 0.2220068871974945,
      "learning_rate": 2.370249099190127e-05,
      "loss": 0.1555,
      "step": 11810
    },
    {
      "epoch": 2.127812781278128,
      "grad_norm": 0.24648036062717438,
      "learning_rate": 2.3613471110181162e-05,
      "loss": 0.151,
      "step": 11820
    },
    {
      "epoch": 2.1296129612961296,
      "grad_norm": 0.21943627297878265,
      "learning_rate": 2.3524567003557452e-05,
      "loss": 0.1478,
      "step": 11830
    },
    {
      "epoch": 2.1314131413141313,
      "grad_norm": 0.26270410418510437,
      "learning_rate": 2.3435779062111035e-05,
      "loss": 0.1523,
      "step": 11840
    },
    {
      "epoch": 2.1332133213321334,
      "grad_norm": 0.2519550919532776,
      "learning_rate": 2.334710767541315e-05,
      "loss": 0.1491,
      "step": 11850
    },
    {
      "epoch": 2.135013501350135,
      "grad_norm": 0.21814179420471191,
      "learning_rate": 2.3258553232523645e-05,
      "loss": 0.1518,
      "step": 11860
    },
    {
      "epoch": 2.1368136813681367,
      "grad_norm": 0.23130427300930023,
      "learning_rate": 2.3170116121989184e-05,
      "loss": 0.145,
      "step": 11870
    },
    {
      "epoch": 2.1386138613861387,
      "grad_norm": 0.26288557052612305,
      "learning_rate": 2.3081796731841737e-05,
      "loss": 0.1504,
      "step": 11880
    },
    {
      "epoch": 2.1404140414041404,
      "grad_norm": 0.25149691104888916,
      "learning_rate": 2.2993595449596634e-05,
      "loss": 0.1458,
      "step": 11890
    },
    {
      "epoch": 2.142214221422142,
      "grad_norm": 0.23512385785579681,
      "learning_rate": 2.2905512662251072e-05,
      "loss": 0.1483,
      "step": 11900
    },
    {
      "epoch": 2.144014401440144,
      "grad_norm": 0.23007428646087646,
      "learning_rate": 2.281754875628231e-05,
      "loss": 0.1542,
      "step": 11910
    },
    {
      "epoch": 2.1458145814581457,
      "grad_norm": 0.24939289689064026,
      "learning_rate": 2.2729704117645956e-05,
      "loss": 0.1537,
      "step": 11920
    },
    {
      "epoch": 2.147614761476148,
      "grad_norm": 0.27677273750305176,
      "learning_rate": 2.2641979131774373e-05,
      "loss": 0.153,
      "step": 11930
    },
    {
      "epoch": 2.1494149414941495,
      "grad_norm": 0.2536546289920807,
      "learning_rate": 2.255437418357489e-05,
      "loss": 0.153,
      "step": 11940
    },
    {
      "epoch": 2.151215121512151,
      "grad_norm": 0.24029672145843506,
      "learning_rate": 2.2466889657428176e-05,
      "loss": 0.1501,
      "step": 11950
    },
    {
      "epoch": 2.153015301530153,
      "grad_norm": 0.233381450176239,
      "learning_rate": 2.2379525937186513e-05,
      "loss": 0.1507,
      "step": 11960
    },
    {
      "epoch": 2.154815481548155,
      "grad_norm": 0.2100486159324646,
      "learning_rate": 2.229228340617215e-05,
      "loss": 0.1475,
      "step": 11970
    },
    {
      "epoch": 2.1566156615661565,
      "grad_norm": 0.2478436380624771,
      "learning_rate": 2.2205162447175542e-05,
      "loss": 0.1561,
      "step": 11980
    },
    {
      "epoch": 2.1584158415841586,
      "grad_norm": 0.2416856437921524,
      "learning_rate": 2.2118163442453792e-05,
      "loss": 0.1461,
      "step": 11990
    },
    {
      "epoch": 2.16021602160216,
      "grad_norm": 0.27154672145843506,
      "learning_rate": 2.2031286773728875e-05,
      "loss": 0.1571,
      "step": 12000
    },
    {
      "epoch": 2.16021602160216,
      "eval_loss": 0.18735581636428833,
      "eval_runtime": 316.6412,
      "eval_samples_per_second": 124.747,
      "eval_steps_per_second": 3.9,
      "step": 12000
    },
    {
      "epoch": 2.162016201620162,
      "grad_norm": 0.22598417103290558,
      "learning_rate": 2.194453282218602e-05,
      "loss": 0.15,
      "step": 12010
    },
    {
      "epoch": 2.163816381638164,
      "grad_norm": 0.24042212963104248,
      "learning_rate": 2.1857901968472006e-05,
      "loss": 0.1511,
      "step": 12020
    },
    {
      "epoch": 2.1656165616561656,
      "grad_norm": 0.24122324585914612,
      "learning_rate": 2.1771394592693463e-05,
      "loss": 0.1516,
      "step": 12030
    },
    {
      "epoch": 2.1674167416741676,
      "grad_norm": 0.2506106495857239,
      "learning_rate": 2.1685011074415297e-05,
      "loss": 0.1457,
      "step": 12040
    },
    {
      "epoch": 2.1692169216921693,
      "grad_norm": 0.23372212052345276,
      "learning_rate": 2.1598751792658944e-05,
      "loss": 0.1556,
      "step": 12050
    },
    {
      "epoch": 2.171017101710171,
      "grad_norm": 0.2763853967189789,
      "learning_rate": 2.1512617125900736e-05,
      "loss": 0.1559,
      "step": 12060
    },
    {
      "epoch": 2.172817281728173,
      "grad_norm": 0.22115857899188995,
      "learning_rate": 2.1426607452070234e-05,
      "loss": 0.154,
      "step": 12070
    },
    {
      "epoch": 2.1746174617461747,
      "grad_norm": 0.2516426742076874,
      "learning_rate": 2.1340723148548596e-05,
      "loss": 0.1512,
      "step": 12080
    },
    {
      "epoch": 2.1764176417641763,
      "grad_norm": 0.24318169057369232,
      "learning_rate": 2.1254964592166843e-05,
      "loss": 0.1525,
      "step": 12090
    },
    {
      "epoch": 2.1782178217821784,
      "grad_norm": 0.23572991788387299,
      "learning_rate": 2.116933215920431e-05,
      "loss": 0.1531,
      "step": 12100
    },
    {
      "epoch": 2.18001800180018,
      "grad_norm": 0.26744717359542847,
      "learning_rate": 2.1083826225386927e-05,
      "loss": 0.1559,
      "step": 12110
    },
    {
      "epoch": 2.1818181818181817,
      "grad_norm": 0.23791776597499847,
      "learning_rate": 2.09984471658856e-05,
      "loss": 0.1474,
      "step": 12120
    },
    {
      "epoch": 2.1836183618361837,
      "grad_norm": 0.2616663873195648,
      "learning_rate": 2.091319535531453e-05,
      "loss": 0.1509,
      "step": 12130
    },
    {
      "epoch": 2.1854185418541854,
      "grad_norm": 0.23644624650478363,
      "learning_rate": 2.0828071167729646e-05,
      "loss": 0.148,
      "step": 12140
    },
    {
      "epoch": 2.187218721872187,
      "grad_norm": 0.2888582646846771,
      "learning_rate": 2.074307497662682e-05,
      "loss": 0.1486,
      "step": 12150
    },
    {
      "epoch": 2.189018901890189,
      "grad_norm": 0.2453395277261734,
      "learning_rate": 2.0658207154940423e-05,
      "loss": 0.1544,
      "step": 12160
    },
    {
      "epoch": 2.1908190819081907,
      "grad_norm": 0.20162689685821533,
      "learning_rate": 2.0573468075041486e-05,
      "loss": 0.1469,
      "step": 12170
    },
    {
      "epoch": 2.1926192619261924,
      "grad_norm": 0.25583183765411377,
      "learning_rate": 2.048885810873622e-05,
      "loss": 0.1512,
      "step": 12180
    },
    {
      "epoch": 2.1944194419441945,
      "grad_norm": 0.2501237392425537,
      "learning_rate": 2.0404377627264372e-05,
      "loss": 0.1525,
      "step": 12190
    },
    {
      "epoch": 2.196219621962196,
      "grad_norm": 0.23560240864753723,
      "learning_rate": 2.0320027001297454e-05,
      "loss": 0.1453,
      "step": 12200
    },
    {
      "epoch": 2.198019801980198,
      "grad_norm": 0.24243256449699402,
      "learning_rate": 2.023580660093728e-05,
      "loss": 0.1475,
      "step": 12210
    },
    {
      "epoch": 2.1998199819982,
      "grad_norm": 0.24039115011692047,
      "learning_rate": 2.0151716795714287e-05,
      "loss": 0.1554,
      "step": 12220
    },
    {
      "epoch": 2.2016201620162015,
      "grad_norm": 0.23708663880825043,
      "learning_rate": 2.0067757954585847e-05,
      "loss": 0.1523,
      "step": 12230
    },
    {
      "epoch": 2.2034203420342036,
      "grad_norm": 0.24108316004276276,
      "learning_rate": 1.998393044593476e-05,
      "loss": 0.1546,
      "step": 12240
    },
    {
      "epoch": 2.205220522052205,
      "grad_norm": 0.24792498350143433,
      "learning_rate": 1.9900234637567567e-05,
      "loss": 0.1487,
      "step": 12250
    },
    {
      "epoch": 2.207020702070207,
      "grad_norm": 0.23627278208732605,
      "learning_rate": 1.9816670896712948e-05,
      "loss": 0.1462,
      "step": 12260
    },
    {
      "epoch": 2.208820882088209,
      "grad_norm": 0.25154954195022583,
      "learning_rate": 1.9733239590020125e-05,
      "loss": 0.1535,
      "step": 12270
    },
    {
      "epoch": 2.2106210621062106,
      "grad_norm": 0.2281118631362915,
      "learning_rate": 1.964994108355725e-05,
      "loss": 0.1487,
      "step": 12280
    },
    {
      "epoch": 2.212421242124212,
      "grad_norm": 0.2504616677761078,
      "learning_rate": 1.956677574280975e-05,
      "loss": 0.1513,
      "step": 12290
    },
    {
      "epoch": 2.2142214221422143,
      "grad_norm": 0.2554556131362915,
      "learning_rate": 1.9483743932678816e-05,
      "loss": 0.153,
      "step": 12300
    },
    {
      "epoch": 2.216021602160216,
      "grad_norm": 0.23741383850574493,
      "learning_rate": 1.9400846017479736e-05,
      "loss": 0.1534,
      "step": 12310
    },
    {
      "epoch": 2.217821782178218,
      "grad_norm": 0.2858852446079254,
      "learning_rate": 1.9318082360940303e-05,
      "loss": 0.1499,
      "step": 12320
    },
    {
      "epoch": 2.2196219621962197,
      "grad_norm": 0.24434708058834076,
      "learning_rate": 1.923545332619926e-05,
      "loss": 0.1549,
      "step": 12330
    },
    {
      "epoch": 2.2214221422142213,
      "grad_norm": 0.2575809359550476,
      "learning_rate": 1.9152959275804623e-05,
      "loss": 0.1526,
      "step": 12340
    },
    {
      "epoch": 2.2232223222322234,
      "grad_norm": 0.24530677497386932,
      "learning_rate": 1.9070600571712176e-05,
      "loss": 0.1516,
      "step": 12350
    },
    {
      "epoch": 2.225022502250225,
      "grad_norm": 0.2325793206691742,
      "learning_rate": 1.898837757528385e-05,
      "loss": 0.1462,
      "step": 12360
    },
    {
      "epoch": 2.2268226822682267,
      "grad_norm": 0.2487598955631256,
      "learning_rate": 1.890629064728614e-05,
      "loss": 0.1504,
      "step": 12370
    },
    {
      "epoch": 2.2286228622862287,
      "grad_norm": 0.2658658027648926,
      "learning_rate": 1.88243401478885e-05,
      "loss": 0.1542,
      "step": 12380
    },
    {
      "epoch": 2.2304230423042304,
      "grad_norm": 0.24291664361953735,
      "learning_rate": 1.874252643666182e-05,
      "loss": 0.1483,
      "step": 12390
    },
    {
      "epoch": 2.232223222322232,
      "grad_norm": 0.2539977729320526,
      "learning_rate": 1.866084987257674e-05,
      "loss": 0.1543,
      "step": 12400
    },
    {
      "epoch": 2.234023402340234,
      "grad_norm": 0.27160853147506714,
      "learning_rate": 1.857931081400221e-05,
      "loss": 0.1497,
      "step": 12410
    },
    {
      "epoch": 2.2358235823582358,
      "grad_norm": 0.25924521684646606,
      "learning_rate": 1.8497909618703828e-05,
      "loss": 0.1499,
      "step": 12420
    },
    {
      "epoch": 2.237623762376238,
      "grad_norm": 0.2509993016719818,
      "learning_rate": 1.8416646643842305e-05,
      "loss": 0.1506,
      "step": 12430
    },
    {
      "epoch": 2.2394239423942395,
      "grad_norm": 0.2391819804906845,
      "learning_rate": 1.833552224597189e-05,
      "loss": 0.1464,
      "step": 12440
    },
    {
      "epoch": 2.241224122412241,
      "grad_norm": 0.23367911577224731,
      "learning_rate": 1.825453678103877e-05,
      "loss": 0.1479,
      "step": 12450
    },
    {
      "epoch": 2.243024302430243,
      "grad_norm": 0.2332279235124588,
      "learning_rate": 1.8173690604379568e-05,
      "loss": 0.151,
      "step": 12460
    },
    {
      "epoch": 2.244824482448245,
      "grad_norm": 0.2540323734283447,
      "learning_rate": 1.809298407071978e-05,
      "loss": 0.1522,
      "step": 12470
    },
    {
      "epoch": 2.2466246624662465,
      "grad_norm": 0.2716794013977051,
      "learning_rate": 1.8012417534172143e-05,
      "loss": 0.1526,
      "step": 12480
    },
    {
      "epoch": 2.2484248424842486,
      "grad_norm": 0.2647496163845062,
      "learning_rate": 1.7931991348235156e-05,
      "loss": 0.1521,
      "step": 12490
    },
    {
      "epoch": 2.25022502250225,
      "grad_norm": 0.2644495666027069,
      "learning_rate": 1.7851705865791563e-05,
      "loss": 0.1524,
      "step": 12500
    },
    {
      "epoch": 2.25022502250225,
      "eval_loss": 0.18625158071517944,
      "eval_runtime": 316.8157,
      "eval_samples_per_second": 124.678,
      "eval_steps_per_second": 3.898,
      "step": 12500
    },
    {
      "epoch": 2.252025202520252,
      "grad_norm": 0.24630877375602722,
      "learning_rate": 1.7771561439106655e-05,
      "loss": 0.1545,
      "step": 12510
    },
    {
      "epoch": 2.253825382538254,
      "grad_norm": 0.2582838535308838,
      "learning_rate": 1.7691558419826876e-05,
      "loss": 0.1556,
      "step": 12520
    },
    {
      "epoch": 2.2556255625562556,
      "grad_norm": 0.2852724492549896,
      "learning_rate": 1.761169715897823e-05,
      "loss": 0.1497,
      "step": 12530
    },
    {
      "epoch": 2.2574257425742577,
      "grad_norm": 0.23688030242919922,
      "learning_rate": 1.7531978006964678e-05,
      "loss": 0.155,
      "step": 12540
    },
    {
      "epoch": 2.2592259225922593,
      "grad_norm": 0.24841077625751495,
      "learning_rate": 1.745240131356669e-05,
      "loss": 0.1515,
      "step": 12550
    },
    {
      "epoch": 2.261026102610261,
      "grad_norm": 0.27750223875045776,
      "learning_rate": 1.7372967427939735e-05,
      "loss": 0.1481,
      "step": 12560
    },
    {
      "epoch": 2.262826282628263,
      "grad_norm": 0.26373419165611267,
      "learning_rate": 1.729367669861257e-05,
      "loss": 0.1531,
      "step": 12570
    },
    {
      "epoch": 2.2646264626462647,
      "grad_norm": 0.23898537456989288,
      "learning_rate": 1.7214529473485934e-05,
      "loss": 0.1522,
      "step": 12580
    },
    {
      "epoch": 2.2664266426642663,
      "grad_norm": 0.2273661196231842,
      "learning_rate": 1.7135526099830853e-05,
      "loss": 0.1456,
      "step": 12590
    },
    {
      "epoch": 2.2682268226822684,
      "grad_norm": 0.2495691478252411,
      "learning_rate": 1.7056666924287206e-05,
      "loss": 0.1501,
      "step": 12600
    },
    {
      "epoch": 2.27002700270027,
      "grad_norm": 0.24019499123096466,
      "learning_rate": 1.6977952292862188e-05,
      "loss": 0.1475,
      "step": 12610
    },
    {
      "epoch": 2.2718271827182717,
      "grad_norm": 0.23236270248889923,
      "learning_rate": 1.6899382550928783e-05,
      "loss": 0.1498,
      "step": 12620
    },
    {
      "epoch": 2.2736273627362737,
      "grad_norm": 0.2597145438194275,
      "learning_rate": 1.6820958043224232e-05,
      "loss": 0.1512,
      "step": 12630
    },
    {
      "epoch": 2.2754275427542754,
      "grad_norm": 0.256944864988327,
      "learning_rate": 1.674267911384857e-05,
      "loss": 0.151,
      "step": 12640
    },
    {
      "epoch": 2.2772277227722775,
      "grad_norm": 0.226159006357193,
      "learning_rate": 1.666454610626303e-05,
      "loss": 0.1483,
      "step": 12650
    },
    {
      "epoch": 2.279027902790279,
      "grad_norm": 0.2533186078071594,
      "learning_rate": 1.6586559363288636e-05,
      "loss": 0.1488,
      "step": 12660
    },
    {
      "epoch": 2.2808280828082808,
      "grad_norm": 0.2487465888261795,
      "learning_rate": 1.6508719227104636e-05,
      "loss": 0.1469,
      "step": 12670
    },
    {
      "epoch": 2.2826282628262824,
      "grad_norm": 0.24164119362831116,
      "learning_rate": 1.6431026039247028e-05,
      "loss": 0.1482,
      "step": 12680
    },
    {
      "epoch": 2.2844284428442845,
      "grad_norm": 0.29778188467025757,
      "learning_rate": 1.6353480140607036e-05,
      "loss": 0.1559,
      "step": 12690
    },
    {
      "epoch": 2.286228622862286,
      "grad_norm": 0.2506515383720398,
      "learning_rate": 1.6276081871429654e-05,
      "loss": 0.153,
      "step": 12700
    },
    {
      "epoch": 2.288028802880288,
      "grad_norm": 0.24957583844661713,
      "learning_rate": 1.6198831571312084e-05,
      "loss": 0.1486,
      "step": 12710
    },
    {
      "epoch": 2.28982898289829,
      "grad_norm": 0.22023645043373108,
      "learning_rate": 1.6121729579202337e-05,
      "loss": 0.1525,
      "step": 12720
    },
    {
      "epoch": 2.2916291629162915,
      "grad_norm": 0.22594207525253296,
      "learning_rate": 1.6044776233397642e-05,
      "loss": 0.1451,
      "step": 12730
    },
    {
      "epoch": 2.2934293429342936,
      "grad_norm": 0.23617391288280487,
      "learning_rate": 1.596797187154309e-05,
      "loss": 0.1511,
      "step": 12740
    },
    {
      "epoch": 2.295229522952295,
      "grad_norm": 0.2512252628803253,
      "learning_rate": 1.589131683063006e-05,
      "loss": 0.1532,
      "step": 12750
    },
    {
      "epoch": 2.297029702970297,
      "grad_norm": 0.22489163279533386,
      "learning_rate": 1.581481144699471e-05,
      "loss": 0.147,
      "step": 12760
    },
    {
      "epoch": 2.298829882988299,
      "grad_norm": 0.26068440079689026,
      "learning_rate": 1.573845605631661e-05,
      "loss": 0.1532,
      "step": 12770
    },
    {
      "epoch": 2.3006300630063006,
      "grad_norm": 0.2625911831855774,
      "learning_rate": 1.56622509936172e-05,
      "loss": 0.1474,
      "step": 12780
    },
    {
      "epoch": 2.302430243024302,
      "grad_norm": 0.24966157972812653,
      "learning_rate": 1.5586196593258294e-05,
      "loss": 0.1488,
      "step": 12790
    },
    {
      "epoch": 2.3042304230423043,
      "grad_norm": 0.24154245853424072,
      "learning_rate": 1.5510293188940694e-05,
      "loss": 0.1469,
      "step": 12800
    },
    {
      "epoch": 2.306030603060306,
      "grad_norm": 0.23814953863620758,
      "learning_rate": 1.543454111370271e-05,
      "loss": 0.1508,
      "step": 12810
    },
    {
      "epoch": 2.307830783078308,
      "grad_norm": 0.25053203105926514,
      "learning_rate": 1.5358940699918583e-05,
      "loss": 0.1493,
      "step": 12820
    },
    {
      "epoch": 2.3096309630963097,
      "grad_norm": 0.268791139125824,
      "learning_rate": 1.5283492279297183e-05,
      "loss": 0.1546,
      "step": 12830
    },
    {
      "epoch": 2.3114311431143113,
      "grad_norm": 0.26200172305107117,
      "learning_rate": 1.5208196182880485e-05,
      "loss": 0.1525,
      "step": 12840
    },
    {
      "epoch": 2.3132313231323134,
      "grad_norm": 0.24245861172676086,
      "learning_rate": 1.5133052741042064e-05,
      "loss": 0.1524,
      "step": 12850
    },
    {
      "epoch": 2.315031503150315,
      "grad_norm": 0.24470154941082,
      "learning_rate": 1.5058062283485753e-05,
      "loss": 0.1508,
      "step": 12860
    },
    {
      "epoch": 2.3168316831683167,
      "grad_norm": 0.23464371263980865,
      "learning_rate": 1.4983225139244116e-05,
      "loss": 0.1466,
      "step": 12870
    },
    {
      "epoch": 2.3186318631863188,
      "grad_norm": 0.24408625066280365,
      "learning_rate": 1.4908541636677053e-05,
      "loss": 0.1527,
      "step": 12880
    },
    {
      "epoch": 2.3204320432043204,
      "grad_norm": 0.24861980974674225,
      "learning_rate": 1.4834012103470325e-05,
      "loss": 0.1479,
      "step": 12890
    },
    {
      "epoch": 2.322232223222322,
      "grad_norm": 0.24295742809772491,
      "learning_rate": 1.4759636866634096e-05,
      "loss": 0.1531,
      "step": 12900
    },
    {
      "epoch": 2.324032403240324,
      "grad_norm": 0.22370684146881104,
      "learning_rate": 1.468541625250157e-05,
      "loss": 0.1509,
      "step": 12910
    },
    {
      "epoch": 2.3258325832583258,
      "grad_norm": 0.25745144486427307,
      "learning_rate": 1.4611350586727524e-05,
      "loss": 0.1545,
      "step": 12920
    },
    {
      "epoch": 2.327632763276328,
      "grad_norm": 0.23375846445560455,
      "learning_rate": 1.4537440194286839e-05,
      "loss": 0.1485,
      "step": 12930
    },
    {
      "epoch": 2.3294329432943295,
      "grad_norm": 0.2638228237628937,
      "learning_rate": 1.4463685399473138e-05,
      "loss": 0.1444,
      "step": 12940
    },
    {
      "epoch": 2.331233123312331,
      "grad_norm": 0.2419271618127823,
      "learning_rate": 1.4390086525897339e-05,
      "loss": 0.1428,
      "step": 12950
    },
    {
      "epoch": 2.333033303330333,
      "grad_norm": 0.24593929946422577,
      "learning_rate": 1.4316643896486187e-05,
      "loss": 0.1559,
      "step": 12960
    },
    {
      "epoch": 2.334833483348335,
      "grad_norm": 0.20968760550022125,
      "learning_rate": 1.4243357833480926e-05,
      "loss": 0.1417,
      "step": 12970
    },
    {
      "epoch": 2.3366336633663365,
      "grad_norm": 0.24616333842277527,
      "learning_rate": 1.4170228658435825e-05,
      "loss": 0.1519,
      "step": 12980
    },
    {
      "epoch": 2.3384338433843386,
      "grad_norm": 0.2878527045249939,
      "learning_rate": 1.4097256692216781e-05,
      "loss": 0.1504,
      "step": 12990
    },
    {
      "epoch": 2.34023402340234,
      "grad_norm": 0.2552768290042877,
      "learning_rate": 1.4024442254999932e-05,
      "loss": 0.1532,
      "step": 13000
    },
    {
      "epoch": 2.34023402340234,
      "eval_loss": 0.18603602051734924,
      "eval_runtime": 316.9307,
      "eval_samples_per_second": 124.633,
      "eval_steps_per_second": 3.897,
      "step": 13000
    },
    {
      "epoch": 2.342034203420342,
      "grad_norm": 0.2045951932668686,
      "learning_rate": 1.3951785666270178e-05,
      "loss": 0.1478,
      "step": 13010
    },
    {
      "epoch": 2.343834383438344,
      "grad_norm": 0.2600531578063965,
      "learning_rate": 1.3879287244819888e-05,
      "loss": 0.1524,
      "step": 13020
    },
    {
      "epoch": 2.3456345634563456,
      "grad_norm": 0.2529865503311157,
      "learning_rate": 1.3806947308747448e-05,
      "loss": 0.1505,
      "step": 13030
    },
    {
      "epoch": 2.3474347434743477,
      "grad_norm": 0.20862069725990295,
      "learning_rate": 1.3734766175455788e-05,
      "loss": 0.1477,
      "step": 13040
    },
    {
      "epoch": 2.3492349234923493,
      "grad_norm": 0.2537483870983124,
      "learning_rate": 1.3662744161651175e-05,
      "loss": 0.1499,
      "step": 13050
    },
    {
      "epoch": 2.351035103510351,
      "grad_norm": 0.23286651074886322,
      "learning_rate": 1.3590881583341659e-05,
      "loss": 0.1501,
      "step": 13060
    },
    {
      "epoch": 2.352835283528353,
      "grad_norm": 0.23788097500801086,
      "learning_rate": 1.3519178755835716e-05,
      "loss": 0.1483,
      "step": 13070
    },
    {
      "epoch": 2.3546354635463547,
      "grad_norm": 0.25392574071884155,
      "learning_rate": 1.3447635993740926e-05,
      "loss": 0.1507,
      "step": 13080
    },
    {
      "epoch": 2.3564356435643563,
      "grad_norm": 0.25492507219314575,
      "learning_rate": 1.3376253610962568e-05,
      "loss": 0.1496,
      "step": 13090
    },
    {
      "epoch": 2.3582358235823584,
      "grad_norm": 0.24727340042591095,
      "learning_rate": 1.3305031920702166e-05,
      "loss": 0.1538,
      "step": 13100
    },
    {
      "epoch": 2.36003600360036,
      "grad_norm": 0.24883899092674255,
      "learning_rate": 1.3233971235456267e-05,
      "loss": 0.1497,
      "step": 13110
    },
    {
      "epoch": 2.3618361836183617,
      "grad_norm": 0.23327948153018951,
      "learning_rate": 1.3163071867014942e-05,
      "loss": 0.1566,
      "step": 13120
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 0.24529972672462463,
      "learning_rate": 1.3092334126460426e-05,
      "loss": 0.1544,
      "step": 13130
    },
    {
      "epoch": 2.3654365436543654,
      "grad_norm": 0.24224649369716644,
      "learning_rate": 1.302175832416585e-05,
      "loss": 0.1478,
      "step": 13140
    },
    {
      "epoch": 2.3672367236723675,
      "grad_norm": 0.24742375314235687,
      "learning_rate": 1.2951344769793755e-05,
      "loss": 0.148,
      "step": 13150
    },
    {
      "epoch": 2.369036903690369,
      "grad_norm": 0.25495535135269165,
      "learning_rate": 1.2881093772294828e-05,
      "loss": 0.1535,
      "step": 13160
    },
    {
      "epoch": 2.3708370837083708,
      "grad_norm": 0.26157045364379883,
      "learning_rate": 1.2811005639906509e-05,
      "loss": 0.1502,
      "step": 13170
    },
    {
      "epoch": 2.3726372637263724,
      "grad_norm": 0.23220455646514893,
      "learning_rate": 1.274108068015164e-05,
      "loss": 0.1496,
      "step": 13180
    },
    {
      "epoch": 2.3744374437443745,
      "grad_norm": 0.22640873491764069,
      "learning_rate": 1.2671319199837112e-05,
      "loss": 0.1487,
      "step": 13190
    },
    {
      "epoch": 2.376237623762376,
      "grad_norm": 0.24113726615905762,
      "learning_rate": 1.2601721505052538e-05,
      "loss": 0.1483,
      "step": 13200
    },
    {
      "epoch": 2.378037803780378,
      "grad_norm": 0.25200581550598145,
      "learning_rate": 1.2532287901168866e-05,
      "loss": 0.1443,
      "step": 13210
    },
    {
      "epoch": 2.37983798379838,
      "grad_norm": 0.29384201765060425,
      "learning_rate": 1.2463018692837086e-05,
      "loss": 0.1555,
      "step": 13220
    },
    {
      "epoch": 2.3816381638163815,
      "grad_norm": 0.2608053982257843,
      "learning_rate": 1.2393914183986888e-05,
      "loss": 0.153,
      "step": 13230
    },
    {
      "epoch": 2.3834383438343836,
      "grad_norm": 0.25356578826904297,
      "learning_rate": 1.2324974677825302e-05,
      "loss": 0.1498,
      "step": 13240
    },
    {
      "epoch": 2.385238523852385,
      "grad_norm": 0.23115688562393188,
      "learning_rate": 1.2256200476835395e-05,
      "loss": 0.146,
      "step": 13250
    },
    {
      "epoch": 2.387038703870387,
      "grad_norm": 0.23727652430534363,
      "learning_rate": 1.2187591882774924e-05,
      "loss": 0.1439,
      "step": 13260
    },
    {
      "epoch": 2.388838883888389,
      "grad_norm": 0.27953794598579407,
      "learning_rate": 1.2119149196675e-05,
      "loss": 0.1527,
      "step": 13270
    },
    {
      "epoch": 2.3906390639063906,
      "grad_norm": 0.26897141337394714,
      "learning_rate": 1.2050872718838812e-05,
      "loss": 0.1507,
      "step": 13280
    },
    {
      "epoch": 2.3924392439243922,
      "grad_norm": 0.25288793444633484,
      "learning_rate": 1.1982762748840288e-05,
      "loss": 0.1531,
      "step": 13290
    },
    {
      "epoch": 2.3942394239423943,
      "grad_norm": 0.2417706698179245,
      "learning_rate": 1.1914819585522752e-05,
      "loss": 0.1487,
      "step": 13300
    },
    {
      "epoch": 2.396039603960396,
      "grad_norm": 0.26945793628692627,
      "learning_rate": 1.184704352699767e-05,
      "loss": 0.1482,
      "step": 13310
    },
    {
      "epoch": 2.397839783978398,
      "grad_norm": 0.27991485595703125,
      "learning_rate": 1.1779434870643258e-05,
      "loss": 0.1504,
      "step": 13320
    },
    {
      "epoch": 2.3996399639963997,
      "grad_norm": 0.24716030061244965,
      "learning_rate": 1.171199391310328e-05,
      "loss": 0.1487,
      "step": 13330
    },
    {
      "epoch": 2.4014401440144013,
      "grad_norm": 0.2601779103279114,
      "learning_rate": 1.164472095028567e-05,
      "loss": 0.1524,
      "step": 13340
    },
    {
      "epoch": 2.4032403240324034,
      "grad_norm": 0.25586217641830444,
      "learning_rate": 1.1577616277361258e-05,
      "loss": 0.1508,
      "step": 13350
    },
    {
      "epoch": 2.405040504050405,
      "grad_norm": 0.34914344549179077,
      "learning_rate": 1.1510680188762485e-05,
      "loss": 0.1542,
      "step": 13360
    },
    {
      "epoch": 2.4068406840684067,
      "grad_norm": 0.241573303937912,
      "learning_rate": 1.1443912978182115e-05,
      "loss": 0.1514,
      "step": 13370
    },
    {
      "epoch": 2.4086408640864088,
      "grad_norm": 0.26396405696868896,
      "learning_rate": 1.1377314938571876e-05,
      "loss": 0.1523,
      "step": 13380
    },
    {
      "epoch": 2.4104410441044104,
      "grad_norm": 0.2608339190483093,
      "learning_rate": 1.1310886362141304e-05,
      "loss": 0.1517,
      "step": 13390
    },
    {
      "epoch": 2.412241224122412,
      "grad_norm": 0.2570274770259857,
      "learning_rate": 1.1244627540356323e-05,
      "loss": 0.1446,
      "step": 13400
    },
    {
      "epoch": 2.414041404140414,
      "grad_norm": 0.23875649273395538,
      "learning_rate": 1.1178538763938062e-05,
      "loss": 0.1428,
      "step": 13410
    },
    {
      "epoch": 2.4158415841584158,
      "grad_norm": 0.23568397760391235,
      "learning_rate": 1.1112620322861594e-05,
      "loss": 0.1507,
      "step": 13420
    },
    {
      "epoch": 2.417641764176418,
      "grad_norm": 0.2528389096260071,
      "learning_rate": 1.1046872506354533e-05,
      "loss": 0.1497,
      "step": 13430
    },
    {
      "epoch": 2.4194419441944195,
      "grad_norm": 0.23932762444019318,
      "learning_rate": 1.0981295602895903e-05,
      "loss": 0.155,
      "step": 13440
    },
    {
      "epoch": 2.421242124212421,
      "grad_norm": 0.24552787840366364,
      "learning_rate": 1.091588990021482e-05,
      "loss": 0.1463,
      "step": 13450
    },
    {
      "epoch": 2.423042304230423,
      "grad_norm": 0.2551756799221039,
      "learning_rate": 1.085065568528919e-05,
      "loss": 0.1512,
      "step": 13460
    },
    {
      "epoch": 2.424842484248425,
      "grad_norm": 0.2557283341884613,
      "learning_rate": 1.0785593244344538e-05,
      "loss": 0.1518,
      "step": 13470
    },
    {
      "epoch": 2.4266426642664265,
      "grad_norm": 0.2557223439216614,
      "learning_rate": 1.0720702862852673e-05,
      "loss": 0.1521,
      "step": 13480
    },
    {
      "epoch": 2.4284428442844286,
      "grad_norm": 0.23480474948883057,
      "learning_rate": 1.0655984825530474e-05,
      "loss": 0.1458,
      "step": 13490
    },
    {
      "epoch": 2.43024302430243,
      "grad_norm": 0.25644245743751526,
      "learning_rate": 1.0591439416338638e-05,
      "loss": 0.1524,
      "step": 13500
    },
    {
      "epoch": 2.43024302430243,
      "eval_loss": 0.18542732298374176,
      "eval_runtime": 316.9619,
      "eval_samples_per_second": 124.621,
      "eval_steps_per_second": 3.896,
      "step": 13500
    },
    {
      "epoch": 2.432043204320432,
      "grad_norm": 0.2616402804851532,
      "learning_rate": 1.0527066918480444e-05,
      "loss": 0.151,
      "step": 13510
    },
    {
      "epoch": 2.433843384338434,
      "grad_norm": 0.26836949586868286,
      "learning_rate": 1.0462867614400435e-05,
      "loss": 0.1552,
      "step": 13520
    },
    {
      "epoch": 2.4356435643564356,
      "grad_norm": 0.2463960349559784,
      "learning_rate": 1.0398841785783292e-05,
      "loss": 0.1484,
      "step": 13530
    },
    {
      "epoch": 2.4374437443744377,
      "grad_norm": 0.2531854212284088,
      "learning_rate": 1.0334989713552545e-05,
      "loss": 0.1451,
      "step": 13540
    },
    {
      "epoch": 2.4392439243924393,
      "grad_norm": 0.2379830777645111,
      "learning_rate": 1.0271311677869317e-05,
      "loss": 0.1504,
      "step": 13550
    },
    {
      "epoch": 2.441044104410441,
      "grad_norm": 0.2485802173614502,
      "learning_rate": 1.0207807958131139e-05,
      "loss": 0.1491,
      "step": 13560
    },
    {
      "epoch": 2.442844284428443,
      "grad_norm": 0.23781763017177582,
      "learning_rate": 1.0144478832970666e-05,
      "loss": 0.1447,
      "step": 13570
    },
    {
      "epoch": 2.4446444644464447,
      "grad_norm": 0.22030137479305267,
      "learning_rate": 1.0081324580254537e-05,
      "loss": 0.1471,
      "step": 13580
    },
    {
      "epoch": 2.4464446444644463,
      "grad_norm": 0.2484455555677414,
      "learning_rate": 1.001834547708208e-05,
      "loss": 0.1447,
      "step": 13590
    },
    {
      "epoch": 2.4482448244824484,
      "grad_norm": 0.3009362518787384,
      "learning_rate": 9.955541799784145e-06,
      "loss": 0.1509,
      "step": 13600
    },
    {
      "epoch": 2.45004500450045,
      "grad_norm": 0.23621229827404022,
      "learning_rate": 9.892913823921862e-06,
      "loss": 0.1557,
      "step": 13610
    },
    {
      "epoch": 2.4518451845184517,
      "grad_norm": 0.2410489022731781,
      "learning_rate": 9.83046182428546e-06,
      "loss": 0.1519,
      "step": 13620
    },
    {
      "epoch": 2.4536453645364538,
      "grad_norm": 0.22500084340572357,
      "learning_rate": 9.768186074892999e-06,
      "loss": 0.1491,
      "step": 13630
    },
    {
      "epoch": 2.4554455445544554,
      "grad_norm": 0.2408672571182251,
      "learning_rate": 9.70608684898926e-06,
      "loss": 0.1524,
      "step": 13640
    },
    {
      "epoch": 2.4572457245724575,
      "grad_norm": 0.2517479956150055,
      "learning_rate": 9.644164419044488e-06,
      "loss": 0.1529,
      "step": 13650
    },
    {
      "epoch": 2.459045904590459,
      "grad_norm": 0.2682531476020813,
      "learning_rate": 9.582419056753195e-06,
      "loss": 0.1521,
      "step": 13660
    },
    {
      "epoch": 2.4608460846084608,
      "grad_norm": 0.2610343098640442,
      "learning_rate": 9.520851033033013e-06,
      "loss": 0.1477,
      "step": 13670
    },
    {
      "epoch": 2.4626462646264624,
      "grad_norm": 0.2480553388595581,
      "learning_rate": 9.459460618023408e-06,
      "loss": 0.1474,
      "step": 13680
    },
    {
      "epoch": 2.4644464446444645,
      "grad_norm": 0.24392415583133698,
      "learning_rate": 9.398248081084621e-06,
      "loss": 0.1472,
      "step": 13690
    },
    {
      "epoch": 2.466246624662466,
      "grad_norm": 0.26940327882766724,
      "learning_rate": 9.337213690796409e-06,
      "loss": 0.1489,
      "step": 13700
    },
    {
      "epoch": 2.468046804680468,
      "grad_norm": 0.23137642443180084,
      "learning_rate": 9.276357714956862e-06,
      "loss": 0.1489,
      "step": 13710
    },
    {
      "epoch": 2.46984698469847,
      "grad_norm": 0.2962612509727478,
      "learning_rate": 9.215680420581251e-06,
      "loss": 0.1531,
      "step": 13720
    },
    {
      "epoch": 2.4716471647164715,
      "grad_norm": 0.2523591220378876,
      "learning_rate": 9.155182073900908e-06,
      "loss": 0.145,
      "step": 13730
    },
    {
      "epoch": 2.4734473447344736,
      "grad_norm": 0.24645936489105225,
      "learning_rate": 9.094862940361932e-06,
      "loss": 0.1455,
      "step": 13740
    },
    {
      "epoch": 2.4752475247524752,
      "grad_norm": 0.23204006254673004,
      "learning_rate": 9.034723284624119e-06,
      "loss": 0.1432,
      "step": 13750
    },
    {
      "epoch": 2.477047704770477,
      "grad_norm": 0.24490049481391907,
      "learning_rate": 8.974763370559807e-06,
      "loss": 0.1535,
      "step": 13760
    },
    {
      "epoch": 2.478847884788479,
      "grad_norm": 0.2692159116268158,
      "learning_rate": 8.91498346125264e-06,
      "loss": 0.1559,
      "step": 13770
    },
    {
      "epoch": 2.4806480648064806,
      "grad_norm": 0.2663477957248688,
      "learning_rate": 8.855383818996487e-06,
      "loss": 0.1556,
      "step": 13780
    },
    {
      "epoch": 2.4824482448244822,
      "grad_norm": 0.23829378187656403,
      "learning_rate": 8.795964705294301e-06,
      "loss": 0.1486,
      "step": 13790
    },
    {
      "epoch": 2.4842484248424843,
      "grad_norm": 0.20710891485214233,
      "learning_rate": 8.736726380856864e-06,
      "loss": 0.1421,
      "step": 13800
    },
    {
      "epoch": 2.486048604860486,
      "grad_norm": 0.26631659269332886,
      "learning_rate": 8.677669105601788e-06,
      "loss": 0.1447,
      "step": 13810
    },
    {
      "epoch": 2.487848784878488,
      "grad_norm": 0.24347911775112152,
      "learning_rate": 8.618793138652243e-06,
      "loss": 0.1509,
      "step": 13820
    },
    {
      "epoch": 2.4896489648964897,
      "grad_norm": 0.23553785681724548,
      "learning_rate": 8.560098738335914e-06,
      "loss": 0.145,
      "step": 13830
    },
    {
      "epoch": 2.4914491449144913,
      "grad_norm": 0.2728002965450287,
      "learning_rate": 8.501586162183833e-06,
      "loss": 0.1555,
      "step": 13840
    },
    {
      "epoch": 2.4932493249324934,
      "grad_norm": 0.2747136950492859,
      "learning_rate": 8.443255666929244e-06,
      "loss": 0.1463,
      "step": 13850
    },
    {
      "epoch": 2.495049504950495,
      "grad_norm": 0.2582269012928009,
      "learning_rate": 8.385107508506478e-06,
      "loss": 0.1508,
      "step": 13860
    },
    {
      "epoch": 2.4968496849684967,
      "grad_norm": 0.2704346776008606,
      "learning_rate": 8.327141942049848e-06,
      "loss": 0.1501,
      "step": 13870
    },
    {
      "epoch": 2.4986498649864988,
      "grad_norm": 0.2271580845117569,
      "learning_rate": 8.269359221892487e-06,
      "loss": 0.1497,
      "step": 13880
    },
    {
      "epoch": 2.5004500450045004,
      "grad_norm": 0.2527974843978882,
      "learning_rate": 8.211759601565284e-06,
      "loss": 0.1524,
      "step": 13890
    },
    {
      "epoch": 2.502250225022502,
      "grad_norm": 0.2711731195449829,
      "learning_rate": 8.15434333379575e-06,
      "loss": 0.151,
      "step": 13900
    },
    {
      "epoch": 2.504050405040504,
      "grad_norm": 0.26161253452301025,
      "learning_rate": 8.097110670506897e-06,
      "loss": 0.1481,
      "step": 13910
    },
    {
      "epoch": 2.5058505850585058,
      "grad_norm": 0.24525298178195953,
      "learning_rate": 8.04006186281615e-06,
      "loss": 0.1523,
      "step": 13920
    },
    {
      "epoch": 2.507650765076508,
      "grad_norm": 0.23461158573627472,
      "learning_rate": 7.983197161034245e-06,
      "loss": 0.1521,
      "step": 13930
    },
    {
      "epoch": 2.5094509450945095,
      "grad_norm": 0.2537933886051178,
      "learning_rate": 7.926516814664093e-06,
      "loss": 0.1496,
      "step": 13940
    },
    {
      "epoch": 2.511251125112511,
      "grad_norm": 0.24974632263183594,
      "learning_rate": 7.870021072399753e-06,
      "loss": 0.1524,
      "step": 13950
    },
    {
      "epoch": 2.513051305130513,
      "grad_norm": 0.23112963140010834,
      "learning_rate": 7.813710182125262e-06,
      "loss": 0.1514,
      "step": 13960
    },
    {
      "epoch": 2.514851485148515,
      "grad_norm": 0.24382874369621277,
      "learning_rate": 7.757584390913647e-06,
      "loss": 0.1473,
      "step": 13970
    },
    {
      "epoch": 2.5166516651665165,
      "grad_norm": 0.22634842991828918,
      "learning_rate": 7.70164394502576e-06,
      "loss": 0.1502,
      "step": 13980
    },
    {
      "epoch": 2.5184518451845186,
      "grad_norm": 0.24670659005641937,
      "learning_rate": 7.645889089909175e-06,
      "loss": 0.147,
      "step": 13990
    },
    {
      "epoch": 2.5202520252025202,
      "grad_norm": 0.2617325484752655,
      "learning_rate": 7.590320070197221e-06,
      "loss": 0.1538,
      "step": 14000
    },
    {
      "epoch": 2.5202520252025202,
      "eval_loss": 0.18476489186286926,
      "eval_runtime": 316.9381,
      "eval_samples_per_second": 124.63,
      "eval_steps_per_second": 3.897,
      "step": 14000
    },
    {
      "epoch": 2.522052205220522,
      "grad_norm": 0.2742749750614166,
      "learning_rate": 7.534937129707826e-06,
      "loss": 0.1512,
      "step": 14010
    },
    {
      "epoch": 2.523852385238524,
      "grad_norm": 0.2576739192008972,
      "learning_rate": 7.479740511442424e-06,
      "loss": 0.1457,
      "step": 14020
    },
    {
      "epoch": 2.5256525652565256,
      "grad_norm": 0.233218714594841,
      "learning_rate": 7.424730457584994e-06,
      "loss": 0.1514,
      "step": 14030
    },
    {
      "epoch": 2.5274527452745277,
      "grad_norm": 0.2466733306646347,
      "learning_rate": 7.369907209500915e-06,
      "loss": 0.1497,
      "step": 14040
    },
    {
      "epoch": 2.5292529252925293,
      "grad_norm": 0.26636990904808044,
      "learning_rate": 7.315271007735886e-06,
      "loss": 0.1461,
      "step": 14050
    },
    {
      "epoch": 2.531053105310531,
      "grad_norm": 0.2663373053073883,
      "learning_rate": 7.260822092014957e-06,
      "loss": 0.1503,
      "step": 14060
    },
    {
      "epoch": 2.5328532853285326,
      "grad_norm": 0.27210062742233276,
      "learning_rate": 7.2065607012414216e-06,
      "loss": 0.1481,
      "step": 14070
    },
    {
      "epoch": 2.5346534653465347,
      "grad_norm": 0.2604202926158905,
      "learning_rate": 7.15248707349575e-06,
      "loss": 0.1498,
      "step": 14080
    },
    {
      "epoch": 2.5364536453645363,
      "grad_norm": 0.27528998255729675,
      "learning_rate": 7.098601446034603e-06,
      "loss": 0.1512,
      "step": 14090
    },
    {
      "epoch": 2.5382538253825384,
      "grad_norm": 0.23418118059635162,
      "learning_rate": 7.044904055289753e-06,
      "loss": 0.1532,
      "step": 14100
    },
    {
      "epoch": 2.54005400540054,
      "grad_norm": 0.2663939595222473,
      "learning_rate": 6.9913951368670465e-06,
      "loss": 0.1503,
      "step": 14110
    },
    {
      "epoch": 2.5418541854185417,
      "grad_norm": 0.25160840153694153,
      "learning_rate": 6.938074925545396e-06,
      "loss": 0.146,
      "step": 14120
    },
    {
      "epoch": 2.5436543654365438,
      "grad_norm": 0.2505638897418976,
      "learning_rate": 6.884943655275694e-06,
      "loss": 0.1419,
      "step": 14130
    },
    {
      "epoch": 2.5454545454545454,
      "grad_norm": 0.2185797095298767,
      "learning_rate": 6.832001559179857e-06,
      "loss": 0.1476,
      "step": 14140
    },
    {
      "epoch": 2.5472547254725475,
      "grad_norm": 0.2648816406726837,
      "learning_rate": 6.779248869549765e-06,
      "loss": 0.1522,
      "step": 14150
    },
    {
      "epoch": 2.549054905490549,
      "grad_norm": 0.2706734240055084,
      "learning_rate": 6.72668581784624e-06,
      "loss": 0.1565,
      "step": 14160
    },
    {
      "epoch": 2.550855085508551,
      "grad_norm": 0.2829717695713043,
      "learning_rate": 6.674312634698038e-06,
      "loss": 0.1545,
      "step": 14170
    },
    {
      "epoch": 2.5526552655265524,
      "grad_norm": 0.236842080950737,
      "learning_rate": 6.622129549900852e-06,
      "loss": 0.148,
      "step": 14180
    },
    {
      "epoch": 2.5544554455445545,
      "grad_norm": 0.26996558904647827,
      "learning_rate": 6.5701367924162486e-06,
      "loss": 0.1486,
      "step": 14190
    },
    {
      "epoch": 2.556255625562556,
      "grad_norm": 0.26158231496810913,
      "learning_rate": 6.518334590370745e-06,
      "loss": 0.1469,
      "step": 14200
    },
    {
      "epoch": 2.5580558055805582,
      "grad_norm": 0.27110418677330017,
      "learning_rate": 6.466723171054745e-06,
      "loss": 0.1512,
      "step": 14210
    },
    {
      "epoch": 2.55985598559856,
      "grad_norm": 0.23923726379871368,
      "learning_rate": 6.415302760921565e-06,
      "loss": 0.1486,
      "step": 14220
    },
    {
      "epoch": 2.5616561656165615,
      "grad_norm": 0.24342317879199982,
      "learning_rate": 6.364073585586461e-06,
      "loss": 0.1476,
      "step": 14230
    },
    {
      "epoch": 2.5634563456345636,
      "grad_norm": 0.27753910422325134,
      "learning_rate": 6.313035869825573e-06,
      "loss": 0.1447,
      "step": 14240
    },
    {
      "epoch": 2.5652565256525652,
      "grad_norm": 0.2577684223651886,
      "learning_rate": 6.262189837575005e-06,
      "loss": 0.152,
      "step": 14250
    },
    {
      "epoch": 2.5670567056705673,
      "grad_norm": 0.2681333124637604,
      "learning_rate": 6.211535711929839e-06,
      "loss": 0.1478,
      "step": 14260
    },
    {
      "epoch": 2.568856885688569,
      "grad_norm": 0.2367333322763443,
      "learning_rate": 6.161073715143079e-06,
      "loss": 0.1519,
      "step": 14270
    },
    {
      "epoch": 2.5706570657065706,
      "grad_norm": 0.24715279042720795,
      "learning_rate": 6.110804068624804e-06,
      "loss": 0.1436,
      "step": 14280
    },
    {
      "epoch": 2.5724572457245722,
      "grad_norm": 0.25472119450569153,
      "learning_rate": 6.06072699294109e-06,
      "loss": 0.1488,
      "step": 14290
    },
    {
      "epoch": 2.5742574257425743,
      "grad_norm": 0.25060924887657166,
      "learning_rate": 6.010842707813064e-06,
      "loss": 0.1473,
      "step": 14300
    },
    {
      "epoch": 2.576057605760576,
      "grad_norm": 0.2763085663318634,
      "learning_rate": 5.961151432115964e-06,
      "loss": 0.1504,
      "step": 14310
    },
    {
      "epoch": 2.577857785778578,
      "grad_norm": 0.24455751478672028,
      "learning_rate": 5.911653383878202e-06,
      "loss": 0.1527,
      "step": 14320
    },
    {
      "epoch": 2.5796579657965797,
      "grad_norm": 0.2584972679615021,
      "learning_rate": 5.862348780280297e-06,
      "loss": 0.149,
      "step": 14330
    },
    {
      "epoch": 2.5814581458145813,
      "grad_norm": 0.26401957869529724,
      "learning_rate": 5.813237837654095e-06,
      "loss": 0.1491,
      "step": 14340
    },
    {
      "epoch": 2.5832583258325834,
      "grad_norm": 0.24730351567268372,
      "learning_rate": 5.764320771481657e-06,
      "loss": 0.1494,
      "step": 14350
    },
    {
      "epoch": 2.585058505850585,
      "grad_norm": 0.26985540986061096,
      "learning_rate": 5.7155977963943955e-06,
      "loss": 0.1568,
      "step": 14360
    },
    {
      "epoch": 2.5868586858685867,
      "grad_norm": 0.2534083425998688,
      "learning_rate": 5.667069126172131e-06,
      "loss": 0.1517,
      "step": 14370
    },
    {
      "epoch": 2.588658865886589,
      "grad_norm": 0.24735015630722046,
      "learning_rate": 5.6187349737421125e-06,
      "loss": 0.152,
      "step": 14380
    },
    {
      "epoch": 2.5904590459045904,
      "grad_norm": 0.24684253334999084,
      "learning_rate": 5.570595551178137e-06,
      "loss": 0.1499,
      "step": 14390
    },
    {
      "epoch": 2.592259225922592,
      "grad_norm": 0.27820855379104614,
      "learning_rate": 5.522651069699586e-06,
      "loss": 0.1499,
      "step": 14400
    },
    {
      "epoch": 2.594059405940594,
      "grad_norm": 0.24237477779388428,
      "learning_rate": 5.4749017396705125e-06,
      "loss": 0.1464,
      "step": 14410
    },
    {
      "epoch": 2.595859585958596,
      "grad_norm": 0.2519839406013489,
      "learning_rate": 5.42734777059869e-06,
      "loss": 0.147,
      "step": 14420
    },
    {
      "epoch": 2.597659765976598,
      "grad_norm": 0.2477259784936905,
      "learning_rate": 5.3799893711347475e-06,
      "loss": 0.1478,
      "step": 14430
    },
    {
      "epoch": 2.5994599459945995,
      "grad_norm": 0.2585108280181885,
      "learning_rate": 5.332826749071185e-06,
      "loss": 0.1504,
      "step": 14440
    },
    {
      "epoch": 2.601260126012601,
      "grad_norm": 0.24557073414325714,
      "learning_rate": 5.285860111341517e-06,
      "loss": 0.1477,
      "step": 14450
    },
    {
      "epoch": 2.603060306030603,
      "grad_norm": 0.2718562185764313,
      "learning_rate": 5.239089664019347e-06,
      "loss": 0.1506,
      "step": 14460
    },
    {
      "epoch": 2.604860486048605,
      "grad_norm": 0.25463148951530457,
      "learning_rate": 5.192515612317461e-06,
      "loss": 0.1471,
      "step": 14470
    },
    {
      "epoch": 2.6066606660666065,
      "grad_norm": 0.2685580551624298,
      "learning_rate": 5.146138160586927e-06,
      "loss": 0.1494,
      "step": 14480
    },
    {
      "epoch": 2.6084608460846086,
      "grad_norm": 0.2544975280761719,
      "learning_rate": 5.0999575123162e-06,
      "loss": 0.1506,
      "step": 14490
    },
    {
      "epoch": 2.6102610261026102,
      "grad_norm": 0.2684541940689087,
      "learning_rate": 5.053973870130218e-06,
      "loss": 0.1462,
      "step": 14500
    },
    {
      "epoch": 2.6102610261026102,
      "eval_loss": 0.18441444635391235,
      "eval_runtime": 316.8121,
      "eval_samples_per_second": 124.68,
      "eval_steps_per_second": 3.898,
      "step": 14500
    },
    {
      "epoch": 2.612061206120612,
      "grad_norm": 0.25542327761650085,
      "learning_rate": 5.00818743578953e-06,
      "loss": 0.1555,
      "step": 14510
    },
    {
      "epoch": 2.613861386138614,
      "grad_norm": 0.2513805627822876,
      "learning_rate": 4.962598410189417e-06,
      "loss": 0.1526,
      "step": 14520
    },
    {
      "epoch": 2.6156615661566156,
      "grad_norm": 0.25370895862579346,
      "learning_rate": 4.917206993358981e-06,
      "loss": 0.1448,
      "step": 14530
    },
    {
      "epoch": 2.6174617461746177,
      "grad_norm": 0.24957935512065887,
      "learning_rate": 4.8720133844603e-06,
      "loss": 0.1525,
      "step": 14540
    },
    {
      "epoch": 2.6192619261926193,
      "grad_norm": 0.23052428662776947,
      "learning_rate": 4.827017781787507e-06,
      "loss": 0.1525,
      "step": 14550
    },
    {
      "epoch": 2.621062106210621,
      "grad_norm": 0.23451176285743713,
      "learning_rate": 4.782220382765984e-06,
      "loss": 0.1467,
      "step": 14560
    },
    {
      "epoch": 2.6228622862286226,
      "grad_norm": 0.2524135410785675,
      "learning_rate": 4.737621383951457e-06,
      "loss": 0.149,
      "step": 14570
    },
    {
      "epoch": 2.6246624662466247,
      "grad_norm": 0.28082311153411865,
      "learning_rate": 4.693220981029123e-06,
      "loss": 0.1532,
      "step": 14580
    },
    {
      "epoch": 2.6264626462646263,
      "grad_norm": 0.25290679931640625,
      "learning_rate": 4.649019368812824e-06,
      "loss": 0.1559,
      "step": 14590
    },
    {
      "epoch": 2.6282628262826284,
      "grad_norm": 0.2587137222290039,
      "learning_rate": 4.605016741244184e-06,
      "loss": 0.1518,
      "step": 14600
    },
    {
      "epoch": 2.63006300630063,
      "grad_norm": 0.2717086672782898,
      "learning_rate": 4.561213291391708e-06,
      "loss": 0.1436,
      "step": 14610
    },
    {
      "epoch": 2.6318631863186317,
      "grad_norm": 0.25881698727607727,
      "learning_rate": 4.517609211450025e-06,
      "loss": 0.1544,
      "step": 14620
    },
    {
      "epoch": 2.633663366336634,
      "grad_norm": 0.26126447319984436,
      "learning_rate": 4.474204692738981e-06,
      "loss": 0.147,
      "step": 14630
    },
    {
      "epoch": 2.6354635463546354,
      "grad_norm": 0.2291233092546463,
      "learning_rate": 4.430999925702789e-06,
      "loss": 0.1452,
      "step": 14640
    },
    {
      "epoch": 2.6372637263726375,
      "grad_norm": 0.24102480709552765,
      "learning_rate": 4.387995099909276e-06,
      "loss": 0.1467,
      "step": 14650
    },
    {
      "epoch": 2.639063906390639,
      "grad_norm": 0.24567081034183502,
      "learning_rate": 4.345190404048932e-06,
      "loss": 0.1444,
      "step": 14660
    },
    {
      "epoch": 2.640864086408641,
      "grad_norm": 0.2591971158981323,
      "learning_rate": 4.302586025934197e-06,
      "loss": 0.1506,
      "step": 14670
    },
    {
      "epoch": 2.6426642664266424,
      "grad_norm": 0.2461821585893631,
      "learning_rate": 4.260182152498554e-06,
      "loss": 0.1444,
      "step": 14680
    },
    {
      "epoch": 2.6444644464446445,
      "grad_norm": 0.2709466814994812,
      "learning_rate": 4.217978969795739e-06,
      "loss": 0.1502,
      "step": 14690
    },
    {
      "epoch": 2.646264626462646,
      "grad_norm": 0.25642845034599304,
      "learning_rate": 4.175976662998948e-06,
      "loss": 0.149,
      "step": 14700
    },
    {
      "epoch": 2.6480648064806482,
      "grad_norm": 0.2589831054210663,
      "learning_rate": 4.1341754163999824e-06,
      "loss": 0.151,
      "step": 14710
    },
    {
      "epoch": 2.64986498649865,
      "grad_norm": 0.22978566586971283,
      "learning_rate": 4.092575413408472e-06,
      "loss": 0.1479,
      "step": 14720
    },
    {
      "epoch": 2.6516651665166515,
      "grad_norm": 0.2351156771183014,
      "learning_rate": 4.051176836551051e-06,
      "loss": 0.1458,
      "step": 14730
    },
    {
      "epoch": 2.6534653465346536,
      "grad_norm": 0.22891822457313538,
      "learning_rate": 4.0099798674705705e-06,
      "loss": 0.1498,
      "step": 14740
    },
    {
      "epoch": 2.6552655265526552,
      "grad_norm": 0.25037139654159546,
      "learning_rate": 3.968984686925281e-06,
      "loss": 0.1489,
      "step": 14750
    },
    {
      "epoch": 2.6570657065706573,
      "grad_norm": 0.24280855059623718,
      "learning_rate": 3.928191474788068e-06,
      "loss": 0.1503,
      "step": 14760
    },
    {
      "epoch": 2.658865886588659,
      "grad_norm": 0.2644270062446594,
      "learning_rate": 3.887600410045644e-06,
      "loss": 0.153,
      "step": 14770
    },
    {
      "epoch": 2.6606660666066606,
      "grad_norm": 0.27027636766433716,
      "learning_rate": 3.847211670797768e-06,
      "loss": 0.1544,
      "step": 14780
    },
    {
      "epoch": 2.6624662466246622,
      "grad_norm": 0.22358232736587524,
      "learning_rate": 3.807025434256478e-06,
      "loss": 0.1496,
      "step": 14790
    },
    {
      "epoch": 2.6642664266426643,
      "grad_norm": 0.26742109656333923,
      "learning_rate": 3.7670418767452585e-06,
      "loss": 0.1469,
      "step": 14800
    },
    {
      "epoch": 2.666066606660666,
      "grad_norm": 0.28148937225341797,
      "learning_rate": 3.7272611736983485e-06,
      "loss": 0.1466,
      "step": 14810
    },
    {
      "epoch": 2.667866786678668,
      "grad_norm": 0.27054330706596375,
      "learning_rate": 3.687683499659911e-06,
      "loss": 0.1535,
      "step": 14820
    },
    {
      "epoch": 2.6696669666966697,
      "grad_norm": 0.24478384852409363,
      "learning_rate": 3.648309028283298e-06,
      "loss": 0.1455,
      "step": 14830
    },
    {
      "epoch": 2.6714671467146713,
      "grad_norm": 0.24040727317333221,
      "learning_rate": 3.6091379323302643e-06,
      "loss": 0.1485,
      "step": 14840
    },
    {
      "epoch": 2.6732673267326734,
      "grad_norm": 0.23816458880901337,
      "learning_rate": 3.570170383670246e-06,
      "loss": 0.1517,
      "step": 14850
    },
    {
      "epoch": 2.675067506750675,
      "grad_norm": 0.24884434044361115,
      "learning_rate": 3.5314065532795426e-06,
      "loss": 0.1506,
      "step": 14860
    },
    {
      "epoch": 2.6768676867686767,
      "grad_norm": 0.24807927012443542,
      "learning_rate": 3.4928466112406468e-06,
      "loss": 0.1469,
      "step": 14870
    },
    {
      "epoch": 2.678667866786679,
      "grad_norm": 0.26730403304100037,
      "learning_rate": 3.454490726741438e-06,
      "loss": 0.1507,
      "step": 14880
    },
    {
      "epoch": 2.6804680468046804,
      "grad_norm": 0.25136443972587585,
      "learning_rate": 3.4163390680744644e-06,
      "loss": 0.1522,
      "step": 14890
    },
    {
      "epoch": 2.682268226822682,
      "grad_norm": 0.26462504267692566,
      "learning_rate": 3.3783918026362105e-06,
      "loss": 0.1491,
      "step": 14900
    },
    {
      "epoch": 2.684068406840684,
      "grad_norm": 0.24733828008174896,
      "learning_rate": 3.3406490969263403e-06,
      "loss": 0.1483,
      "step": 14910
    },
    {
      "epoch": 2.685868586858686,
      "grad_norm": 0.2524469792842865,
      "learning_rate": 3.3031111165469807e-06,
      "loss": 0.1488,
      "step": 14920
    },
    {
      "epoch": 2.687668766876688,
      "grad_norm": 0.2719024419784546,
      "learning_rate": 3.2657780262020045e-06,
      "loss": 0.1509,
      "step": 14930
    },
    {
      "epoch": 2.6894689468946895,
      "grad_norm": 0.24569503962993622,
      "learning_rate": 3.2286499896962753e-06,
      "loss": 0.148,
      "step": 14940
    },
    {
      "epoch": 2.691269126912691,
      "grad_norm": 0.2492099106311798,
      "learning_rate": 3.1917271699349715e-06,
      "loss": 0.1472,
      "step": 14950
    },
    {
      "epoch": 2.693069306930693,
      "grad_norm": 0.30621275305747986,
      "learning_rate": 3.155009728922853e-06,
      "loss": 0.1507,
      "step": 14960
    },
    {
      "epoch": 2.694869486948695,
      "grad_norm": 0.24573780596256256,
      "learning_rate": 3.1184978277635224e-06,
      "loss": 0.1525,
      "step": 14970
    },
    {
      "epoch": 2.6966696669666965,
      "grad_norm": 0.2667548656463623,
      "learning_rate": 3.0821916266587704e-06,
      "loss": 0.1468,
      "step": 14980
    },
    {
      "epoch": 2.6984698469846986,
      "grad_norm": 0.2779809236526489,
      "learning_rate": 3.0460912849078373e-06,
      "loss": 0.1481,
      "step": 14990
    },
    {
      "epoch": 2.7002700270027002,
      "grad_norm": 0.2689227759838104,
      "learning_rate": 3.0101969609067026e-06,
      "loss": 0.1525,
      "step": 15000
    },
    {
      "epoch": 2.7002700270027002,
      "eval_loss": 0.1841530054807663,
      "eval_runtime": 316.8744,
      "eval_samples_per_second": 124.655,
      "eval_steps_per_second": 3.897,
      "step": 15000
    },
    {
      "epoch": 2.702070207020702,
      "grad_norm": 0.2605115473270416,
      "learning_rate": 2.9745088121474306e-06,
      "loss": 0.1461,
      "step": 15010
    },
    {
      "epoch": 2.703870387038704,
      "grad_norm": 0.2522047162055969,
      "learning_rate": 2.9390269952174752e-06,
      "loss": 0.1487,
      "step": 15020
    },
    {
      "epoch": 2.7056705670567056,
      "grad_norm": 0.2660447061061859,
      "learning_rate": 2.9037516657989426e-06,
      "loss": 0.15,
      "step": 15030
    },
    {
      "epoch": 2.7074707470747077,
      "grad_norm": 0.24962642788887024,
      "learning_rate": 2.8686829786679524e-06,
      "loss": 0.1483,
      "step": 15040
    },
    {
      "epoch": 2.7092709270927093,
      "grad_norm": 0.2292640060186386,
      "learning_rate": 2.833821087693972e-06,
      "loss": 0.1488,
      "step": 15050
    },
    {
      "epoch": 2.711071107110711,
      "grad_norm": 0.2553640305995941,
      "learning_rate": 2.799166145839072e-06,
      "loss": 0.1435,
      "step": 15060
    },
    {
      "epoch": 2.7128712871287126,
      "grad_norm": 0.2647371292114258,
      "learning_rate": 2.7647183051573433e-06,
      "loss": 0.1505,
      "step": 15070
    },
    {
      "epoch": 2.7146714671467147,
      "grad_norm": 0.23110374808311462,
      "learning_rate": 2.7304777167941662e-06,
      "loss": 0.1527,
      "step": 15080
    },
    {
      "epoch": 2.7164716471647163,
      "grad_norm": 0.23950469493865967,
      "learning_rate": 2.6964445309855814e-06,
      "loss": 0.1521,
      "step": 15090
    },
    {
      "epoch": 2.7182718271827184,
      "grad_norm": 0.25491249561309814,
      "learning_rate": 2.662618897057606e-06,
      "loss": 0.1511,
      "step": 15100
    },
    {
      "epoch": 2.72007200720072,
      "grad_norm": 0.23746976256370544,
      "learning_rate": 2.629000963425582e-06,
      "loss": 0.1513,
      "step": 15110
    },
    {
      "epoch": 2.7218721872187217,
      "grad_norm": 0.26366907358169556,
      "learning_rate": 2.5955908775935567e-06,
      "loss": 0.1492,
      "step": 15120
    },
    {
      "epoch": 2.723672367236724,
      "grad_norm": 0.2542189955711365,
      "learning_rate": 2.5623887861536022e-06,
      "loss": 0.1504,
      "step": 15130
    },
    {
      "epoch": 2.7254725472547254,
      "grad_norm": 0.23070378601551056,
      "learning_rate": 2.529394834785176e-06,
      "loss": 0.147,
      "step": 15140
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 0.2437846064567566,
      "learning_rate": 2.4966091682544933e-06,
      "loss": 0.1423,
      "step": 15150
    },
    {
      "epoch": 2.729072907290729,
      "grad_norm": 0.26925092935562134,
      "learning_rate": 2.4640319304138904e-06,
      "loss": 0.1486,
      "step": 15160
    },
    {
      "epoch": 2.730873087308731,
      "grad_norm": 0.2459663599729538,
      "learning_rate": 2.4316632642011785e-06,
      "loss": 0.1489,
      "step": 15170
    },
    {
      "epoch": 2.7326732673267324,
      "grad_norm": 0.25734445452690125,
      "learning_rate": 2.399503311639034e-06,
      "loss": 0.1466,
      "step": 15180
    },
    {
      "epoch": 2.7344734473447345,
      "grad_norm": 0.24813705682754517,
      "learning_rate": 2.367552213834373e-06,
      "loss": 0.1485,
      "step": 15190
    },
    {
      "epoch": 2.736273627362736,
      "grad_norm": 0.23575493693351746,
      "learning_rate": 2.335810110977721e-06,
      "loss": 0.148,
      "step": 15200
    },
    {
      "epoch": 2.7380738073807382,
      "grad_norm": 0.24375689029693604,
      "learning_rate": 2.3042771423426147e-06,
      "loss": 0.1487,
      "step": 15210
    },
    {
      "epoch": 2.73987398739874,
      "grad_norm": 0.2605895698070526,
      "learning_rate": 2.2729534462849643e-06,
      "loss": 0.1499,
      "step": 15220
    },
    {
      "epoch": 2.7416741674167415,
      "grad_norm": 0.259457528591156,
      "learning_rate": 2.241839160242476e-06,
      "loss": 0.152,
      "step": 15230
    },
    {
      "epoch": 2.7434743474347436,
      "grad_norm": 0.2650042474269867,
      "learning_rate": 2.21093442073404e-06,
      "loss": 0.1478,
      "step": 15240
    },
    {
      "epoch": 2.7452745274527453,
      "grad_norm": 0.263526976108551,
      "learning_rate": 2.1802393633591043e-06,
      "loss": 0.1513,
      "step": 15250
    },
    {
      "epoch": 2.7470747074707473,
      "grad_norm": 0.2525840401649475,
      "learning_rate": 2.1497541227971376e-06,
      "loss": 0.1469,
      "step": 15260
    },
    {
      "epoch": 2.748874887488749,
      "grad_norm": 0.23625925183296204,
      "learning_rate": 2.119478832806987e-06,
      "loss": 0.1499,
      "step": 15270
    },
    {
      "epoch": 2.7506750675067506,
      "grad_norm": 0.26291337609291077,
      "learning_rate": 2.0894136262262886e-06,
      "loss": 0.1443,
      "step": 15280
    },
    {
      "epoch": 2.7524752475247523,
      "grad_norm": 0.2513592839241028,
      "learning_rate": 2.059558634970937e-06,
      "loss": 0.1493,
      "step": 15290
    },
    {
      "epoch": 2.7542754275427543,
      "grad_norm": 0.25313541293144226,
      "learning_rate": 2.0299139900344653e-06,
      "loss": 0.1516,
      "step": 15300
    },
    {
      "epoch": 2.756075607560756,
      "grad_norm": 0.24161511659622192,
      "learning_rate": 2.000479821487461e-06,
      "loss": 0.1465,
      "step": 15310
    },
    {
      "epoch": 2.757875787578758,
      "grad_norm": 0.2668939232826233,
      "learning_rate": 1.9712562584770168e-06,
      "loss": 0.1558,
      "step": 15320
    },
    {
      "epoch": 2.7596759675967597,
      "grad_norm": 0.2625901401042938,
      "learning_rate": 1.9422434292261926e-06,
      "loss": 0.1463,
      "step": 15330
    },
    {
      "epoch": 2.7614761476147613,
      "grad_norm": 0.25404852628707886,
      "learning_rate": 1.9134414610333775e-06,
      "loss": 0.1462,
      "step": 15340
    },
    {
      "epoch": 2.7632763276327634,
      "grad_norm": 0.23519384860992432,
      "learning_rate": 1.8848504802718047e-06,
      "loss": 0.1474,
      "step": 15350
    },
    {
      "epoch": 2.765076507650765,
      "grad_norm": 0.26018208265304565,
      "learning_rate": 1.8564706123889385e-06,
      "loss": 0.1482,
      "step": 15360
    },
    {
      "epoch": 2.766876687668767,
      "grad_norm": 0.2638219892978668,
      "learning_rate": 1.828301981905972e-06,
      "loss": 0.1458,
      "step": 15370
    },
    {
      "epoch": 2.768676867686769,
      "grad_norm": 0.2408057600259781,
      "learning_rate": 1.8003447124172678e-06,
      "loss": 0.1468,
      "step": 15380
    },
    {
      "epoch": 2.7704770477047704,
      "grad_norm": 0.24625858664512634,
      "learning_rate": 1.7725989265897803e-06,
      "loss": 0.1465,
      "step": 15390
    },
    {
      "epoch": 2.772277227722772,
      "grad_norm": 0.27019932866096497,
      "learning_rate": 1.745064746162578e-06,
      "loss": 0.1505,
      "step": 15400
    },
    {
      "epoch": 2.774077407740774,
      "grad_norm": 0.2558149993419647,
      "learning_rate": 1.717742291946256e-06,
      "loss": 0.1526,
      "step": 15410
    },
    {
      "epoch": 2.775877587758776,
      "grad_norm": 0.26841697096824646,
      "learning_rate": 1.690631683822419e-06,
      "loss": 0.1493,
      "step": 15420
    },
    {
      "epoch": 2.777677767776778,
      "grad_norm": 0.24670471251010895,
      "learning_rate": 1.663733040743193e-06,
      "loss": 0.1497,
      "step": 15430
    },
    {
      "epoch": 2.7794779477947795,
      "grad_norm": 0.22726938128471375,
      "learning_rate": 1.637046480730653e-06,
      "loss": 0.145,
      "step": 15440
    },
    {
      "epoch": 2.781278127812781,
      "grad_norm": 0.2703075110912323,
      "learning_rate": 1.610572120876319e-06,
      "loss": 0.1504,
      "step": 15450
    },
    {
      "epoch": 2.783078307830783,
      "grad_norm": 0.2631528079509735,
      "learning_rate": 1.5843100773406782e-06,
      "loss": 0.1581,
      "step": 15460
    },
    {
      "epoch": 2.784878487848785,
      "grad_norm": 0.2453453093767166,
      "learning_rate": 1.5582604653526177e-06,
      "loss": 0.156,
      "step": 15470
    },
    {
      "epoch": 2.7866786678667865,
      "grad_norm": 0.2584494352340698,
      "learning_rate": 1.5324233992089486e-06,
      "loss": 0.1441,
      "step": 15480
    },
    {
      "epoch": 2.7884788478847886,
      "grad_norm": 0.2735593318939209,
      "learning_rate": 1.5067989922739278e-06,
      "loss": 0.1486,
      "step": 15490
    },
    {
      "epoch": 2.7902790279027903,
      "grad_norm": 0.2688192129135132,
      "learning_rate": 1.4813873569787085e-06,
      "loss": 0.1524,
      "step": 15500
    },
    {
      "epoch": 2.7902790279027903,
      "eval_loss": 0.1839544177055359,
      "eval_runtime": 316.874,
      "eval_samples_per_second": 124.655,
      "eval_steps_per_second": 3.897,
      "step": 15500
    },
    {
      "epoch": 2.792079207920792,
      "grad_norm": 0.2587384581565857,
      "learning_rate": 1.4561886048208906e-06,
      "loss": 0.1492,
      "step": 15510
    },
    {
      "epoch": 2.793879387938794,
      "grad_norm": 0.23423872888088226,
      "learning_rate": 1.4312028463640214e-06,
      "loss": 0.1494,
      "step": 15520
    },
    {
      "epoch": 2.7956795679567956,
      "grad_norm": 0.23268194496631622,
      "learning_rate": 1.4064301912370736e-06,
      "loss": 0.1481,
      "step": 15530
    },
    {
      "epoch": 2.7974797479747977,
      "grad_norm": 0.2556062340736389,
      "learning_rate": 1.3818707481340288e-06,
      "loss": 0.1491,
      "step": 15540
    },
    {
      "epoch": 2.7992799279927993,
      "grad_norm": 0.23285488784313202,
      "learning_rate": 1.3575246248133554e-06,
      "loss": 0.1469,
      "step": 15550
    },
    {
      "epoch": 2.801080108010801,
      "grad_norm": 0.2658802270889282,
      "learning_rate": 1.3333919280975326e-06,
      "loss": 0.1544,
      "step": 15560
    },
    {
      "epoch": 2.8028802880288026,
      "grad_norm": 0.24913348257541656,
      "learning_rate": 1.3094727638726213e-06,
      "loss": 0.1488,
      "step": 15570
    },
    {
      "epoch": 2.8046804680468047,
      "grad_norm": 0.25489741563796997,
      "learning_rate": 1.2857672370877593e-06,
      "loss": 0.1473,
      "step": 15580
    },
    {
      "epoch": 2.8064806480648063,
      "grad_norm": 0.24931199848651886,
      "learning_rate": 1.2622754517547186e-06,
      "loss": 0.1476,
      "step": 15590
    },
    {
      "epoch": 2.8082808280828084,
      "grad_norm": 0.27009451389312744,
      "learning_rate": 1.2389975109474483e-06,
      "loss": 0.1477,
      "step": 15600
    },
    {
      "epoch": 2.81008100810081,
      "grad_norm": 0.24279376864433289,
      "learning_rate": 1.2159335168016207e-06,
      "loss": 0.1454,
      "step": 15610
    },
    {
      "epoch": 2.8118811881188117,
      "grad_norm": 0.26038306951522827,
      "learning_rate": 1.1930835705141807e-06,
      "loss": 0.1499,
      "step": 15620
    },
    {
      "epoch": 2.813681368136814,
      "grad_norm": 0.23750147223472595,
      "learning_rate": 1.1704477723428975e-06,
      "loss": 0.1471,
      "step": 15630
    },
    {
      "epoch": 2.8154815481548154,
      "grad_norm": 0.25444722175598145,
      "learning_rate": 1.1480262216059523e-06,
      "loss": 0.1455,
      "step": 15640
    },
    {
      "epoch": 2.8172817281728175,
      "grad_norm": 0.26549386978149414,
      "learning_rate": 1.125819016681462e-06,
      "loss": 0.152,
      "step": 15650
    },
    {
      "epoch": 2.819081908190819,
      "grad_norm": 0.24366837739944458,
      "learning_rate": 1.1038262550070788e-06,
      "loss": 0.1476,
      "step": 15660
    },
    {
      "epoch": 2.820882088208821,
      "grad_norm": 0.2591298222541809,
      "learning_rate": 1.0820480330795413e-06,
      "loss": 0.1515,
      "step": 15670
    },
    {
      "epoch": 2.8226822682268224,
      "grad_norm": 0.27928173542022705,
      "learning_rate": 1.0604844464542629e-06,
      "loss": 0.1525,
      "step": 15680
    },
    {
      "epoch": 2.8244824482448245,
      "grad_norm": 0.2531876862049103,
      "learning_rate": 1.0391355897449163e-06,
      "loss": 0.1477,
      "step": 15690
    },
    {
      "epoch": 2.826282628262826,
      "grad_norm": 0.28459903597831726,
      "learning_rate": 1.0180015566230105e-06,
      "loss": 0.1538,
      "step": 15700
    },
    {
      "epoch": 2.8280828082808283,
      "grad_norm": 0.23398880660533905,
      "learning_rate": 9.97082439817476e-07,
      "loss": 0.1475,
      "step": 15710
    },
    {
      "epoch": 2.82988298829883,
      "grad_norm": 0.2824847996234894,
      "learning_rate": 9.763783311142804e-07,
      "loss": 0.151,
      "step": 15720
    },
    {
      "epoch": 2.8316831683168315,
      "grad_norm": 0.23989708721637726,
      "learning_rate": 9.558893213559795e-07,
      "loss": 0.1476,
      "step": 15730
    },
    {
      "epoch": 2.8334833483348336,
      "grad_norm": 0.25739720463752747,
      "learning_rate": 9.356155004413724e-07,
      "loss": 0.1478,
      "step": 15740
    },
    {
      "epoch": 2.8352835283528353,
      "grad_norm": 0.23915521800518036,
      "learning_rate": 9.155569573250811e-07,
      "loss": 0.1503,
      "step": 15750
    },
    {
      "epoch": 2.8370837083708373,
      "grad_norm": 0.28185513615608215,
      "learning_rate": 8.957137800171544e-07,
      "loss": 0.1521,
      "step": 15760
    },
    {
      "epoch": 2.838883888388839,
      "grad_norm": 0.2462952882051468,
      "learning_rate": 8.760860555826922e-07,
      "loss": 0.1488,
      "step": 15770
    },
    {
      "epoch": 2.8406840684068406,
      "grad_norm": 0.26540225744247437,
      "learning_rate": 8.566738701414556e-07,
      "loss": 0.1517,
      "step": 15780
    },
    {
      "epoch": 2.8424842484248423,
      "grad_norm": 0.24777419865131378,
      "learning_rate": 8.374773088675014e-07,
      "loss": 0.147,
      "step": 15790
    },
    {
      "epoch": 2.8442844284428443,
      "grad_norm": 0.2147495150566101,
      "learning_rate": 8.184964559888042e-07,
      "loss": 0.1417,
      "step": 15800
    },
    {
      "epoch": 2.846084608460846,
      "grad_norm": 0.22454354166984558,
      "learning_rate": 7.997313947868679e-07,
      "loss": 0.1452,
      "step": 15810
    },
    {
      "epoch": 2.847884788478848,
      "grad_norm": 0.2616855502128601,
      "learning_rate": 7.811822075963926e-07,
      "loss": 0.1455,
      "step": 15820
    },
    {
      "epoch": 2.8496849684968497,
      "grad_norm": 0.2501278221607208,
      "learning_rate": 7.628489758048918e-07,
      "loss": 0.1502,
      "step": 15830
    },
    {
      "epoch": 2.8514851485148514,
      "grad_norm": 0.24493446946144104,
      "learning_rate": 7.447317798523313e-07,
      "loss": 0.1517,
      "step": 15840
    },
    {
      "epoch": 2.8532853285328534,
      "grad_norm": 0.2605389356613159,
      "learning_rate": 7.268306992307961e-07,
      "loss": 0.1499,
      "step": 15850
    },
    {
      "epoch": 2.855085508550855,
      "grad_norm": 0.24543540179729462,
      "learning_rate": 7.091458124841299e-07,
      "loss": 0.1464,
      "step": 15860
    },
    {
      "epoch": 2.856885688568857,
      "grad_norm": 0.2754738926887512,
      "learning_rate": 6.916771972075853e-07,
      "loss": 0.1522,
      "step": 15870
    },
    {
      "epoch": 2.858685868586859,
      "grad_norm": 0.2708391845226288,
      "learning_rate": 6.744249300474959e-07,
      "loss": 0.1495,
      "step": 15880
    },
    {
      "epoch": 2.8604860486048604,
      "grad_norm": 0.25527334213256836,
      "learning_rate": 6.573890867009324e-07,
      "loss": 0.1441,
      "step": 15890
    },
    {
      "epoch": 2.862286228622862,
      "grad_norm": 0.2569367289543152,
      "learning_rate": 6.405697419153645e-07,
      "loss": 0.1477,
      "step": 15900
    },
    {
      "epoch": 2.864086408640864,
      "grad_norm": 0.2683582901954651,
      "learning_rate": 6.239669694883488e-07,
      "loss": 0.147,
      "step": 15910
    },
    {
      "epoch": 2.865886588658866,
      "grad_norm": 0.2617550790309906,
      "learning_rate": 6.075808422671858e-07,
      "loss": 0.1544,
      "step": 15920
    },
    {
      "epoch": 2.867686768676868,
      "grad_norm": 0.27115964889526367,
      "learning_rate": 5.914114321486253e-07,
      "loss": 0.1456,
      "step": 15930
    },
    {
      "epoch": 2.8694869486948695,
      "grad_norm": 0.27563372254371643,
      "learning_rate": 5.754588100785274e-07,
      "loss": 0.1504,
      "step": 15940
    },
    {
      "epoch": 2.871287128712871,
      "grad_norm": 0.2443901002407074,
      "learning_rate": 5.597230460515579e-07,
      "loss": 0.1496,
      "step": 15950
    },
    {
      "epoch": 2.873087308730873,
      "grad_norm": 0.2179286628961563,
      "learning_rate": 5.442042091108879e-07,
      "loss": 0.1494,
      "step": 15960
    },
    {
      "epoch": 2.874887488748875,
      "grad_norm": 0.26086828112602234,
      "learning_rate": 5.289023673478888e-07,
      "loss": 0.1514,
      "step": 15970
    },
    {
      "epoch": 2.8766876687668765,
      "grad_norm": 0.2388072907924652,
      "learning_rate": 5.138175879018215e-07,
      "loss": 0.1387,
      "step": 15980
    },
    {
      "epoch": 2.8784878487848786,
      "grad_norm": 0.25155436992645264,
      "learning_rate": 4.98949936959564e-07,
      "loss": 0.146,
      "step": 15990
    },
    {
      "epoch": 2.8802880288028803,
      "grad_norm": 0.26457369327545166,
      "learning_rate": 4.842994797552903e-07,
      "loss": 0.1551,
      "step": 16000
    },
    {
      "epoch": 2.8802880288028803,
      "eval_loss": 0.1839226484298706,
      "eval_runtime": 316.7621,
      "eval_samples_per_second": 124.699,
      "eval_steps_per_second": 3.899,
      "step": 16000
    },
    {
      "epoch": 2.882088208820882,
      "grad_norm": 0.273855984210968,
      "learning_rate": 4.698662805702303e-07,
      "loss": 0.1469,
      "step": 16010
    },
    {
      "epoch": 2.883888388838884,
      "grad_norm": 0.2336675524711609,
      "learning_rate": 4.5565040273232717e-07,
      "loss": 0.1448,
      "step": 16020
    },
    {
      "epoch": 2.8856885688568856,
      "grad_norm": 0.2694539427757263,
      "learning_rate": 4.4165190861601977e-07,
      "loss": 0.1497,
      "step": 16030
    },
    {
      "epoch": 2.8874887488748877,
      "grad_norm": 0.24019034206867218,
      "learning_rate": 4.2787085964192675e-07,
      "loss": 0.1449,
      "step": 16040
    },
    {
      "epoch": 2.8892889288928894,
      "grad_norm": 0.24828891456127167,
      "learning_rate": 4.143073162765909e-07,
      "loss": 0.1528,
      "step": 16050
    },
    {
      "epoch": 2.891089108910891,
      "grad_norm": 0.25545403361320496,
      "learning_rate": 4.009613380322186e-07,
      "loss": 0.1509,
      "step": 16060
    },
    {
      "epoch": 2.8928892889288926,
      "grad_norm": 0.22392459213733673,
      "learning_rate": 3.878329834664185e-07,
      "loss": 0.1457,
      "step": 16070
    },
    {
      "epoch": 2.8946894689468947,
      "grad_norm": 0.27916207909584045,
      "learning_rate": 3.7492231018193546e-07,
      "loss": 0.1536,
      "step": 16080
    },
    {
      "epoch": 2.8964896489648964,
      "grad_norm": 0.269851416349411,
      "learning_rate": 3.6222937482640054e-07,
      "loss": 0.1509,
      "step": 16090
    },
    {
      "epoch": 2.8982898289828984,
      "grad_norm": 0.26758426427841187,
      "learning_rate": 3.497542330920922e-07,
      "loss": 0.148,
      "step": 16100
    },
    {
      "epoch": 2.9000900090009,
      "grad_norm": 0.2428741604089737,
      "learning_rate": 3.374969397156758e-07,
      "loss": 0.1521,
      "step": 16110
    },
    {
      "epoch": 2.9018901890189017,
      "grad_norm": 0.24811172485351562,
      "learning_rate": 3.2545754847798096e-07,
      "loss": 0.1452,
      "step": 16120
    },
    {
      "epoch": 2.903690369036904,
      "grad_norm": 0.25179925560951233,
      "learning_rate": 3.136361122037523e-07,
      "loss": 0.1523,
      "step": 16130
    },
    {
      "epoch": 2.9054905490549054,
      "grad_norm": 0.25934886932373047,
      "learning_rate": 3.0203268276142724e-07,
      "loss": 0.1517,
      "step": 16140
    },
    {
      "epoch": 2.9072907290729075,
      "grad_norm": 0.28840506076812744,
      "learning_rate": 2.9064731106289713e-07,
      "loss": 0.1447,
      "step": 16150
    },
    {
      "epoch": 2.909090909090909,
      "grad_norm": 0.23484854400157928,
      "learning_rate": 2.794800470632908e-07,
      "loss": 0.1519,
      "step": 16160
    },
    {
      "epoch": 2.910891089108911,
      "grad_norm": 0.2751929461956024,
      "learning_rate": 2.6853093976075825e-07,
      "loss": 0.1504,
      "step": 16170
    },
    {
      "epoch": 2.9126912691269125,
      "grad_norm": 0.248916357755661,
      "learning_rate": 2.578000371962541e-07,
      "loss": 0.1485,
      "step": 16180
    },
    {
      "epoch": 2.9144914491449145,
      "grad_norm": 0.23560328781604767,
      "learning_rate": 2.472873864533154e-07,
      "loss": 0.1525,
      "step": 16190
    },
    {
      "epoch": 2.916291629162916,
      "grad_norm": 0.24383249878883362,
      "learning_rate": 2.369930336578785e-07,
      "loss": 0.1479,
      "step": 16200
    },
    {
      "epoch": 2.9180918091809183,
      "grad_norm": 0.24726276099681854,
      "learning_rate": 2.269170239780516e-07,
      "loss": 0.1449,
      "step": 16210
    },
    {
      "epoch": 2.91989198919892,
      "grad_norm": 0.2594893276691437,
      "learning_rate": 2.170594016239258e-07,
      "loss": 0.1481,
      "step": 16220
    },
    {
      "epoch": 2.9216921692169215,
      "grad_norm": 0.25005781650543213,
      "learning_rate": 2.0742020984739763e-07,
      "loss": 0.1492,
      "step": 16230
    },
    {
      "epoch": 2.9234923492349236,
      "grad_norm": 0.23853708803653717,
      "learning_rate": 1.9799949094195247e-07,
      "loss": 0.1469,
      "step": 16240
    },
    {
      "epoch": 2.9252925292529253,
      "grad_norm": 0.26397478580474854,
      "learning_rate": 1.8879728624249248e-07,
      "loss": 0.1457,
      "step": 16250
    },
    {
      "epoch": 2.9270927092709274,
      "grad_norm": 0.24489332735538483,
      "learning_rate": 1.79813636125159e-07,
      "loss": 0.1449,
      "step": 16260
    },
    {
      "epoch": 2.928892889288929,
      "grad_norm": 0.2689048647880554,
      "learning_rate": 1.7104858000713818e-07,
      "loss": 0.1505,
      "step": 16270
    },
    {
      "epoch": 2.9306930693069306,
      "grad_norm": 0.24991478025913239,
      "learning_rate": 1.6250215634652787e-07,
      "loss": 0.149,
      "step": 16280
    },
    {
      "epoch": 2.9324932493249323,
      "grad_norm": 0.2709287703037262,
      "learning_rate": 1.5417440264210435e-07,
      "loss": 0.1536,
      "step": 16290
    },
    {
      "epoch": 2.9342934293429344,
      "grad_norm": 0.2631204426288605,
      "learning_rate": 1.4606535543321698e-07,
      "loss": 0.1485,
      "step": 16300
    },
    {
      "epoch": 2.936093609360936,
      "grad_norm": 0.2393130362033844,
      "learning_rate": 1.3817505029960487e-07,
      "loss": 0.1427,
      "step": 16310
    },
    {
      "epoch": 2.937893789378938,
      "grad_norm": 0.22632035613059998,
      "learning_rate": 1.305035218612416e-07,
      "loss": 0.1447,
      "step": 16320
    },
    {
      "epoch": 2.9396939693969397,
      "grad_norm": 0.22549673914909363,
      "learning_rate": 1.2305080377817413e-07,
      "loss": 0.1416,
      "step": 16330
    },
    {
      "epoch": 2.9414941494149414,
      "grad_norm": 0.25479191541671753,
      "learning_rate": 1.1581692875038408e-07,
      "loss": 0.1524,
      "step": 16340
    },
    {
      "epoch": 2.9432943294329434,
      "grad_norm": 0.2400633990764618,
      "learning_rate": 1.0880192851766002e-07,
      "loss": 0.1509,
      "step": 16350
    },
    {
      "epoch": 2.945094509450945,
      "grad_norm": 0.25138646364212036,
      "learning_rate": 1.0200583385943097e-07,
      "loss": 0.1452,
      "step": 16360
    },
    {
      "epoch": 2.946894689468947,
      "grad_norm": 0.23548929393291473,
      "learning_rate": 9.542867459463867e-08,
      "loss": 0.1509,
      "step": 16370
    },
    {
      "epoch": 2.948694869486949,
      "grad_norm": 0.2601611018180847,
      "learning_rate": 8.907047958161552e-08,
      "loss": 0.1512,
      "step": 16380
    },
    {
      "epoch": 2.9504950495049505,
      "grad_norm": 0.23375637829303741,
      "learning_rate": 8.293127671796241e-08,
      "loss": 0.1511,
      "step": 16390
    },
    {
      "epoch": 2.952295229522952,
      "grad_norm": 0.24652038514614105,
      "learning_rate": 7.701109294040442e-08,
      "loss": 0.1483,
      "step": 16400
    },
    {
      "epoch": 2.954095409540954,
      "grad_norm": 0.23191030323505402,
      "learning_rate": 7.13099542246909e-08,
      "loss": 0.1493,
      "step": 16410
    },
    {
      "epoch": 2.955895589558956,
      "grad_norm": 0.24625301361083984,
      "learning_rate": 6.582788558547881e-08,
      "loss": 0.1499,
      "step": 16420
    },
    {
      "epoch": 2.957695769576958,
      "grad_norm": 0.24682140350341797,
      "learning_rate": 6.056491107621077e-08,
      "loss": 0.1404,
      "step": 16430
    },
    {
      "epoch": 2.9594959495949595,
      "grad_norm": 0.2572762966156006,
      "learning_rate": 5.552105378903161e-08,
      "loss": 0.1468,
      "step": 16440
    },
    {
      "epoch": 2.961296129612961,
      "grad_norm": 0.2402862310409546,
      "learning_rate": 5.069633585466083e-08,
      "loss": 0.1483,
      "step": 16450
    },
    {
      "epoch": 2.963096309630963,
      "grad_norm": 0.2241675853729248,
      "learning_rate": 4.60907784423259e-08,
      "loss": 0.1473,
      "step": 16460
    },
    {
      "epoch": 2.964896489648965,
      "grad_norm": 0.25388041138648987,
      "learning_rate": 4.170440175963464e-08,
      "loss": 0.1488,
      "step": 16470
    },
    {
      "epoch": 2.9666966696669665,
      "grad_norm": 0.25252118706703186,
      "learning_rate": 3.7537225052519665e-08,
      "loss": 0.1527,
      "step": 16480
    },
    {
      "epoch": 2.9684968496849686,
      "grad_norm": 0.268611341714859,
      "learning_rate": 3.35892666051274e-08,
      "loss": 0.1499,
      "step": 16490
    },
    {
      "epoch": 2.9702970297029703,
      "grad_norm": 0.2767070531845093,
      "learning_rate": 2.9860543739756994e-08,
      "loss": 0.1496,
      "step": 16500
    },
    {
      "epoch": 2.9702970297029703,
      "eval_loss": 0.18389751017093658,
      "eval_runtime": 316.7689,
      "eval_samples_per_second": 124.697,
      "eval_steps_per_second": 3.899,
      "step": 16500
    },
    {
      "epoch": 2.972097209720972,
      "grad_norm": 0.2480713129043579,
      "learning_rate": 2.6351072816771516e-08,
      "loss": 0.1479,
      "step": 16510
    },
    {
      "epoch": 2.973897389738974,
      "grad_norm": 0.2470661997795105,
      "learning_rate": 2.3060869234531325e-08,
      "loss": 0.1521,
      "step": 16520
    },
    {
      "epoch": 2.9756975697569756,
      "grad_norm": 0.27208593487739563,
      "learning_rate": 1.9989947429338573e-08,
      "loss": 0.1469,
      "step": 16530
    },
    {
      "epoch": 2.9774977497749777,
      "grad_norm": 0.25373196601867676,
      "learning_rate": 1.7138320875337288e-08,
      "loss": 0.145,
      "step": 16540
    },
    {
      "epoch": 2.9792979297929794,
      "grad_norm": 0.23094311356544495,
      "learning_rate": 1.4506002084502257e-08,
      "loss": 0.1518,
      "step": 16550
    },
    {
      "epoch": 2.981098109810981,
      "grad_norm": 0.26245826482772827,
      "learning_rate": 1.2093002606550219e-08,
      "loss": 0.1544,
      "step": 16560
    },
    {
      "epoch": 2.9828982898289826,
      "grad_norm": 0.2400779128074646,
      "learning_rate": 9.899333028901004e-09,
      "loss": 0.1517,
      "step": 16570
    },
    {
      "epoch": 2.9846984698469847,
      "grad_norm": 0.2639738917350769,
      "learning_rate": 7.925002976627571e-09,
      "loss": 0.1525,
      "step": 16580
    },
    {
      "epoch": 2.9864986498649864,
      "grad_norm": 0.2562364339828491,
      "learning_rate": 6.170021112417157e-09,
      "loss": 0.1464,
      "step": 16590
    },
    {
      "epoch": 2.9882988298829884,
      "grad_norm": 0.2701812982559204,
      "learning_rate": 4.63439513653241e-09,
      "loss": 0.149,
      "step": 16600
    },
    {
      "epoch": 2.99009900990099,
      "grad_norm": 0.24928928911685944,
      "learning_rate": 3.318131786772538e-09,
      "loss": 0.1467,
      "step": 16610
    },
    {
      "epoch": 2.9918991899189917,
      "grad_norm": 0.2699834108352661,
      "learning_rate": 2.221236838462204e-09,
      "loss": 0.1508,
      "step": 16620
    },
    {
      "epoch": 2.993699369936994,
      "grad_norm": 0.264082133769989,
      "learning_rate": 1.3437151043904638e-09,
      "loss": 0.1474,
      "step": 16630
    },
    {
      "epoch": 2.9954995499549955,
      "grad_norm": 0.26569864153862,
      "learning_rate": 6.855704348385228e-10,
      "loss": 0.1401,
      "step": 16640
    },
    {
      "epoch": 2.9972997299729975,
      "grad_norm": 0.250567764043808,
      "learning_rate": 2.4680571750757e-10,
      "loss": 0.1489,
      "step": 16650
    },
    {
      "epoch": 2.999099909990999,
      "grad_norm": 0.2329290807247162,
      "learning_rate": 2.7422877552085722e-11,
      "loss": 0.1518,
      "step": 16660
    },
    {
      "epoch": 3.0,
      "step": 16665,
      "total_flos": 1.7185337421470892e+19,
      "train_loss": 0.18622600764367017,
      "train_runtime": 37243.0541,
      "train_samples_per_second": 28.636,
      "train_steps_per_second": 0.447
    }
  ],
  "logging_steps": 10,
  "max_steps": 16665,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.7185337421470892e+19,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
